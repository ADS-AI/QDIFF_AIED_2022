{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"qdiff_skill_ensemble_with_statistical_significance.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"widgets":{"application/vnd.jupyter.widget-state+json":{"4b7a94815cc74515882f8e5bcdcbcfdf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_6c2203b2dbe649cab69e4ed48c8dafee","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2e126a03d90a417e97f330cc850c7c26","IPY_MODEL_6592a3df822347a2a82eff13181fa884"]}},"6c2203b2dbe649cab69e4ed48c8dafee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2e126a03d90a417e97f330cc850c7c26":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7d330f240996476494215338a4fda230","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1c5ad2156d5440e6a559e4fa2affba8b"}},"6592a3df822347a2a82eff13181fa884":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0b4079e9560d476c99ea28ff42435627","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:11&lt;00:00, 20.6kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7f2d8c826d394f7ca70e34f25accdf74"}},"7d330f240996476494215338a4fda230":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1c5ad2156d5440e6a559e4fa2affba8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0b4079e9560d476c99ea28ff42435627":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7f2d8c826d394f7ca70e34f25accdf74":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3f6743aa7ad94a7aba793263f279a3fb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_265862f4f3cd4520a22d22f3de707862","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f1796ef5f27d4a709b59f7a547ffb7f6","IPY_MODEL_07f5e938e3b44a41b11846e0ee1e0890"]}},"265862f4f3cd4520a22d22f3de707862":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f1796ef5f27d4a709b59f7a547ffb7f6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c70f2d1886d04569bb700f24fd686ca3","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":433,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":433,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0ecbfd96049c49f6887657c2561ab757"}},"07f5e938e3b44a41b11846e0ee1e0890":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6dbfe84667bc4583a38cd9d4501f62cf","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 433/433 [00:06&lt;00:00, 66.9B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_30925da280d64ee2a6d4bf92317989eb"}},"c70f2d1886d04569bb700f24fd686ca3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0ecbfd96049c49f6887657c2561ab757":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6dbfe84667bc4583a38cd9d4501f62cf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"30925da280d64ee2a6d4bf92317989eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"33b787b6ae5b4449b1dee364d28eac2d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7b89de2cba5545d889113d1580e7ae1b","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_48b55747fad24d0ab1cbe61aa57d003b","IPY_MODEL_40eab5db38294d288ffeb4789ab9fdea"]}},"7b89de2cba5545d889113d1580e7ae1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"48b55747fad24d0ab1cbe61aa57d003b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e610c17f26b14c64ad0d9bc56301f245","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":440473133,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":440473133,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8d835f209b394df0922f6ac4a501513f"}},"40eab5db38294d288ffeb4789ab9fdea":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8293b5175f9446ec9b40910b50147f41","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 440M/440M [00:06&lt;00:00, 71.3MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8d93844bcae44d7ca7ac06adcb1e0323"}},"e610c17f26b14c64ad0d9bc56301f245":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8d835f209b394df0922f6ac4a501513f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8293b5175f9446ec9b40910b50147f41":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8d93844bcae44d7ca7ac06adcb1e0323":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sR9av2JU3kf6","outputId":"dca2e205-7ed5-470e-be99-3692f5782a05"},"source":["import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","import torch\n","import logging\n","logging.basicConfig(level=logging.ERROR)\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oS6UjTMF4Y9w","outputId":"c851bdcf-1fad-4361-f557-e7a0826aaa15"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BzKeqoCs3kgA","outputId":"c52ccb34-b8e0-43a1-a1d0-c6e914f9a1de"},"source":["\n","!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/87/ef312eef26f5cecd8b17ae9654cdd8d1fae1eb6dbd87257d6d73c128a4d0/transformers-4.3.2-py3-none-any.whl (1.8MB)\n","\u001b[K     |████████████████████████████████| 1.8MB 7.9MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 49.0MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/5b/44baae602e0a30bcc53fbdbc60bd940c15e143d252d658dfdefce736ece5/tokenizers-0.10.1-cp36-cp36m-manylinux2010_x86_64.whl (3.2MB)\n","\u001b[K     |████████████████████████████████| 3.2MB 54.2MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=f3c3ef1bd7b5e4eefc60a10302cf8ffc1353e1650476e99281386e8cc8edfca5\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, tokenizers, transformers\n","Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":402},"id":"GsADhaO93kgD","outputId":"7401d2b3-f25d-43e5-c0e5-8845742c0550"},"source":["import pandas as pd\n","final_data = pd.read_csv(\"train_skill_name_difficulty.csv\")\n","final_data"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>board_syllabus</th>\n","      <th>question_answer</th>\n","      <th>skill_label</th>\n","      <th>difficulty_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Raj English&gt;&gt;XII&gt;&gt;Biology&gt;&gt;Domestication, Cult...</td>\n","      <td>Among the following, freshwater fish is rohu ...</td>\n","      <td>3</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Maharashtra New&gt;&gt;VI&gt;&gt;General Science&gt;&gt;Sound&gt;&gt;P...</td>\n","      <td>Which of the following statement is true? Sou...</td>\n","      <td>3</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ICSE OLD&gt;&gt;XI&gt;&gt;Computer Science&gt;&gt;Functions&gt;&gt;Con...</td>\n","      <td>The process of using multiple constructors wi...</td>\n","      <td>3</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>CBSE&gt;&gt;VI&gt;&gt;Science&gt;&gt;Separation of Substances&gt;&gt;S...</td>\n","      <td>Sieving is based on the difference in the siz...</td>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>AP&gt;&gt;X&gt;&gt;Biology&gt;&gt;Excretion - The Wastage Dispos...</td>\n","      <td>The removal of toxic and unwanted waste subst...</td>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>39124</th>\n","      <td>CAPS(South Africa)&gt;&gt;Grade 7&gt;&gt;Natural Sciences&gt;...</td>\n","      <td>How heat loss problems are prevented by birds...</td>\n","      <td>2</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>39125</th>\n","      <td>CBSE&gt;&gt;X&gt;&gt;Science&gt;&gt;Metals and Non-Metal</td>\n","      <td>Give reasons why copper is used to make hot w...</td>\n","      <td>3</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>39126</th>\n","      <td>CBSE&gt;&gt;VII&gt;&gt;Science&gt;&gt;Motion and Time</td>\n","      <td>The horizontal line in the graph is denoted as...</td>\n","      <td>2</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>39127</th>\n","      <td>Tamil Nadu&gt;&gt;VI&gt;&gt;Science&gt;&gt;Term 1&gt;&gt;Physics&gt;&gt;Forc...</td>\n","      <td>SI unit of force is newton The SI unit of for...</td>\n","      <td>3</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>39128</th>\n","      <td>Tamil Nadu&gt;&gt;VIII&gt;&gt;Science&gt;&gt;Term 1&gt;&gt;Physics&gt;&gt;Fo...</td>\n","      <td>In machines sliding frictions is replaced to ...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>39129 rows × 4 columns</p>\n","</div>"],"text/plain":["                                          board_syllabus  ... difficulty_label\n","0      Raj English>>XII>>Biology>>Domestication, Cult...  ...                0\n","1      Maharashtra New>>VI>>General Science>>Sound>>P...  ...                2\n","2      ICSE OLD>>XI>>Computer Science>>Functions>>Con...  ...                0\n","3      CBSE>>VI>>Science>>Separation of Substances>>S...  ...                1\n","4      AP>>X>>Biology>>Excretion - The Wastage Dispos...  ...                1\n","...                                                  ...  ...              ...\n","39124  CAPS(South Africa)>>Grade 7>>Natural Sciences>...  ...                1\n","39125             CBSE>>X>>Science>>Metals and Non-Metal  ...                2\n","39126                CBSE>>VII>>Science>>Motion and Time  ...                1\n","39127  Tamil Nadu>>VI>>Science>>Term 1>>Physics>>Forc...  ...                2\n","39128  Tamil Nadu>>VIII>>Science>>Term 1>>Physics>>Fo...  ...                1\n","\n","[39129 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A0Ja4jiFhmdg","outputId":"d3d12736-5089-4092-c959-875ce6a77ba4"},"source":["final_data[\"question_answer\"].values"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([' Among the following, freshwater fish is rohu Rohu is a fresh water fish. Other common freshwater fish are catla, common carp.',\n","       ' Which of the following statement is true? Sound requires a medium for propagation. Sound travels through a medium (solid, liquid or gas). It cannot travel through vacuum.',\n","       ' The process of using multiple constructors with the same name but with different parameters is known as: Constructor overloading Constructor overloading is a technique in Java in which a class can have any number of constructors that differ in parameter lists.',\n","       ...,\n","       'The horizontal line in the graph is denoted as the X-axis. The horizontal line points in the horizontal direction and is denoted as the X-axis in the graph.',\n","       ' SI unit of force is newton The SI unit of force is Newton (N), named after famous scientist Isaac Newton who discovered force of gravitation.',\n","       ' In machines sliding frictions is replaced to rolling by use of ball bearings Ball bearing roll to produce rolling;friction.'],\n","      dtype=object)"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iQhO6qqt6lge","outputId":"c9bcced4-d5f6-4615-ce5a-26339471e999"},"source":["final_data['skill_label'].value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3    11376\n","4    10707\n","2     8619\n","1     4997\n","0     3430\n","Name: skill_label, dtype: int64"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"9exlBELH5oq9"},"source":["\n","\n","!cp \"/content/drive/My Drive/research_skill_name_prediction/label_encoder_skill_lstm\"  /content"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-OBarOLBz2nO"},"source":["def get_labels(prediction):\n","    predicted_label =  LE.inverse_transform([prediction])\n","    return predicted_label[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":281},"id":"XqWem79lbn1J","outputId":"6cf23bc9-7d20-412f-cf19-08ee0a0993ee"},"source":["final_data['difficulty_label'].value_counts().sort_values(ascending=False).plot(kind='bar')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7f979ad98358>"]},"metadata":{"tags":[]},"execution_count":9},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"RxliBQEJ9eTG"},"source":["val = pd.read_csv(\"val_skill_name_difficulty.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":402},"id":"07eBaI9wA2hL","outputId":"3040e21a-2938-44c4-80f1-b8a0cdf85cf8"},"source":["val"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>board_syllabus</th>\n","      <th>question_answer</th>\n","      <th>skill_label</th>\n","      <th>difficulty_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>AP&gt;&gt;VII&gt;&gt;Science&gt;&gt;Animal Fibre&gt;&gt;Silk</td>\n","      <td>Name the two types of protein from which silk...</td>\n","      <td>3</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Maharashtra New&gt;&gt;VIII&gt;&gt;General Science&gt;&gt;Man Ma...</td>\n","      <td>Give reasons: (i) Thermocol is used for the p...</td>\n","      <td>4</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>CBSE&gt;&gt;VI&gt;&gt;Science&gt;&gt;Fun with Magnets&gt;&gt;Demagneti...</td>\n","      <td>Identify the odd option. Rubbing a magnetic m...</td>\n","      <td>4</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Tamil Nadu&gt;&gt;VIII&gt;&gt;Science&gt;&gt;Term 1&gt;&gt;Physics&gt;&gt;Li...</td>\n","      <td>Find the speed of light in glass of refractiv...</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Raj English&gt;&gt;XII&gt;&gt;Biology&gt;&gt;Integumentary Syste...</td>\n","      <td>Which of the following function is associated...</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2055</th>\n","      <td>CBSE&gt;&gt;XI&gt;&gt;Chemistry&gt;&gt;Chemistry : Part I&gt;&gt;Equil...</td>\n","      <td>The solubility of A 2 X 3 is y mol.dm -3 . So...</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2056</th>\n","      <td>CBSE&gt;&gt;VI&gt;&gt;Computer Science&gt;&gt;Using Mail Merge&gt;&gt;...</td>\n","      <td>To create an invitation letter, click on Mail...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2057</th>\n","      <td>CBSE&gt;&gt;XII&gt;&gt;Physics&gt;&gt;Physics : Part - II&gt;&gt;Ray O...</td>\n","      <td>Choose the correct option about the intensity...</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2058</th>\n","      <td>ICSE OLD&gt;&gt;VII&gt;&gt;Biology&gt;&gt;Organ System of Human ...</td>\n","      <td>Which of the following instrument is used to ...</td>\n","      <td>4</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2059</th>\n","      <td>CBSE&gt;&gt;X&gt;&gt;Science&gt;&gt;Carbon and its Compounds&gt;&gt;Al...</td>\n","      <td>Identify the correct statement about allotrop...</td>\n","      <td>4</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2060 rows × 4 columns</p>\n","</div>"],"text/plain":["                                         board_syllabus  ... difficulty_label\n","0                  AP>>VII>>Science>>Animal Fibre>>Silk  ...                0\n","1     Maharashtra New>>VIII>>General Science>>Man Ma...  ...                2\n","2     CBSE>>VI>>Science>>Fun with Magnets>>Demagneti...  ...                2\n","3     Tamil Nadu>>VIII>>Science>>Term 1>>Physics>>Li...  ...                0\n","4     Raj English>>XII>>Biology>>Integumentary Syste...  ...                0\n","...                                                 ...  ...              ...\n","2055  CBSE>>XI>>Chemistry>>Chemistry : Part I>>Equil...  ...                0\n","2056  CBSE>>VI>>Computer Science>>Using Mail Merge>>...  ...                1\n","2057  CBSE>>XII>>Physics>>Physics : Part - II>>Ray O...  ...                0\n","2058  ICSE OLD>>VII>>Biology>>Organ System of Human ...  ...                1\n","2059  CBSE>>X>>Science>>Carbon and its Compounds>>Al...  ...                0\n","\n","[2060 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":402},"id":"g8tVsjiWj-cF","outputId":"605dd6d3-74e0-4692-96b5-5cf69494c489"},"source":["test = pd.read_csv(\"test_skill_name_difficulty.csv\")\n","test"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>board_syllabus</th>\n","      <th>question_answer</th>\n","      <th>skill_label</th>\n","      <th>difficulty_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>CBSE&gt;&gt;Nursery&gt;&gt;Environmental Science&gt;&gt;Common V...</td>\n","      <td>Write down the names of some common vegetable...</td>\n","      <td>3</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>CBSE&gt;&gt;XII&gt;&gt;Physics&gt;&gt;Physics : Part - II&gt;&gt;Atoms</td>\n","      <td>Name the series of hydrogen atom which lies i...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ICSE OLD&gt;&gt;XI&gt;&gt;Political Science&gt;&gt;State, Govern...</td>\n","      <td>Which of the following is not the element of ...</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Maharashtra New&gt;&gt;VII&gt;&gt;General Science&gt;&gt;Static ...</td>\n","      <td>The process of electrically charging an objec...</td>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Tamil Nadu&gt;&gt;IX&gt;&gt;Science&gt;&gt;Physics&gt;&gt;Measurement&gt;...</td>\n","      <td>The mass of an object is measured in kilogram...</td>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>4572</th>\n","      <td>ICSE OLD&gt;&gt;VIII&gt;&gt;Biology&gt;&gt;Nervous System And Se...</td>\n","      <td>Which of the following is the first cranial n...</td>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4573</th>\n","      <td>CBSE&gt;&gt;VII&gt;&gt;Computer Science&gt;&gt;Advance features ...</td>\n","      <td>To ungroup the worksheets: Right-click on any...</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4574</th>\n","      <td>CBSE&gt;&gt;VIII&gt;&gt;Science&gt;&gt;Chemical Effects of Elect...</td>\n","      <td>After passing electricity through a solution ...</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4575</th>\n","      <td>CLSP&gt;&gt;Stage 9&gt;&gt;Science&gt;&gt;Chemistry&gt;&gt;Material pr...</td>\n","      <td>Identify the scientists who gave the “plum-pu...</td>\n","      <td>3</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4576</th>\n","      <td>AP&gt;&gt;VIII&gt;&gt;Physical Science&gt;&gt;Physical Science (...</td>\n","      <td>What do you understand by the term static ele...</td>\n","      <td>2</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4577 rows × 4 columns</p>\n","</div>"],"text/plain":["                                         board_syllabus  ... difficulty_label\n","0     CBSE>>Nursery>>Environmental Science>>Common V...  ...                2\n","1        CBSE>>XII>>Physics>>Physics : Part - II>>Atoms  ...                1\n","2     ICSE OLD>>XI>>Political Science>>State, Govern...  ...                0\n","3     Maharashtra New>>VII>>General Science>>Static ...  ...                1\n","4     Tamil Nadu>>IX>>Science>>Physics>>Measurement>...  ...                1\n","...                                                 ...  ...              ...\n","4572  ICSE OLD>>VIII>>Biology>>Nervous System And Se...  ...                1\n","4573  CBSE>>VII>>Computer Science>>Advance features ...  ...                0\n","4574  CBSE>>VIII>>Science>>Chemical Effects of Elect...  ...                2\n","4575  CLSP>>Stage 9>>Science>>Chemistry>>Material pr...  ...                0\n","4576  AP>>VIII>>Physical Science>>Physical Science (...  ...                1\n","\n","[4577 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":84,"referenced_widgets":["4b7a94815cc74515882f8e5bcdcbcfdf","6c2203b2dbe649cab69e4ed48c8dafee","2e126a03d90a417e97f330cc850c7c26","6592a3df822347a2a82eff13181fa884","7d330f240996476494215338a4fda230","1c5ad2156d5440e6a559e4fa2affba8b","0b4079e9560d476c99ea28ff42435627","7f2d8c826d394f7ca70e34f25accdf74"]},"id":"FIrS5sxE3kgk","outputId":"98b8717a-01e9-405a-f429-8dbe087453df"},"source":["from transformers import BertTokenizer\n","\n","# Load the BERT tokenizer.\n","print('Loading BERT tokenizer...')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading BERT tokenizer...\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4b7a94815cc74515882f8e5bcdcbcfdf","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wp64MkNB3kg1"},"source":["\n","def get_labels(prediction):\n","    predicted_label =  LE.inverse_transform([prediction])\n","    return predicted_label[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"uPgTmJPS3kg4","outputId":"7ebf9cd0-be95-4298-df05-dbcd42ddb7f7"},"source":["import joblib\n","from sklearn.preprocessing import LabelEncoder\n","\n","LE = LabelEncoder()\n","LE = joblib.load('label_encoder_skill_lstm')\n","\n","get_labels(0)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Analysing'"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":402},"id":"I_UpqLMG3kg9","outputId":"783e9751-f3e4-450e-eba6-564cbf15a19a"},"source":["final_data"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>board_syllabus</th>\n","      <th>question_answer</th>\n","      <th>skill_label</th>\n","      <th>difficulty_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Raj English&gt;&gt;XII&gt;&gt;Biology&gt;&gt;Domestication, Cult...</td>\n","      <td>Among the following, freshwater fish is rohu ...</td>\n","      <td>3</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Maharashtra New&gt;&gt;VI&gt;&gt;General Science&gt;&gt;Sound&gt;&gt;P...</td>\n","      <td>Which of the following statement is true? Sou...</td>\n","      <td>3</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ICSE OLD&gt;&gt;XI&gt;&gt;Computer Science&gt;&gt;Functions&gt;&gt;Con...</td>\n","      <td>The process of using multiple constructors wi...</td>\n","      <td>3</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>CBSE&gt;&gt;VI&gt;&gt;Science&gt;&gt;Separation of Substances&gt;&gt;S...</td>\n","      <td>Sieving is based on the difference in the siz...</td>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>AP&gt;&gt;X&gt;&gt;Biology&gt;&gt;Excretion - The Wastage Dispos...</td>\n","      <td>The removal of toxic and unwanted waste subst...</td>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>39124</th>\n","      <td>CAPS(South Africa)&gt;&gt;Grade 7&gt;&gt;Natural Sciences&gt;...</td>\n","      <td>How heat loss problems are prevented by birds...</td>\n","      <td>2</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>39125</th>\n","      <td>CBSE&gt;&gt;X&gt;&gt;Science&gt;&gt;Metals and Non-Metal</td>\n","      <td>Give reasons why copper is used to make hot w...</td>\n","      <td>3</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>39126</th>\n","      <td>CBSE&gt;&gt;VII&gt;&gt;Science&gt;&gt;Motion and Time</td>\n","      <td>The horizontal line in the graph is denoted as...</td>\n","      <td>2</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>39127</th>\n","      <td>Tamil Nadu&gt;&gt;VI&gt;&gt;Science&gt;&gt;Term 1&gt;&gt;Physics&gt;&gt;Forc...</td>\n","      <td>SI unit of force is newton The SI unit of for...</td>\n","      <td>3</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>39128</th>\n","      <td>Tamil Nadu&gt;&gt;VIII&gt;&gt;Science&gt;&gt;Term 1&gt;&gt;Physics&gt;&gt;Fo...</td>\n","      <td>In machines sliding frictions is replaced to ...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>39129 rows × 4 columns</p>\n","</div>"],"text/plain":["                                          board_syllabus  ... difficulty_label\n","0      Raj English>>XII>>Biology>>Domestication, Cult...  ...                0\n","1      Maharashtra New>>VI>>General Science>>Sound>>P...  ...                2\n","2      ICSE OLD>>XI>>Computer Science>>Functions>>Con...  ...                0\n","3      CBSE>>VI>>Science>>Separation of Substances>>S...  ...                1\n","4      AP>>X>>Biology>>Excretion - The Wastage Dispos...  ...                1\n","...                                                  ...  ...              ...\n","39124  CAPS(South Africa)>>Grade 7>>Natural Sciences>...  ...                1\n","39125             CBSE>>X>>Science>>Metals and Non-Metal  ...                2\n","39126                CBSE>>VII>>Science>>Motion and Time  ...                1\n","39127  Tamil Nadu>>VI>>Science>>Term 1>>Physics>>Forc...  ...                2\n","39128  Tamil Nadu>>VIII>>Science>>Term 1>>Physics>>Fo...  ...                1\n","\n","[39129 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"MHdVe13Fr3vt"},"source":["new_data = final_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AkyM7gqv3khI"},"source":["question_answer = new_data[\"question_answer\"].values\n","categories = new_data[\"skill_label\"].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ndpw0p1SBUoZ","outputId":"9677bf08-9793-4fab-b854-b6875f71563d"},"source":["question_answer"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([' Among the following, freshwater fish is rohu Rohu is a fresh water fish. Other common freshwater fish are catla, common carp.',\n","       ' Which of the following statement is true? Sound requires a medium for propagation. Sound travels through a medium (solid, liquid or gas). It cannot travel through vacuum.',\n","       ' The process of using multiple constructors with the same name but with different parameters is known as: Constructor overloading Constructor overloading is a technique in Java in which a class can have any number of constructors that differ in parameter lists.',\n","       ...,\n","       'The horizontal line in the graph is denoted as the X-axis. The horizontal line points in the horizontal direction and is denoted as the X-axis in the graph.',\n","       ' SI unit of force is newton The SI unit of force is Newton (N), named after famous scientist Isaac Newton who discovered force of gravitation.',\n","       ' In machines sliding frictions is replaced to rolling by use of ball bearings Ball bearing roll to produce rolling;friction.'],\n","      dtype=object)"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"tFkS_H_83khL","outputId":"b171ec4d-b48b-4f59-ca88-1c8b90f44027"},"source":["question_answer[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["' Among the following, freshwater fish is rohu Rohu is a fresh water fish. Other common freshwater fish are catla, common carp.'"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ian7gSDE3khR","outputId":"d4cc5ec3-965b-4644-e921-0f599aa69316"},"source":["len(categories)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["39129"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y_ZeuHc63khU","outputId":"3a3fc55a-e95f-499f-e874-50f45d4e0c87"},"source":["input_ids = []\n","attention_masks = []\n","\n","for sent in question_answer:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens.\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 128,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        truncation=True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","# Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', question_answer[0])\n","print('Token IDs:', input_ids[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2155: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Original:   Among the following, freshwater fish is rohu Rohu is a fresh water fish. Other common freshwater fish are catla, common carp.\n","Token IDs: tensor([  101,  2426,  1996,  2206,  1010, 12573,  3869,  2003, 20996,  6979,\n","        20996,  6979,  2003,  1037,  4840,  2300,  3869,  1012,  2060,  2691,\n","        12573,  3869,  2024,  4937,  2721,  1010,  2691, 29267,  1012,   102,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iVGvVZb13kha","outputId":"feaef672-88d8-4df1-ef12-7dbcd977710d"},"source":["print('Original: ', len(question_answer[1]))\n","print('Token IDs:', len(input_ids[1]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Original:  171\n","Token IDs: 128\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5nmRiaBbA9OH"},"source":["val_text = val[\"question_answer\"].values\n","val_labels = val[\"skill_label\"].values\n","test_text = test[\"question_answer\"].values\n","test_labels = test[\"skill_label\"].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E-s_H1WdyvCw","outputId":"5dd18b2f-e656-42b3-c0cd-93c5bfc29f07"},"source":["test_labels"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([3, 1, 0, ..., 0, 3, 2])"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YF-mKCC1CUjD","outputId":"3b80f7d6-f942-46f1-b00a-4725204a601c"},"source":["val_labels"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([3, 4, 4, ..., 0, 4, 4])"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sOQuDahhAzOO","outputId":"2f9cee89-cdc0-4376-a080-7e15086ca87e"},"source":["val_input_ids = []\n","val_attention_masks = []\n","\n","for sent in val_text:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens.\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 128,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        truncation=True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    val_input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    val_attention_masks.append(encoded_dict['attention_mask'])\n","# Convert the lists into tensors.\n","val_input_ids = torch.cat(val_input_ids, dim=0)\n","val_attention_masks = torch.cat(val_attention_masks, dim=0)\n","\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', val_text[0])\n","print('Token IDs:', val_attention_masks[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2155: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Original:   Name the two types of protein from which silk is made. Sericin and fibroin \n","Token IDs: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Siskea7qDLUG","outputId":"c8e8de67-ccd5-4727-ef3d-810b7ccec89e"},"source":["print('Original: ', val_text[1])\n","print('Token IDs:', val_input_ids[1])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Original:   Give reasons: (i) Thermocol is used for the packing of delicate items. (ii) Name two diseases that may develop in people working in thermocol industries. (i) Thermocol is a good shock-absorber, therefore, it is used for packing of delicate items. (ii) People working in thermocol industries may suffer from blood cancer such as leukemia and lymphoma or have problems in eyes and respiratory system. \n","Token IDs: tensor([  101,  2507,  4436,  1024,  1006,  1045,  1007,  1996, 10867, 24163,\n","         2140,  2003,  2109,  2005,  1996, 14743,  1997, 10059,  5167,  1012,\n","         1006,  2462,  1007,  2171,  2048,  7870,  2008,  2089,  4503,  1999,\n","         2111,  2551,  1999,  1996, 10867, 24163,  2140,  6088,  1012,  1006,\n","         1045,  1007,  1996, 10867, 24163,  2140,  2003,  1037,  2204,  5213,\n","         1011, 16888,  2121,  1010,  3568,  1010,  2009,  2003,  2109,  2005,\n","        14743,  1997, 10059,  5167,  1012,  1006,  2462,  1007,  2111,  2551,\n","         1999,  1996, 10867, 24163,  2140,  6088,  2089,  9015,  2013,  2668,\n","         4456,  2107,  2004, 25468,  1998,  1048, 24335,  8458,  9626,  2030,\n","         2031,  3471,  1999,  2159,  1998, 16464,  2291,  1012,   102,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"irMTimjf3khd"},"source":["labels = torch.tensor(categories)\n","val_labels = torch.tensor(val_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PdOgWP_LKTHi","outputId":"1a420c83-2ba1-48e2-82c0-075f58854991"},"source":["val_labels"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([3, 4, 4,  ..., 0, 4, 4])"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"OJJ0I8Ud3khf","outputId":"83cb5c50-4685-4546-b5f3-fec56d68056f"},"source":["get_labels(1)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Applying'"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z1ZAbQRfiG63","outputId":"193146bb-fb74-4684-b581-71b72955e602"},"source":["len(set(final_data[\"question_answer\"].values).intersection(val[\"question_answer\"].values))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BNDW74Ny3khj","outputId":"02a4b8fc-3608-4c98-9817-e7f6ff88c64f"},"source":["num_classes = len(list(set(categories)))\n","list(set(categories))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0, 1, 2, 3, 4]"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"ZmaLk5Ab3khl"},"source":["from torch.utils.data import TensorDataset, random_split\n","# train_poincare_tensor = torch.tensor(poincare_embeddings_final,dtype=torch.float)\n","# difficulty_tensor = torch.tensor(difficulty_level_vectors,dtype=torch.float)\n","# Combine the training inputs into a TensorDataset.\n","dataset = TensorDataset(input_ids, attention_masks, labels)\n","val_dataset = TensorDataset(val_input_ids, val_attention_masks,val_labels) \n","# Create a 90-10train-validation split.\n","\n","# Calculate the number of samples to include in each set.\n","# train_size = int(0.90 * len(dataset))\n","# val_size = len(dataset) - train_size\n","\n","# # Divide the dataset by randomly selecting samples.\n","# train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","# print('{:>5,} training samples'.format(train_size))\n","# # print('{:>5,} validation samples'.format(val_size))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D_lTinod3kho"},"source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","batch_size = 32\n","train_dataloader = DataLoader(\n","            dataset,  # The training samples.\n","            sampler = RandomSampler(dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","\n","validation_dataloader = DataLoader(\n","            val_dataset, # The validation samples.\n","            sampler = SequentialSampler(val_dataset), \n","            batch_size = batch_size \n","        )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_2tmAMlw3khr"},"source":["from transformers import BertModel, AdamW, BertConfig\n","\n","# # Loads BertForSequenceClassification, the pretrained BERT model with a single \n","# model = BertModel.from_pretrained(\n","#     \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n","# )\n","\n","# # Tell pytorch to run this model on the GPU.\n","# model.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DkDmTZhVChN6","outputId":"ca6433f5-9790-4d11-f4fc-63b066685c62"},"source":["\n","\n","\n","\n","set(test[\"question_answer\"].values).intersection(set(final_data[\"question_answer\"].values))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["set()"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JJvEax7_uTEh","outputId":"5e67c2a9-e51b-42ef-867f-2015b1ac88f5"},"source":["!unzip \"/content/drive/MyDrive/research_skill_name_prediction/model_bert_skill_prediction.zip\"\n","!unzip \"/content/drive/MyDrive/research_skill_name_prediction/model_bert_difficulty_prediction.zip\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["Archive:  /content/drive/MyDrive/research_skill_name_prediction/model_bert_skill_prediction.zip\n","   creating: model_bert_skill_prediction/\n","  inflating: model_bert_skill_prediction/tokenizer_config.json  \n","  inflating: model_bert_skill_prediction/model_weights  \n","  inflating: model_bert_skill_prediction/vocab.txt  \n","  inflating: model_bert_skill_prediction/special_tokens_map.json  \n","Archive:  /content/drive/MyDrive/research_skill_name_prediction/model_bert_difficulty_prediction.zip\n","   creating: model_bert_difficulty_prediction/\n","  inflating: model_bert_difficulty_prediction/tokenizer_config.json  \n","  inflating: model_bert_difficulty_prediction/model_weights  \n","  inflating: model_bert_difficulty_prediction/vocab.txt  \n","  inflating: model_bert_difficulty_prediction/special_tokens_map.json  \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8CFbt2CWqSLz","outputId":"a517fd56-4f56-4c9e-cf5e-bd8c488eeffe"},"source":["!unzip \"/content/drive/MyDrive/research_skill_name_prediction/model_bert_multi_task_prediction.zip\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["Archive:  /content/drive/MyDrive/research_skill_name_prediction/model_bert_multi_task_prediction.zip\n","   creating: model_bert_multi_task_prediction/\n","  inflating: model_bert_multi_task_prediction/model_weights  \n","  inflating: model_bert_multi_task_prediction/tokenizer_config.json  \n","  inflating: model_bert_multi_task_prediction/special_tokens_map.json  \n","  inflating: model_bert_multi_task_prediction/vocab.txt  \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lJjikk9efqJP","outputId":"ab1570be-d62b-41e9-92b0-c2579cb5b2a5"},"source":["!pip install torchtext\n","import torchtext"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (0.3.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.19.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.23.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.7.0+cu101)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.41.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext) (0.16.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchtext) (3.7.4.3)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchtext) (0.8)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fsD6CUxrvnL0","outputId":"1a4ab229-f371-4b10-ccb7-ee1c0c18e993"},"source":["!unzip \"/content/drive/MyDrive/research_skill_name_prediction/model_bert_skill_cascade.zip\"\n","!unzip \"/content/drive/MyDrive/research_skill_name_prediction/model_save_BLOOM_difficulty_Lstm.zip\"\n","!unzip \"/content/drive/MyDrive/research_skill_name_prediction/model_save_gru_difficulty_name.zip\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["Archive:  /content/drive/MyDrive/research_skill_name_prediction/model_bert_skill_cascade.zip\n","   creating: model_bert_skill_cascade/\n","  inflating: model_bert_skill_cascade/model_weights  \n","  inflating: model_bert_skill_cascade/tokenizer_config.json  \n","  inflating: model_bert_skill_cascade/special_tokens_map.json  \n","  inflating: model_bert_skill_cascade/vocab.txt  \n","Archive:  /content/drive/MyDrive/research_skill_name_prediction/model_save_BLOOM_difficulty_Lstm.zip\n","   creating: model_save_BLOOM_difficulty_Lstm/\n","  inflating: model_save_BLOOM_difficulty_Lstm/model_weights  \n","Archive:  /content/drive/MyDrive/research_skill_name_prediction/model_save_gru_difficulty_name.zip\n","   creating: model_save_gru_difficulty_name/\n","  inflating: model_save_gru_difficulty_name/model_weights  \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bjFKa6cwb_EJ","outputId":"89c53394-ec2c-41d6-efe5-b22d04107b08"},"source":["!unzip \"/content/drive/MyDrive/research_skill_name_prediction/model_save_gru_skill_name.zip\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["Archive:  /content/drive/MyDrive/research_skill_name_prediction/model_save_gru_skill_name.zip\n","   creating: model_save_gru_skill_name/\n","  inflating: model_save_gru_skill_name/model_weights  \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YBoO8oWNFf2z","outputId":"6f43610f-4816-4a3f-fd6d-06963ab585db"},"source":["!unzip \"/content/drive/MyDrive/research_skill_name_prediction/model_bert_difficulty_cascade.zip\"\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Archive:  /content/drive/MyDrive/research_skill_name_prediction/model_bert_difficulty_cascade.zip\n","   creating: model_bert_difficulty_cascade/\n","  inflating: model_bert_difficulty_cascade/model_weights  \n","  inflating: model_bert_difficulty_cascade/special_tokens_map.json  \n","  inflating: model_bert_difficulty_cascade/tokenizer_config.json  \n","  inflating: model_bert_difficulty_cascade/vocab.txt  \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cYjl4K_tlgDr"},"source":["\n","text = torchtext.data.Field(lower=True, batch_first=True, tokenize='spacy', include_lengths=True)\n","target = torchtext.data.Field(sequential=False, use_vocab=False, is_target=True)\n","# use field objects to read training, validation and test sets\n","train = torchtext.data.TabularDataset(path='train_skill_name_difficulty.csv', format='csv',\n","                                      fields={'question_answer': ('text',text),\n","                                              'difficulty_label': ('target',target)})\n","val = torchtext.data.TabularDataset(path='val_skill_name_difficulty.csv', format='csv',\n","                                    fields={'question_answer': ('text',text),\n","                                              'difficulty_label': ('target',target)})\n","test_text = torchtext.data.TabularDataset(path='test_skill_name_difficulty.csv', format='csv',\n","                                     fields={'difficulty_label': ('label', target),\n","                                             'question_answer': ('text',text)})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oZYlAtyggBro"},"source":["batch_size = 32\n","train_iter = torchtext.data.BucketIterator(dataset=train,\n","                                           batch_size=batch_size,\n","                                           sort_key=lambda x: x.text.__len__(),\n","                                           shuffle=True,\n","                                           sort_within_batch=True) \n","val_iter = torchtext.data.BucketIterator(dataset=val,\n","                                         batch_size=batch_size,\n","                                         sort_key=lambda x: x.text.__len__(),\n","                                         train=False,\n","                                         sort_within_batch=True)\n","test_iter = torchtext.data.BucketIterator(dataset=test_text,\n","                                          batch_size=batch_size,\n","                                          sort=False,\n","                                          sort_within_batch=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-rvFl6SviM5p"},"source":["import torchtext.vocab as vocab\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"spmW5b5ufklC","outputId":"dc0e9d2b-37f1-4a1a-98c3-5df737741196"},"source":["text.build_vocab(train, val, test_text,vectors=\"glove.6B.100d\", min_freq=3)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":[".vector_cache/glove.6B.zip: 862MB [06:51, 2.10MB/s]                           \n","100%|█████████▉| 398329/400000 [00:16<00:00, 25548.29it/s]"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-sLtbTYSfklC","outputId":"dd9081c2-b09f-4f1d-a510-1530380b97e7"},"source":["print(text.vocab.vectors.shape)\n","print(f\"Unique tokens in text vocabulary: {len(text.vocab)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([17896, 100])\n","Unique tokens in text vocabulary: 17896\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5a0Hs7nZi8TW","outputId":"d0b07446-c317-4456-8da2-efd56ac17b66"},"source":["num_classes"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"code","metadata":{"id":"ZA_r6iiReO4N"},"source":["# attention layer code inspired from: https://discuss.pytorch.org/t/self-attention-on-words-and-masking/5671/4\n","from torch import nn\n","class Attention(nn.Module):\n","    def __init__(self, hidden_size, batch_first=False):\n","        super(Attention, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","        self.batch_first = batch_first\n","\n","        self.att_weights = nn.Parameter(torch.Tensor(1, hidden_size), requires_grad=True)\n","\n","        stdv = 1.0 / np.sqrt(self.hidden_size)\n","        for weight in self.att_weights:\n","            nn.init.uniform_(weight, -stdv, stdv)\n","\n","    def get_mask(self):\n","        pass\n","\n","    def forward(self, inputs, lengths):\n","        if self.batch_first:\n","            batch_size, max_len = inputs.size()[:2]\n","        else:\n","            max_len, batch_size = inputs.size()[:2]\n","            \n","        # apply attention layer\n","        weights = torch.bmm(inputs,\n","                            self.att_weights\n","                            .permute(1, 0)  \n","                            .unsqueeze(0)  \n","                            .repeat(batch_size, 1, 1) \n","                            )\n","    \n","        attentions = torch.softmax(F.relu(weights.squeeze()), dim=-1)\n","\n","        mask = torch.ones(attentions.size(), requires_grad=True).cuda()\n","        for i, l in enumerate(lengths):  # skip the first sentence\n","            if l < max_len:\n","                mask[i, l:] = 0\n","\n","        masked = attentions * mask\n","        _sums = masked.sum(-1).unsqueeze(-1)  # sums per row\n","        \n","        attentions = masked.div(_sums)\n","\n","        weighted = torch.mul(inputs, attentions.unsqueeze(-1).expand_as(inputs))\n","\n","        representations = weighted.sum(1).squeeze()\n","\n","        return representations, attentions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aHmQK0OP9mv-"},"source":["from torch import nn\n","class MultiClassClassifierGRU(nn.Module):\n","  def __init__(self, pretrained_lm, hidden_dim=228, lstm_layer=2, dropout=0.2):\n","        super(MultiClassClassifierGRU, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.embedding = nn.Embedding.from_pretrained(pretrained_lm)\n","        self.embedding.weight.requires_grad = False\n","        self.gru1 = nn.GRU(input_size=self.embedding.embedding_dim,\n","                            hidden_size=hidden_dim,\n","                            num_layers=1, \n","                            bidirectional=True)\n","        self.atten1 = Attention(hidden_dim*2, batch_first=True) # 2 is bidrectional\n","        self.gru2 = nn.GRU(input_size=hidden_dim*2,\n","                            hidden_size=hidden_dim,\n","                            num_layers=1, \n","                            bidirectional=True)\n","        self.atten2 = Attention(hidden_dim*2, batch_first=True)\n","        self.fc1 = nn.Sequential(nn.Linear(hidden_dim*7*2, hidden_dim*7*2),\n","                                 nn.BatchNorm1d(hidden_dim*7*2),\n","                                 nn.ReLU()) \n","        self.fc2 = nn.Linear(hidden_dim*7*2, num_classes)\n","\n","    \n","  def forward(self, x, x_len):\n","        x = self.embedding(x)\n","        x = self.dropout(x)\n","        \n","        x = nn.utils.rnn.pack_padded_sequence(x, x_len, batch_first=True, enforce_sorted=False)\n","        out1, h_n = self.gru1(x)\n","\n","        x, lengths = nn.utils.rnn.pad_packed_sequence(out1, batch_first=True)\n","        lengths_tensor = torch.autograd.Variable(torch.FloatTensor(lengths.float())).view(-1,1)\n","\n","        average_pooling = torch.sum(x,dim=1)/lengths_tensor.to(device)\n","        max_pooling =  torch.nn.functional.adaptive_max_pool1d(x.permute(0,2,1), (1,)).view(x.size()[0],-1)\n","\n","        # print(pooled_output_1.size())\n","\n","        x, _ = self.atten1(x, lengths) # skip connect\n","        if len(x.shape)==1:\n","          x = x.reshape(1,-1)\n","        pooled_output_1 = torch.cat([x,h_n[-1],average_pooling,max_pooling],dim=1)\n","\n","\n","        # print(pooled_output_1.size())\n","\n","        out2, h_n = self.gru2(out1)\n","        y, lengths = nn.utils.rnn.pad_packed_sequence(out2, batch_first=True)\n","\n","        lengths_tensor = torch.autograd.Variable(torch.FloatTensor(lengths.float())).view(-1,1)\n","\n","        average_pooling = torch.sum(y,dim=1)/lengths_tensor.to(device)\n","        max_pooling =  torch.nn.functional.adaptive_max_pool1d(y.permute(0,2,1), (1,)).view(y.size()[0],-1)\n","        y, _ = self.atten2(y, lengths)\n","        if len(y.shape)==1:\n","          y = y.reshape(1,-1)\n","        pooled_output_2 = torch.cat([y,h_n[-1],average_pooling,max_pooling],dim=1)\n","\n","        z = torch.cat([pooled_output_1, pooled_output_2], dim=1)\n","\n","        z = self.fc1(self.dropout(z))\n","        z = self.fc2(self.dropout(z))\n","        return z\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VeuaTKv7b8SY","outputId":"b51df529-7387-4aec-ad74-c05271c2d6bd"},"source":["import numpy as np\n","model_gru = MultiClassClassifierGRU(text.vocab.vectors, hidden_dim=400, lstm_layer=2, dropout=0.3).cuda()\n","model_gru.load_state_dict(torch.load(\"model_save_gru_skill_name/model_weights\"))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\r100%|█████████▉| 398329/400000 [00:29<00:00, 25548.29it/s]"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"code","metadata":{"id":"P0Qbs-C7kD0K"},"source":["from torch import nn\n","class MultiClassClassifierLstm(nn.Module):\n","  def __init__(self, pretrained_lm, hidden_dim=128, lstm_layer=2, dropout=0.2):\n","        super(MultiClassClassifierLstm, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.embedding = nn.Embedding.from_pretrained(pretrained_lm)\n","        self.embedding.weight.requires_grad = False\n","        self.lstm1 = nn.LSTM(input_size=self.embedding.embedding_dim,\n","                            hidden_size=hidden_dim,\n","                            num_layers=1, \n","                            bidirectional=True)\n","        self.atten1 = Attention(hidden_dim*2, batch_first=True) # 2 is bidrectional\n","        self.lstm2 = nn.LSTM(input_size=hidden_dim*2,\n","                            hidden_size=hidden_dim,\n","                            num_layers=1, \n","                            bidirectional=True)\n","        self.atten2 = Attention(hidden_dim*2, batch_first=True)\n","        self.fc1 = nn.Sequential(nn.Linear(hidden_dim*lstm_layer*2, hidden_dim*lstm_layer*2),\n","                                 nn.BatchNorm1d(hidden_dim*lstm_layer*2),\n","                                 nn.ReLU()) \n","        self.fc2 = nn.Linear(hidden_dim*lstm_layer*2, 5)\n","\n","    \n","  def forward(self, x, x_len):\n","        x = self.embedding(x)\n","        x = self.dropout(x)\n","        \n","        x = nn.utils.rnn.pack_padded_sequence(x, x_len, batch_first=True, enforce_sorted=False)\n","        out1, (h_n, c_n) = self.lstm1(x)\n","        x, lengths = nn.utils.rnn.pad_packed_sequence(out1, batch_first=True)\n","        x, _ = self.atten1(x, lengths) # skip connect\n","\n","        out2, (h_n, c_n) = self.lstm2(out1)\n","        y, lengths = nn.utils.rnn.pad_packed_sequence(out2, batch_first=True)\n","        y, _ = self.atten2(y, lengths)\n","        \n","        if len(x.shape)==1:\n","          x = x.reshape(1,-1)\n","          y = y.reshape(1,-1)\n","\n","        z = torch.cat([x, y], dim=1)\n","        z = self.fc1(self.dropout(z))\n","        z = self.fc2(self.dropout(z))\n","        return z\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r-lFAuxXXXbk"},"source":["!cp -r \"/content/drive/MyDrive/research_skill_name_prediction/model_save_BLOOM_skill_Lstm\" /content/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lpqEAkGCkt7W","outputId":"a161796c-4443-45a9-b0c3-2f298a28598a"},"source":["model_lstm = MultiClassClassifierLstm(text.vocab.vectors, hidden_dim=128, lstm_layer=2, dropout=0.3).cuda()\n","model_lstm.load_state_dict(torch.load(\"model_save_BLOOM_skill_Lstm/model_weights\"))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":55}]},{"cell_type":"code","metadata":{"id":"nAdyYUrfRBzQ"},"source":["from torch import nn\n","class MultiClassClassifier(nn.Module):\n","    def __init__(self, bert_model_path, labels_count, hidden_dim=768, mlp_dim=500, extras_dim=100, dropout=0.1, freeze_bert=False):\n","        super().__init__()\n","\n","        self.bert = BertModel.from_pretrained(bert_model_path,output_hidden_states=False,output_attentions=False)\n","        self.dropout = nn.Dropout(dropout)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(hidden_dim, mlp_dim),\n","            nn.ReLU(),\n","            # nn.Linear(mlp_dim, mlp_dim),\n","            # # nn.ReLU(),\n","            # # nn.Linear(mlp_dim, mlp_dim),\n","            # nn.ReLU(),            \n","            nn.Linear(mlp_dim, labels_count)\n","        )\n","        # self.softmax = nn.LogSoftmax(dim=1)\n","        if freeze_bert:\n","            print(\"Freezing layers\")\n","            for param in self.bert.parameters():\n","                param.requires_grad = False\n","\n","    def forward(self, tokens, masks):\n","        pooled_output = self.bert(tokens, attention_mask=masks)\n","        dropout_output = self.dropout(pooled_output['pooler_output'])\n","        # concat_output = torch.cat((dropout_output, topic_emb), dim=1)\n","        # concat_output = self.dropout(concat_output)\n","        mlp_output = self.mlp(dropout_output)\n","        # proba = self.sigmoid(mlp_output)\n","        # proba = self.softmax(mlp_output)\n","\n","        return mlp_output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["3f6743aa7ad94a7aba793263f279a3fb","265862f4f3cd4520a22d22f3de707862","f1796ef5f27d4a709b59f7a547ffb7f6","07f5e938e3b44a41b11846e0ee1e0890","c70f2d1886d04569bb700f24fd686ca3","0ecbfd96049c49f6887657c2561ab757","6dbfe84667bc4583a38cd9d4501f62cf","30925da280d64ee2a6d4bf92317989eb","33b787b6ae5b4449b1dee364d28eac2d","7b89de2cba5545d889113d1580e7ae1b","48b55747fad24d0ab1cbe61aa57d003b","40eab5db38294d288ffeb4789ab9fdea","e610c17f26b14c64ad0d9bc56301f245","8d835f209b394df0922f6ac4a501513f","8293b5175f9446ec9b40910b50147f41","8d93844bcae44d7ca7ac06adcb1e0323"]},"id":"H0TDkaA56dFJ","outputId":"87be4798-c7d0-44b8-91e9-b3de7cf2ba6f"},"source":["from transformers import BertForSequenceClassification, AdamW, BertConfig\n","\n","# Loads BertForSequenceClassification, the pretrained BERT model with a single \n","model = MultiClassClassifier('bert-base-uncased',num_classes, 768,500,140,dropout=0.1,freeze_bert=False)\n","\n","model.load_state_dict(torch.load(\"model_bert_skill_prediction/model_weights\"))\n","\n","# Tell pytorch to run this model on the GPU.\n","model.cuda()\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3f6743aa7ad94a7aba793263f279a3fb","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"33b787b6ae5b4449b1dee364d28eac2d","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["MultiClassClassifier(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (mlp): Sequential(\n","    (0): Linear(in_features=768, out_features=500, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=500, out_features=5, bias=True)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":57}]},{"cell_type":"code","metadata":{"id":"_g8cFpXrpo9Q"},"source":["from torch import nn\n","class MultiClassClassifier(nn.Module):\n","    def __init__(self, bert_model_path, labels_count,skill_label_count, hidden_dim=768, mlp_dim=500, extras_dim=140, dropout=0.1, freeze_bert=False):\n","        super().__init__()\n","\n","        self.bert = BertModel.from_pretrained(bert_model_path,output_hidden_states=False,output_attentions=False)\n","        self.dropout = nn.Dropout(dropout)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(hidden_dim , mlp_dim),\n","            nn.ReLU(),\n","            # nn.Linear(mlp_dim, mlp_dim),\n","            # nn.ReLU(),\n","            # nn.Linear(mlp_dim, mlp_dim),\n","            # nn.ReLU(),            \n","            nn.Linear(mlp_dim, labels_count)\n","        )\n","        self.mlp2 = nn.Sequential(\n","            nn.Linear(hidden_dim , mlp_dim),\n","            nn.ReLU(),         \n","            nn.Linear(mlp_dim, skill_label_count)\n","        )\n","        # self.softmax = nn.LogSoftmax(dim=1)\n","        if freeze_bert:\n","            print(\"Freezing layers\")\n","            for param in self.bert.parameters():\n","                param.requires_grad = False\n","\n","    def forward(self, tokens, masks):\n","        pooled_output = self.bert(tokens, attention_mask=masks)\n","        dropout_output = self.dropout(pooled_output['pooler_output'])\n","        concat_output = dropout_output\n","        mlp_output = self.mlp(concat_output)\n","        skill_output = self.mlp2(concat_output)\n","        # proba = self.sigmoid(mlp_output)\n","        # proba = self.softmax(mlp_output)\n","\n","        return mlp_output,skill_output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PzdVd6lUEfv8","outputId":"055803dc-9645-4207-c03d-2c7ce38f9c97"},"source":["num_classes"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5"]},"metadata":{"tags":[]},"execution_count":59}]},{"cell_type":"code","metadata":{"id":"kctN7UA1bhCx","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e3c57016-f4f8-44dc-ba06-dd056f548290"},"source":["model_multi_task = MultiClassClassifier('bert-base-uncased',3, 5,768,500,140,dropout=0.1,freeze_bert=False)\n","model_multi_task.load_state_dict(torch.load('model_bert_multi_task_prediction/model_weights'))\n","model_multi_task.cuda()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MultiClassClassifier(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (mlp): Sequential(\n","    (0): Linear(in_features=768, out_features=500, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=500, out_features=3, bias=True)\n","  )\n","  (mlp2): Sequential(\n","    (0): Linear(in_features=768, out_features=500, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=500, out_features=5, bias=True)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":60}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6RuioUjMvcHi","outputId":"d08542b4-78ce-462c-a793-8033e97d9f52"},"source":["model_cascade = MultiClassClassifier('bert-base-uncased',num_classes, 768,500,140,dropout=0.1,freeze_bert=False)\n","\n","model_cascade.load_state_dict(torch.load(\"model_bert_skill_cascade/model_weights\"))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":63}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sj4KoYVkNJm-","outputId":"99187dea-a57a-490a-81c2-c6f3522d51a6"},"source":["model_cascade.cuda()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MultiClassClassifier(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (mlp): Sequential(\n","    (0): Linear(in_features=768, out_features=500, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=500, out_features=5, bias=True)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":64}]},{"cell_type":"code","metadata":{"id":"6gtKYG0VeVwk"},"source":["# for param in model.bert.encoder.layer[0:12].parameters():\n","#     param.requires_grad=False\n","# for param in model.bert.embeddings.parameters():\n","#     param.requires_grad=False\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"awQ2Y9Jb3kht"},"source":["optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Ys-M4-e3khv"},"source":["\n","from transformers import get_linear_schedule_with_warmup\n","\n","\n","epochs = 30\n","\n","# Total number of training steps is [number of batches] x [number of epochs]. \n","total_steps = len(train_dataloader) * epochs\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QrYqErOD3khx","outputId":"b1701de8-1fd6-4e24-dde0-d21933b854ec"},"source":["len(train_dataloader) "],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1223"]},"metadata":{"tags":[]},"execution_count":68}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EWVSE9LM3kh0","outputId":"5d2e945d-221b-4e3f-85f0-be021d195a36"},"source":["1935 * 32"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["61920"]},"metadata":{"tags":[]},"execution_count":69}]},{"cell_type":"code","metadata":{"id":"rcvxVVi63kh3"},"source":["scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GUw3zm6g3kh5"},"source":["import numpy as np\n","\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ta6zfUTa3kh7"},"source":["import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GFq9gd5kQSHb"},"source":["import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9AjZgn3fwTmX"},"source":["class EarlyStopping:\n","    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n","    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n","        \"\"\"\n","        Args:\n","            patience (int): How long to wait after last time validation loss improved.\n","                            Default: 7\n","            verbose (bool): If True, prints a message for each validation loss improvement. \n","                            Default: False\n","            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n","                            Default: 0\n","            path (str): Path for the checkpoint to be saved to.\n","                            Default: 'checkpoint.pt'\n","            trace_func (function): trace print function.\n","                            Default: print            \n","        \"\"\"\n","        self.patience = patience\n","        self.verbose = verbose\n","        self.counter = 0\n","        self.best_score = None\n","        self.early_stop = False\n","        self.val_loss_min = np.Inf\n","        self.delta = delta\n","        self.path = path\n","        self.trace_func = trace_func\n","    def __call__(self, val_loss, model):\n","\n","        score = -val_loss\n","\n","        if self.best_score is None:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","        elif score < self.best_score + self.delta:\n","            self.counter += 1\n","            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","            self.counter = 0\n","\n","    def save_checkpoint(self, val_loss, model):\n","        '''Saves model when validation loss decrease.'''\n","        if self.verbose:\n","            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n","        torch.save(model.state_dict(), self.path)\n","        self.val_loss_min = val_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-qclnCpSZb2O"},"source":["loss_func = nn.CrossEntropyLoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":402},"id":"FcSwwzAFyE7S","outputId":"d5dfe91f-4c2a-4eae-9c28-912fcfd1e677"},"source":["test"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>board_syllabus</th>\n","      <th>question_answer</th>\n","      <th>skill_label</th>\n","      <th>difficulty_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>CBSE&gt;&gt;Nursery&gt;&gt;Environmental Science&gt;&gt;Common V...</td>\n","      <td>Write down the names of some common vegetable...</td>\n","      <td>3</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>CBSE&gt;&gt;XII&gt;&gt;Physics&gt;&gt;Physics : Part - II&gt;&gt;Atoms</td>\n","      <td>Name the series of hydrogen atom which lies i...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ICSE OLD&gt;&gt;XI&gt;&gt;Political Science&gt;&gt;State, Govern...</td>\n","      <td>Which of the following is not the element of ...</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Maharashtra New&gt;&gt;VII&gt;&gt;General Science&gt;&gt;Static ...</td>\n","      <td>The process of electrically charging an objec...</td>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Tamil Nadu&gt;&gt;IX&gt;&gt;Science&gt;&gt;Physics&gt;&gt;Measurement&gt;...</td>\n","      <td>The mass of an object is measured in kilogram...</td>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>4572</th>\n","      <td>ICSE OLD&gt;&gt;VIII&gt;&gt;Biology&gt;&gt;Nervous System And Se...</td>\n","      <td>Which of the following is the first cranial n...</td>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4573</th>\n","      <td>CBSE&gt;&gt;VII&gt;&gt;Computer Science&gt;&gt;Advance features ...</td>\n","      <td>To ungroup the worksheets: Right-click on any...</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4574</th>\n","      <td>CBSE&gt;&gt;VIII&gt;&gt;Science&gt;&gt;Chemical Effects of Elect...</td>\n","      <td>After passing electricity through a solution ...</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4575</th>\n","      <td>CLSP&gt;&gt;Stage 9&gt;&gt;Science&gt;&gt;Chemistry&gt;&gt;Material pr...</td>\n","      <td>Identify the scientists who gave the “plum-pu...</td>\n","      <td>3</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4576</th>\n","      <td>AP&gt;&gt;VIII&gt;&gt;Physical Science&gt;&gt;Physical Science (...</td>\n","      <td>What do you understand by the term static ele...</td>\n","      <td>2</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4577 rows × 4 columns</p>\n","</div>"],"text/plain":["                                         board_syllabus  ... difficulty_label\n","0     CBSE>>Nursery>>Environmental Science>>Common V...  ...                2\n","1        CBSE>>XII>>Physics>>Physics : Part - II>>Atoms  ...                1\n","2     ICSE OLD>>XI>>Political Science>>State, Govern...  ...                0\n","3     Maharashtra New>>VII>>General Science>>Static ...  ...                1\n","4     Tamil Nadu>>IX>>Science>>Physics>>Measurement>...  ...                1\n","...                                                 ...  ...              ...\n","4572  ICSE OLD>>VIII>>Biology>>Nervous System And Se...  ...                1\n","4573  CBSE>>VII>>Computer Science>>Advance features ...  ...                0\n","4574  CBSE>>VIII>>Science>>Chemical Effects of Elect...  ...                2\n","4575  CLSP>>Stage 9>>Science>>Chemistry>>Material pr...  ...                0\n","4576  AP>>VIII>>Physical Science>>Physical Science (...  ...                1\n","\n","[4577 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":76}]},{"cell_type":"code","metadata":{"id":"4178_yLFMWmx"},"source":["test_features = test[\"question_answer\"].values\n","test_labels = test[\"skill_label\"].values\n","test_skill_labels = test[\"skill_label\"].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DggP9Sxdv_-l","outputId":"1f2b961b-3eab-41fe-f529-fdadd83d18ff"},"source":["\n","\n","\n","\n","test_labels"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([3, 1, 0, ..., 0, 3, 2])"]},"metadata":{"tags":[]},"execution_count":78}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3ZpmBJuIC2nM","outputId":"d0a5cece-0520-4a3b-ee7e-22def4eb96e9"},"source":["test_features"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([' Write down the names of some common vegetables. Answer may vary Probable answer – Names of some common vegetables are potato, onion, tomato and carrot. ',\n","       ' Name the series of hydrogen atom which lies in the U.V region. Lyman series lies in the U.V region. ',\n","       ' Which of the following is not the element of an association? Relationships are abstract Each association has elements 1) a group of people, 2) voluntary Membership, 3) shared and common interests or needs as the basis, 4) some set of objective goals which are to be achieved collectively by all the members, 5) a voluntary organisation, and 6) co-operation among the members objectives pooled resources for funds needed for action.',\n","       ...,\n","       ' After passing electricity through a solution the change that takes place is a chemical change Physical changes are those changes that do not result in the production of a new substance whereas a chemical change involves the production of a new substance. For example: When electricity is passed in a solution containing water, bubbles of hydrogen and oxygen are formed.',\n","       ' Identify the scientists who gave the “plum-pudding” model of the atom. J. J. Thomson The &ldquo;plum-pudding&rdquo; model of the atom was developed by the J. J Thomson. He compared his model used for describing the structure of atom with a plum pudding in which negatively charged electrons were surrounded by a positively charged &lsquo;pudding&rsquo;.',\n","       ' What do you understand by the term static electricity? It is the electricity developed due to stationary electric charges on the body. '],\n","      dtype=object)"]},"metadata":{"tags":[]},"execution_count":79}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"MlOvANUwprAw","outputId":"595a3b07-d66a-45b2-9929-6c06bdb74f77"},"source":["test_features[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["' Write down the names of some common vegetables. Answer may vary Probable answer – Names of some common vegetables are potato, onion, tomato and carrot. '"]},"metadata":{"tags":[]},"execution_count":80}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LmmWYcW2sNHe","outputId":"b457f1f8-2ed3-4e3f-bd46-7391fc8d3a72"},"source":["input_ids = []\n","attention_masks = []\n","for sent in test_features:\n","\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 256,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        truncation=True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","test_labels = torch.tensor(test_labels)\n","test_skill_labels = torch.tensor(test_skill_labels)\n","\n","# Set the batch size.  \n","batch_size = 32  \n","\n","# test_poincare_tensor = torch.tensor(poincare_embeddings_final,dtype=torch.float)\n","# print(test_poincare_tensor.shape)\n","# difficulty_tensor = torch.tensor(difficulty_level_vectors,dtype=torch.float)\n","# print(\"difficulty_tensor\",difficulty_tensor.shape)\n","# Combine the training inputs into a TensorDataset.\n","prediction_data = TensorDataset(input_ids, attention_masks, test_labels,test_skill_labels)\n","# Create the DataLoader.\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2155: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KkLhcFdI9doK","outputId":"db5f1598-c4b6-4da8-bdd0-9241abb52e99"},"source":["\n","\n","!pip install ax-platform==0.1.9"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting ax-platform==0.1.9\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/e5/defa97540bf23447f15d142a644eed9a9d9fd1925cf1e3c4f47a49282ec0/ax_platform-0.1.9-py3-none-any.whl (499kB)\n","\u001b[K     |████████████████████████████████| 501kB 7.7MB/s \n","\u001b[?25hRequirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from ax-platform==0.1.9) (4.4.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from ax-platform==0.1.9) (1.1.5)\n","Collecting botorch==0.2.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/e4/d696b12a84d505e9592fb6f8458a968b19efc22e30cc517dd2d2817e27e4/botorch-0.2.1-py3-none-any.whl (221kB)\n","\u001b[K     |████████████████████████████████| 225kB 15.0MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from ax-platform==0.1.9) (1.4.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from ax-platform==0.1.9) (2.11.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from ax-platform==0.1.9) (0.22.2.post1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from plotly->ax-platform==0.1.9) (1.15.0)\n","Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->ax-platform==0.1.9) (1.3.3)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->ax-platform==0.1.9) (2.8.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->ax-platform==0.1.9) (2018.9)\n","Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas->ax-platform==0.1.9) (1.19.5)\n","Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.6/dist-packages (from botorch==0.2.1->ax-platform==0.1.9) (1.7.0+cu101)\n","Collecting gpytorch>=1.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/99/5724d11056e703f1d8e6ecaa6153991eb3b5b014affbeafa2901a522fe46/gpytorch-1.3.1.tar.gz (283kB)\n","\u001b[K     |████████████████████████████████| 286kB 15.8MB/s \n","\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->ax-platform==0.1.9) (1.1.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->ax-platform==0.1.9) (1.0.0)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.1->botorch==0.2.1->ax-platform==0.1.9) (0.8)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.1->botorch==0.2.1->ax-platform==0.1.9) (3.7.4.3)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.1->botorch==0.2.1->ax-platform==0.1.9) (0.16.0)\n","Building wheels for collected packages: gpytorch\n","  Building wheel for gpytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gpytorch: filename=gpytorch-1.3.1-py2.py3-none-any.whl size=474648 sha256=8a6626dd15e1155c939af85d5f2d70ddccea485abebf1ffae1be55465d4aac06\n","  Stored in directory: /root/.cache/pip/wheels/74/8d/e0/a2fefbbe64dbbcbf79bccc3385baa67b693ae4f8bb86306214\n","Successfully built gpytorch\n","Installing collected packages: gpytorch, botorch, ax-platform\n","Successfully installed ax-platform-0.1.9 botorch-0.2.1 gpytorch-1.3.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"G0H4JBrl-FnZ"},"source":["from ax import optimize"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SX-RiYbQPDpx"},"source":["from sklearn.metrics import precision_recall_fscore_support\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zFediYEjlKjX"},"source":["def get_confusion_matrix(predicted,actual):\n","    conf_matrix = np.zeros((5, 5))\n","    for pred,act in zip(predicted,actual):\n","        conf_matrix[act,pred]+=1\n","    return conf_matrix\n","        \n","def get_TP(confusion_matrix,label):\n","    tp = confusion_matrix[label][label]\n","    return tp\n","\n","def get_FN(confusion_matrix,label):\n","    row = confusion_matrix[label,]\n","    row_truepositives = row[label]\n","    fn = row.sum() - row_truepositives\n","    return fn\n","\n","def get_FP(confusion_matrix,tag):\n","    col = confusion_matrix[:,tag]\n","    col_tp = col[tag]\n","    #  sum of all values in column except tp\n","    fp = col.sum() - col_tp\n","    return fp\n","def Precision(conf_matrix):\n","    precision = 0.0\n","    for label in [0,1,2,3,4]:\n","        dividor= get_TP(conf_matrix,label)+get_FP(conf_matrix,label)\n","        if dividor != 0.0:\n","            precision += (get_TP(conf_matrix,label))/dividor\n","    return (precision / 5)\n","\n","def Recall(conf_matrix):\n","    recall = 0.0\n","    for label in [0,1,2,3,4]:\n","        dividor=get_TP(conf_matrix,label)+get_FN(conf_matrix,label)\n","        if dividor != 0.0:\n","            recall += (get_TP(conf_matrix,label))/dividor\n","    return (recall / 5)\n","\n","def F1(precision,recall):\n","    return (2*precision*recall)/(precision+recall)\n","def accuracy_per_class(preds_flat, labels_flat):\n","\n","    for label in np.unique(labels_flat):\n","        y_preds = preds_flat[labels_flat==label]\n","        y_true = labels_flat[labels_flat==label]\n","        print(f'Class: {label}')\n","        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')\n","def print_metrics(predictions,test_labels):\n","    conf_matrix = get_confusion_matrix(predictions,test_labels)\n","    precision = Precision(conf_matrix)\n","    recall = Recall(conf_matrix)\n","    f1_score = F1(precision,recall)\n","    return (precision,recall,f1_score)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aFEi23ejPfOm"},"source":["\n","def Precision_macro_weighted(conf_matrix,test_samples):\n","    accum =0\n","    label_wise_precision = dict()\n","    for label in [0,1,2,3,4]:\n","        true_sample = [sample for sample in test_samples if sample==label ]\n","        if (get_TP(conf_matrix,label)+get_FP(conf_matrix,label))!=0:\n","            accum+= float(len(true_sample)) *(get_TP(conf_matrix,label)/(get_TP(conf_matrix,label)+get_FP(conf_matrix,label)))\n","            label_wise_precision[label] = get_TP(conf_matrix,label)/(get_TP(conf_matrix,label)+get_FP(conf_matrix,label))\n","\n","    \n","    precision =  accum/len(test_samples)\n","            \n","    return precision\n","\n","\n","def Recall_macro_weighted(conf_matrix,test_samples):\n","    accum =0\n","    label_wise_recall = dict()\n","    for label in [0,1,2,3,4]:\n","        true_sample = [sample for sample in test_samples if sample==label ]\n","\n","        if (get_TP(conf_matrix,label)+get_FN(conf_matrix,label))!=0:\n","            accum+= float(len(true_sample)) * (get_TP(conf_matrix,label)/(get_TP(conf_matrix,label)+get_FN(conf_matrix,label)))\n","            label_wise_recall[label] = get_TP(conf_matrix,label)/(get_TP(conf_matrix,label)+get_FN(conf_matrix,label))\n","\n","    \n","    recall =  accum/len(test_samples)\n","    return recall\n","def print_weighted_metrics(predictions,test_labels):\n","    conf_matrix = get_confusion_matrix(predictions,test_labels)\n","    precision = Precision_macro_weighted(conf_matrix,test_labels)\n","    recall = Recall_macro_weighted(conf_matrix,test_labels)\n","    f1_score = F1(precision,recall)\n","    return (precision,recall,f1_score)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9F7JWv8uFrL-"},"source":["def make_lstm_and_gru_predictions(index, params):\n","  model_lstm.eval()\n","  model_gru.eval()\n","  for i,batch in enumerate(test_iter):\n","    outputs = []\n","    if i == index:\n","      with torch.no_grad():\n","          question, x_len = batch.text\n","          x = question.cuda()\n","          # outs = sigmoid(outs.cpu().data.numpy()).tolist()\n","          y = batch.label.type(torch.long).cuda()\n","          if params['lstm']>=0.5:\n","            lstm_outputs = model_lstm(x,x_len)\n","            outputs.append(lstm_outputs)\n","          if params[\"gru\"]>=0.5:\n","            gru_outputs = model_gru(x,x_len)\n","            outputs.append(gru_outputs)\n","\n","    return outputs\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZqKPe4ZN6vPY"},"source":["def make_predictions(params):\n","  # Prediction on test set\n","    print(params)\n","    print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n","\n","    # Put model in evaluation mode\n","    model.eval()\n","    model_multi_task.eval()\n","    model_cascade.eval()\n","\n","    # Tracking variables \n","    predictions , true_labels,ids = [], [], []\n","\n","\n","    # Predict \n","    for index,batch in enumerate(prediction_dataloader):\n","      final_outputs = []\n","      # Add batch to GPU\n","      batch = tuple(t.to(device) for t in batch)\n","      \n","      # Unpack the inputs from our dataloader\n","      b_input_ids, b_input_mask, b_labels, id = batch\n","      # Telling the model not to compute or store gradients, saving memory and \n","      # speeding up prediction\n","      with torch.no_grad():\n","          # Forward pass, calculate logit predictions\n","          if params['multi_task']>=0.5:\n","            # print(\"multi\")\n","            _,outputs = model_multi_task(b_input_ids,b_input_mask)\n","            final_outputs.append(outputs)\n","          if params['cascade']>=0.5:\n","            # print(\"cascade\")\n","            output_cascade = model_cascade(b_input_ids,b_input_mask)\n","            final_outputs.append(output_cascade)\n","          if params['difficulty'] >=0:\n","            # print(\"normal\")\n","            output_difficulty = model(b_input_ids,b_input_mask)\n","            final_outputs.append(output_difficulty)\n","\n","          out = make_lstm_and_gru_predictions(index,params)\n","          if len(out)>0:\n","            final_outputs.append(out[0])\n","          if len(out) >1:\n","            final_outputs.append(out[1])\n","      # logits_2 = outputs\n","      # logist_1 = output_bert[0]\n","      predictions_1 = final_outputs\n","      logits = torch.mean(torch.stack(predictions_1), dim=0)\n","      # else:\n","        # logits = predictions_1[0]\n","      # Move logits and labels to CPU\n","      logits = logits.detach().cpu().numpy()\n","      label_ids = b_labels.to('cpu').numpy()\n","\n","      \n","      # Store predictions and true labels\n","      predictions.append(logits)\n","      true_labels.append(label_ids)\n","    flat_predictions = np.concatenate(predictions, axis=0)\n","\n","    flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","\n","    # Combine the correct labels for each batch into a single list.\n","    flat_true_labels = np.concatenate(true_labels, axis=0)\n","    # metrics = precision_recall_fscore_support(flat_true_labels, flat_predictions, average='micro')\n","    metrics = print_metrics(flat_predictions,flat_true_labels)\n","    print(accuracy_per_class(flat_predictions,flat_true_labels))\n","    print(metrics)\n","    print('    DONE.')\n","    return metrics[2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bD8aNLwc6sgM","outputId":"539cb9d2-9a6d-4f27-943b-3ba58c1d3db9"},"source":["import torch.nn.functional as F\n","from ax.modelbridge.generation_strategy import GenerationStep, GenerationStrategy\n","from ax.modelbridge.registry import Models\n","\n","best_parameters, best_values, experiment, model_100 = optimize(\n","        parameters=[\n","          {\n","            \"name\": \"multi_task\",\n","            \"type\": \"range\",\n","            \"bounds\": [1,2],\n","          },\n","          {\n","            \"name\": \"cascade\",\n","            \"type\": \"range\",\n","            \"bounds\": [0,1],\n","          },\n","               {\n","            \"name\": \"lstm\",\n","            \"type\": \"range\",\n","            \"bounds\": [0,1],\n","          },\n","          {\n","            \"name\": \"gru\",\n","            \"type\": \"range\",\n","            \"bounds\": [0,1],\n","          },\n","           {\n","            \"name\": \"difficulty\",\n","            \"type\": \"range\",\n","            \"bounds\": [0,1],\n","          },\n","        ],\n","        # Booth function\n","        evaluation_function=make_predictions,\n","        generation_strategy = GenerationStrategy(name=\"Sobol+GPEI\", steps=[GenerationStep(model=Models.SOBOL, num_arms=10),\n","                GenerationStep(model=Models.GPEI, num_arms=12)]),\n","        minimize=False,\n","    )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[INFO 01-20 13:40:55] ax.service.managed_loop: Started full optimization with 20 steps.\n","[INFO 01-20 13:40:55] ax.service.managed_loop: Running optimization trial 1...\n"],"name":"stderr"},{"output_type":"stream","text":["{'multi_task': 2, 'cascade': 0, 'lstm': 1, 'gru': 0, 'difficulty': 0}\n","Predicting labels for 4,577 test sentences...\n"],"name":"stdout"},{"output_type":"stream","text":["[INFO 01-20 13:45:33] ax.service.managed_loop: Running optimization trial 2...\n"],"name":"stderr"},{"output_type":"stream","text":["Class: Analysing\n","Accuracy: 109/404\n","\n","Class: Applying\n","Accuracy: 185/602\n","\n","Class: Knowledge & understanding\n","Accuracy: 634/933\n","\n","Class: Remembering\n","Accuracy: 819/1356\n","\n","Class: Understanding\n","Accuracy: 589/1282\n","\n","None\n","(0.4941818825312142, 0.46401200634376333, 0.47862197718934923)\n","    DONE.\n","{'multi_task': 2, 'cascade': 1, 'lstm': 1, 'gru': 0, 'difficulty': 1}\n","Predicting labels for 4,577 test sentences...\n"],"name":"stdout"},{"output_type":"stream","text":["[INFO 01-20 13:52:28] ax.service.managed_loop: Running optimization trial 3...\n"],"name":"stderr"},{"output_type":"stream","text":["Class: Analysing\n","Accuracy: 106/404\n","\n","Class: Applying\n","Accuracy: 196/602\n","\n","Class: Knowledge & understanding\n","Accuracy: 613/933\n","\n","Class: Remembering\n","Accuracy: 836/1356\n","\n","Class: Understanding\n","Accuracy: 587/1282\n","\n","None\n","(0.4989497174670675, 0.4638750973124732, 0.48077354299818803)\n","    DONE.\n","{'multi_task': 1, 'cascade': 0, 'lstm': 0, 'gru': 1, 'difficulty': 1}\n","Predicting labels for 4,577 test sentences...\n"],"name":"stdout"},{"output_type":"stream","text":["[INFO 01-20 13:57:06] ax.service.managed_loop: Running optimization trial 4...\n"],"name":"stderr"},{"output_type":"stream","text":["Class: Analysing\n","Accuracy: 109/404\n","\n","Class: Applying\n","Accuracy: 185/602\n","\n","Class: Knowledge & understanding\n","Accuracy: 634/933\n","\n","Class: Remembering\n","Accuracy: 820/1356\n","\n","Class: Understanding\n","Accuracy: 590/1282\n","\n","None\n","(0.4944645160508415, 0.46431550520938164, 0.47891599008600194)\n","    DONE.\n","{'multi_task': 1, 'cascade': 1, 'lstm': 1, 'gru': 0, 'difficulty': 0}\n","Predicting labels for 4,577 test sentences...\n"],"name":"stdout"},{"output_type":"stream","text":["[INFO 01-20 14:04:01] ax.service.managed_loop: Running optimization trial 5...\n"],"name":"stderr"},{"output_type":"stream","text":["Class: Analysing\n","Accuracy: 106/404\n","\n","Class: Applying\n","Accuracy: 196/602\n","\n","Class: Knowledge & understanding\n","Accuracy: 613/933\n","\n","Class: Remembering\n","Accuracy: 836/1356\n","\n","Class: Understanding\n","Accuracy: 587/1282\n","\n","None\n","(0.49895235095650897, 0.4638750973124732, 0.48077476555192256)\n","    DONE.\n","{'multi_task': 2, 'cascade': 1, 'lstm': 0, 'gru': 1, 'difficulty': 0}\n","Predicting labels for 4,577 test sentences...\n"],"name":"stdout"},{"output_type":"stream","text":["[INFO 01-20 14:10:57] ax.service.managed_loop: Running optimization trial 6...\n"],"name":"stderr"},{"output_type":"stream","text":["Class: Analysing\n","Accuracy: 106/404\n","\n","Class: Applying\n","Accuracy: 195/602\n","\n","Class: Knowledge & understanding\n","Accuracy: 613/933\n","\n","Class: Remembering\n","Accuracy: 837/1356\n","\n","Class: Understanding\n","Accuracy: 586/1282\n","\n","None\n","(0.498515232312102, 0.463534357783971, 0.4803888290877835)\n","    DONE.\n","{'multi_task': 2, 'cascade': 1, 'lstm': 0, 'gru': 0, 'difficulty': 0}\n","Predicting labels for 4,577 test sentences...\n"],"name":"stdout"},{"output_type":"stream","text":["[INFO 01-20 14:17:53] ax.service.managed_loop: Running optimization trial 7...\n"],"name":"stderr"},{"output_type":"stream","text":["Class: Analysing\n","Accuracy: 106/404\n","\n","Class: Applying\n","Accuracy: 196/602\n","\n","Class: Knowledge & understanding\n","Accuracy: 613/933\n","\n","Class: Remembering\n","Accuracy: 837/1356\n","\n","Class: Understanding\n","Accuracy: 586/1282\n","\n","None\n","(0.4988289369744159, 0.46386658369759226, 0.48071289390087196)\n","    DONE.\n","{'multi_task': 1, 'cascade': 1, 'lstm': 1, 'gru': 0, 'difficulty': 1}\n","Predicting labels for 4,577 test sentences...\n"],"name":"stdout"},{"output_type":"stream","text":["[INFO 01-20 14:24:49] ax.service.managed_loop: Running optimization trial 8...\n"],"name":"stderr"},{"output_type":"stream","text":["Class: Analysing\n","Accuracy: 106/404\n","\n","Class: Applying\n","Accuracy: 195/602\n","\n","Class: Knowledge & understanding\n","Accuracy: 613/933\n","\n","Class: Remembering\n","Accuracy: 837/1356\n","\n","Class: Understanding\n","Accuracy: 587/1282\n","\n","None\n","(0.4987662606617568, 0.4636903640242206, 0.48058915703283195)\n","    DONE.\n","{'multi_task': 2, 'cascade': 1, 'lstm': 0, 'gru': 1, 'difficulty': 0}\n","Predicting labels for 4,577 test sentences...\n"],"name":"stdout"},{"output_type":"stream","text":["[INFO 01-20 14:31:47] ax.service.managed_loop: Running optimization trial 9...\n"],"name":"stderr"},{"output_type":"stream","text":["Class: Analysing\n","Accuracy: 106/404\n","\n","Class: Applying\n","Accuracy: 196/602\n","\n","Class: Knowledge & understanding\n","Accuracy: 614/933\n","\n","Class: Remembering\n","Accuracy: 837/1356\n","\n","Class: Understanding\n","Accuracy: 586/1282\n","\n","None\n","(0.49896975789107056, 0.4640809459698324, 0.4808933866598818)\n","    DONE.\n","{'multi_task': 2, 'cascade': 1, 'lstm': 1, 'gru': 1, 'difficulty': 1}\n","Predicting labels for 4,577 test sentences...\n"],"name":"stdout"},{"output_type":"stream","text":["[INFO 01-20 14:38:45] ax.service.managed_loop: Running optimization trial 10...\n"],"name":"stderr"},{"output_type":"stream","text":["Class: Analysing\n","Accuracy: 106/404\n","\n","Class: Applying\n","Accuracy: 196/602\n","\n","Class: Knowledge & understanding\n","Accuracy: 614/933\n","\n","Class: Remembering\n","Accuracy: 836/1356\n","\n","Class: Understanding\n","Accuracy: 585/1282\n","\n","None\n","(0.4987260150560669, 0.4637774471042141, 0.48061723860821526)\n","    DONE.\n","{'multi_task': 1, 'cascade': 1, 'lstm': 1, 'gru': 1, 'difficulty': 0}\n","Predicting labels for 4,577 test sentences...\n"],"name":"stdout"},{"output_type":"stream","text":["[INFO 01-20 14:45:42] ax.service.managed_loop: Running optimization trial 11...\n"],"name":"stderr"},{"output_type":"stream","text":["Class: Analysing\n","Accuracy: 106/404\n","\n","Class: Applying\n","Accuracy: 195/602\n","\n","Class: Knowledge & understanding\n","Accuracy: 613/933\n","\n","Class: Remembering\n","Accuracy: 835/1356\n","\n","Class: Understanding\n","Accuracy: 585/1282\n","\n","None\n","(0.4976730334144321, 0.4630833662929839, 0.47975554198125614)\n","    DONE.\n","{'multi_task': 2, 'cascade': 1, 'lstm': 1, 'gru': 0, 'difficulty': 1}\n","Predicting labels for 4,577 test sentences...\n"],"name":"stdout"},{"output_type":"stream","text":["[INFO 01-20 14:52:40] ax.service.managed_loop: Running optimization trial 12...\n"],"name":"stderr"},{"output_type":"stream","text":["Class: Analysing\n","Accuracy: 106/404\n","\n","Class: Applying\n","Accuracy: 196/602\n","\n","Class: Knowledge & understanding\n","Accuracy: 613/933\n","\n","Class: Remembering\n","Accuracy: 836/1356\n","\n","Class: Understanding\n","Accuracy: 587/1282\n","\n","None\n","(0.49888771441493585, 0.4638750973124732, 0.48074475717857085)\n","    DONE.\n","{'multi_task': 2, 'cascade': 1, 'lstm': 1, 'gru': 0, 'difficulty': 1}\n","Predicting labels for 4,577 test sentences...\n"],"name":"stdout"},{"output_type":"stream","text":["[INFO 01-20 14:59:38] ax.service.managed_loop: Running optimization trial 13...\n"],"name":"stderr"},{"output_type":"stream","text":["Class: Analysing\n","Accuracy: 106/404\n","\n","Class: Applying\n","Accuracy: 195/602\n","\n","Class: Knowledge & understanding\n","Accuracy: 613/933\n","\n","Class: Remembering\n","Accuracy: 837/1356\n","\n","Class: Understanding\n","Accuracy: 585/1282\n","\n","None\n","(0.49829792419232566, 0.4633783515437214, 0.4802041529269747)\n","    DONE.\n","{'multi_task': 1, 'cascade': 1, 'lstm': 1, 'gru': 0, 'difficulty': 0}\n","Predicting labels for 4,577 test sentences...\n"],"name":"stdout"},{"output_type":"stream","text":["[INFO 01-20 15:06:36] ax.service.managed_loop: Running optimization trial 14...\n"],"name":"stderr"},{"output_type":"stream","text":["Class: Analysing\n","Accuracy: 106/404\n","\n","Class: Applying\n","Accuracy: 196/602\n","\n","Class: Knowledge & understanding\n","Accuracy: 613/933\n","\n","Class: Remembering\n","Accuracy: 837/1356\n","\n","Class: Understanding\n","Accuracy: 585/1282\n","\n","None\n","(0.4986432712937237, 0.4637105774573427, 0.4805429096104601)\n","    DONE.\n","{'multi_task': 2, 'cascade': 1, 'lstm': 0, 'gru': 0, 'difficulty': 0}\n","Predicting labels for 4,577 test sentences...\n"],"name":"stdout"},{"output_type":"stream","text":["[INFO 01-20 15:13:34] ax.service.managed_loop: Running optimization trial 15...\n"],"name":"stderr"},{"output_type":"stream","text":["Class: Analysing\n","Accuracy: 106/404\n","\n","Class: Applying\n","Accuracy: 196/602\n","\n","Class: Knowledge & understanding\n","Accuracy: 613/933\n","\n","Class: Remembering\n","Accuracy: 837/1356\n","\n","Class: Understanding\n","Accuracy: 586/1282\n","\n","None\n","(0.4988289369744159, 0.46386658369759226, 0.48071289390087196)\n","    DONE.\n","{'multi_task': 2, 'cascade': 1, 'lstm': 0, 'gru': 0, 'difficulty': 0}\n","Predicting labels for 4,577 test sentences...\n"],"name":"stdout"},{"output_type":"stream","text":["[INFO 01-20 15:20:32] ax.service.managed_loop: Running optimization trial 16...\n"],"name":"stderr"},{"output_type":"stream","text":["Class: Analysing\n","Accuracy: 106/404\n","\n","Class: Applying\n","Accuracy: 196/602\n","\n","Class: Knowledge & understanding\n","Accuracy: 613/933\n","\n","Class: Remembering\n","Accuracy: 837/1356\n","\n","Class: Understanding\n","Accuracy: 586/1282\n","\n","None\n","(0.4988289369744159, 0.46386658369759226, 0.48071289390087196)\n","    DONE.\n","{'multi_task': 2, 'cascade': 1, 'lstm': 0, 'gru': 0, 'difficulty': 0}\n","Predicting labels for 4,577 test sentences...\n"],"name":"stdout"},{"output_type":"stream","text":["[INFO 01-20 15:27:30] ax.service.managed_loop: Running optimization trial 17...\n"],"name":"stderr"},{"output_type":"stream","text":["Class: Analysing\n","Accuracy: 106/404\n","\n","Class: Applying\n","Accuracy: 196/602\n","\n","Class: Knowledge & understanding\n","Accuracy: 613/933\n","\n","Class: Remembering\n","Accuracy: 837/1356\n","\n","Class: Understanding\n","Accuracy: 586/1282\n","\n","None\n","(0.4988289369744159, 0.46386658369759226, 0.48071289390087196)\n","    DONE.\n","{'multi_task': 2, 'cascade': 1, 'lstm': 0, 'gru': 0, 'difficulty': 0}\n","Predicting labels for 4,577 test sentences...\n"],"name":"stdout"},{"output_type":"stream","text":["[INFO 01-20 15:34:26] ax.service.managed_loop: Running optimization trial 18...\n"],"name":"stderr"},{"output_type":"stream","text":["Class: Analysing\n","Accuracy: 106/404\n","\n","Class: Applying\n","Accuracy: 196/602\n","\n","Class: Knowledge & understanding\n","Accuracy: 613/933\n","\n","Class: Remembering\n","Accuracy: 837/1356\n","\n","Class: Understanding\n","Accuracy: 586/1282\n","\n","None\n","(0.4988289369744159, 0.46386658369759226, 0.48071289390087196)\n","    DONE.\n","{'multi_task': 2, 'cascade': 1, 'lstm': 0, 'gru': 0, 'difficulty': 0}\n","Predicting labels for 4,577 test sentences...\n"],"name":"stdout"},{"output_type":"stream","text":["[INFO 01-20 15:41:22] ax.service.managed_loop: Running optimization trial 19...\n"],"name":"stderr"},{"output_type":"stream","text":["Class: Analysing\n","Accuracy: 106/404\n","\n","Class: Applying\n","Accuracy: 196/602\n","\n","Class: Knowledge & understanding\n","Accuracy: 613/933\n","\n","Class: Remembering\n","Accuracy: 837/1356\n","\n","Class: Understanding\n","Accuracy: 586/1282\n","\n","None\n","(0.4988289369744159, 0.46386658369759226, 0.48071289390087196)\n","    DONE.\n","{'multi_task': 2, 'cascade': 1, 'lstm': 0, 'gru': 0, 'difficulty': 0}\n","Predicting labels for 4,577 test sentences...\n"],"name":"stdout"},{"output_type":"stream","text":["[INFO 01-20 15:48:18] ax.service.managed_loop: Running optimization trial 20...\n"],"name":"stderr"},{"output_type":"stream","text":["Class: Analysing\n","Accuracy: 106/404\n","\n","Class: Applying\n","Accuracy: 196/602\n","\n","Class: Knowledge & understanding\n","Accuracy: 613/933\n","\n","Class: Remembering\n","Accuracy: 837/1356\n","\n","Class: Understanding\n","Accuracy: 586/1282\n","\n","None\n","(0.4988289369744159, 0.46386658369759226, 0.48071289390087196)\n","    DONE.\n","{'multi_task': 2, 'cascade': 1, 'lstm': 0, 'gru': 0, 'difficulty': 0}\n","Predicting labels for 4,577 test sentences...\n","Class: Analysing\n","Accuracy: 106/404\n","\n","Class: Applying\n","Accuracy: 196/602\n","\n","Class: Knowledge & understanding\n","Accuracy: 613/933\n","\n","Class: Remembering\n","Accuracy: 837/1356\n","\n","Class: Understanding\n","Accuracy: 586/1282\n","\n","None\n","(0.4988289369744159, 0.46386658369759226, 0.48071289390087196)\n","    DONE.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"m5i0-xVz5iEH"},"source":["from torch import nn\n","class MLPStackedEnsemble(nn.Module):\n","  def __init__(self,hidden_dim=5,dropout=0.2):\n","    super(MLPStackedEnsemble, self).__init__()\n","\n","    self.mlp = nn.Sequential(\n","        nn.Linear(hidden_dim, hidden_dim),\n","        nn.Dropout(p=dropout),\n","\n","        nn.ReLU(),\n","        nn.Linear(hidden_dim,hidden_dim),\n","        nn.ReLU()\n","    )\n","    self.output = nn.Linear(hidden_dim,5)\n","  def forward(self,input):\n","    output = self.mlp(input)\n","    final_out = self.output(output)\n","    return final_out\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QkNckKJj6r9h"},"source":["!cp -r \"/content/drive/MyDrive/research_skill_name_prediction/model_MLP_skill_ensemble_latest\" /content"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TBqbaDo11IZc","outputId":"44b37508-4046-4caf-eba4-ae24426846e0"},"source":["from sklearn.model_selection import KFold\n","kf = KFold(n_splits=5, shuffle=True)\n","kf.split(test_features)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<generator object _BaseKFold.split at 0x7f971877b888>"]},"metadata":{"tags":[]},"execution_count":90}]},{"cell_type":"code","metadata":{"id":"k6Fk2gObC8iw"},"source":["def get_bo_predictions(prediction_dataloader):\n","  predictions = []\n","  true_labels = []\n","  for index,batch in enumerate(prediction_dataloader):\n","      final_outputs = []\n","      # Add batch to GPU\n","      batch = tuple(t.to(device) for t in batch)\n","      \n","      # Unpack the inputs from our dataloader\n","      b_input_ids, b_input_mask,_, b_labels = batch\n","      # Telling the model not to compute or store gradients, saving memory and \n","      # speeding up prediction\n","      with torch.no_grad():\n","          # Forward pass, calculate logit predictions\n","            # print(\"multi\")\n","          _,outputs = model_multi_task(b_input_ids,b_input_mask)\n","            # print(\"outputs\",outputs.shape,b_input_ids.shape,batch[0].shape,index)\n","          final_outputs.append(outputs)\n","            # print(\"cascade\")\n","          output_cascade = model_cascade(b_input_ids,b_input_mask)\n","          final_outputs.append(output_cascade)\n","            # print(\"normal\")\n","          output_difficulty = model(b_input_ids,b_input_mask)\n","          final_outputs.append(output_difficulty)\n","      # logits_2 = outputs\n","      # logist_1 = output_bert[0]\n","      predictions_1 = final_outputs\n","      logits = torch.mean(torch.stack(predictions_1), dim=0)\n","      # else:\n","        # logits = predictions_1[0]\n","      # Move logits and labels to CPU\n","      logits = logits.detach().cpu().numpy()\n","      label_ids = b_labels.to('cpu').numpy()\n","\n","      \n","      # Store predictions and true labels\n","      predictions.append(logits)\n","      true_labels.append(label_ids)\n","  flat_predictions = np.concatenate(predictions, axis=0)\n","\n","  flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","\n","    # Combine the correct labels for each batch into a single list.\n","  flat_true_labels = np.concatenate(true_labels, axis=0)\n","  # metrics = precision_recall_fscore_support(flat_true_labels, flat_predictions, average='micro')\n","  metrics = print_weighted_metrics(flat_predictions,flat_true_labels)\n","  macro_metrics = print_metrics(flat_predictions,flat_true_labels)\n","  return metrics[2],macro_metrics[2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PmWtK4p96l-0","outputId":"5201a207-5e0e-4b10-cdcf-0a97e5a883ce"},"source":["stacking_model = MLPStackedEnsemble()\n","stacking_model.load_state_dict(torch.load(\"model_MLP_skill_ensemble_latest/model_weights\"))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":110}]},{"cell_type":"code","metadata":{"id":"LR5RSd_Jk5kr","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f43804c7-2b42-4f6a-d76f-81825185e565"},"source":["stacking_model.cuda()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MLPStackedEnsemble(\n","  (mlp): Sequential(\n","    (0): Linear(in_features=5, out_features=5, bias=True)\n","    (1): Dropout(p=0.2, inplace=False)\n","    (2): ReLU()\n","    (3): Linear(in_features=5, out_features=5, bias=True)\n","    (4): ReLU()\n","  )\n","  (output): Linear(in_features=5, out_features=5, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":111}]},{"cell_type":"code","metadata":{"id":"TaSgK-xP90vz"},"source":["def make_lstm_and_gru_predictions_mlp(index,iterator):\n","  model_lstm.eval()\n","  model_gru.eval()\n","  for i,batch in enumerate(iterator):\n","    outputs = []\n","    if i == index:\n","      with torch.no_grad():\n","          question, x_len = batch.text\n","          x = question.cuda()\n","          # outs = sigmoid(outs.cpu().data.numpy()).tolist()\n","          y = batch.label.type(torch.long).cuda()\n","\n","          lstm_outputs = model_lstm(x,x_len)\n","          outputs.append(lstm_outputs)\n","          gru_outputs = model_gru(x,x_len)\n","          outputs.append(gru_outputs)\n","\n","          return outputs\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bG_F0YCW90yW"},"source":["def make_mlp_predictions(prediction_dataloader):\n","  # Prediction on test set\n","    predictions = []\n","    true_labels = []\n","    for index,batch in enumerate(prediction_dataloader):\n","\n","        # Progress update every 40 batches.\n","        stacking_model.eval()\n","        model.eval()\n","        model_multi_task.eval()\n","        model_cascade.eval()\n","        final_outputs = []\n","        # Add batch to GPU\n","        batch = tuple(t.to(device) for t in batch)\n","        # print(\"index\",index)\n","        # Unpack the inputs from our dataloader\n","        b_input_ids, b_input_mask, _,b_labels = batch\n","\n","        # Telling the model not to compute or store gradients, saving memory and \n","        # speeding up prediction\n","        with torch.no_grad():\n","            # Forward pass, calculate logit predictions\n","            # if params['multi_task']>=0.5:\n","              # print(\"multi\")\n","            _,outputs = model_multi_task(b_input_ids,b_input_mask)\n","            # print(np.concatenate(outputs,axis=0).shape)\n","            final_outputs.append(np.argmax(outputs.detach().cpu().numpy(), axis=1).flatten())\n","          # if params['cascade']>=0.5:\n","            # print(\"cascade\")\n","            output_cascade = model_cascade(b_input_ids,b_input_mask)\n","            final_outputs.append(np.argmax(output_cascade.detach().cpu().numpy(),axis=1).flatten())\n","          # if params['difficulty'] >=0:\n","            # print(\"normal\")\n","            output_difficulty = model(b_input_ids,b_input_mask)\n","            final_outputs.append(np.argmax(output_difficulty.detach().cpu().numpy(),axis=1).flatten())\n","\n","            out = make_lstm_and_gru_predictions_mlp(index,test_iter)\n","            final_outputs.append(np.argmax(out[0].detach().cpu().numpy(),axis=1).flatten())\n","            final_outputs.append(np.argmax(out[1].detach().cpu().numpy(),axis=1).flatten())\n","\n","\n","\n","\n","            inputs_ensemble = np.vstack(final_outputs).transpose()\n","            inputs_ensemble = torch.tensor(inputs_ensemble,dtype=float).float().cuda()  \n","            probas = stacking_model(inputs_ensemble)\n","      # else:\n","        # logits = predictions_1[0]\n","      # Move logits and labels to CPU\n","        logits = probas.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","      \n","      # Store predictions and true labels\n","        predictions.append(logits)\n","        true_labels.append(label_ids)\n","    flat_predictions = np.concatenate(predictions, axis=0)\n","\n","    flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","\n","    # Combine the correct labels for each batch into a single list.\n","    flat_true_labels = np.concatenate(true_labels, axis=0)\n","    # metrics = precision_recall_fscore_support(flat_true_labels, flat_predictions, average='micro')\n","    metrics = print_weighted_metrics(flat_predictions,flat_true_labels)\n","    macro_metrics = print_metrics(flat_predictions, flat_true_labels)\n","    print(\"weighted_metrics\",metrics)\n","    print(\"macro_metrics\",macro_metrics)\n","\n","    print('    DONE.')\n","    return metrics[2],macro_metrics[2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3QFUh7sqKv9n"},"source":["test_labels = test[\"difficulty_label\"].values\n","test_skill_labels = test[\"skill_label\"].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":106},"id":"YlXpc52FUTp1","outputId":"d2bfee55-47ce-4586-dcb1-56ad8e9904fe"},"source":["test.iloc[[2,3],:]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>board_syllabus</th>\n","      <th>question_answer</th>\n","      <th>skill_label</th>\n","      <th>difficulty_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2</th>\n","      <td>ICSE OLD&gt;&gt;XI&gt;&gt;Political Science&gt;&gt;State, Govern...</td>\n","      <td>Which of the following is not the element of ...</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Maharashtra New&gt;&gt;VII&gt;&gt;General Science&gt;&gt;Static ...</td>\n","      <td>The process of electrically charging an objec...</td>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                      board_syllabus  ... difficulty_label\n","2  ICSE OLD>>XI>>Political Science>>State, Govern...  ...                0\n","3  Maharashtra New>>VII>>General Science>>Static ...  ...                1\n","\n","[2 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":115}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K3gCmcJN2lJH","outputId":"2407a88d-1cce-4560-b702-40ddea3a20e8"},"source":["import torch.nn.functional as F\n","f1_bo_ensemble = []\n","f1_mlp_ensemble =[]\n","macro_f1_bo_ensemble = []\n","macro_f1_mlp_ensemble = []\n","for indices in kf.split(test_features):\n","  input_ids = []\n","  attention_masks = []\n","  for sent in test_features[indices[0]]:\n","\n","\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 256,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        truncation=True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","  input_ids = torch.cat(input_ids, dim=0)\n","  attention_masks = torch.cat(attention_masks, dim=0)\n","  test_labels_tensor = torch.tensor(test_labels[indices[0]])\n","  test_skill_labels_tensor = torch.tensor(test_skill_labels[indices[0]])\n","  text = torchtext.data.Field(lower=True, batch_first=True, tokenize='spacy', include_lengths=True)\n","  target_diff = torchtext.data.Field(sequential=False, use_vocab=False, is_target=True)\n","  test.iloc[indices[0],:].to_csv(\"interim_test.csv\",index=False)\n","  text.build_vocab(train, val, test_text,vectors=\"glove.6B.100d\", min_freq=3)\n","\n","  test_text = torchtext.data.TabularDataset(path=\"interim_test.csv\",format='csv',\n","                                     fields={'skill_label': ('label', target_diff),\n","                                             'question_answer': ('text',text)})\n","  # test_text = torchtext.data.TabularDataset(examples=test.iloc[indices[1],:],\n","  #                                    fields={'difficulty_label': ('label', target_diff),\n","  #                                            'question_answer': ('text',text)})\n","  test_iter = torchtext.data.Iterator(dataset=test_text, batch_size=32,train=False, sort=False, sort_within_batch=False,shuffle=False)\n","# Set the batch size.  \n","  batch_size = 32  \n","\n","  prediction_data = TensorDataset(input_ids, attention_masks, test_labels_tensor,test_skill_labels_tensor)\n","  # Create the DataLoader.\n","  prediction_sampler = SequentialSampler(prediction_data)\n","  prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n","  f1_bo,macro_f1_bo = get_bo_predictions(prediction_dataloader)\n","  f1_mlp,macro_f1_mlp = make_mlp_predictions(prediction_dataloader)\n","  f1_bo_ensemble.append(f1_bo)\n","  macro_f1_bo_ensemble.append(macro_f1_bo)\n","  macro_f1_mlp_ensemble.append(macro_f1_mlp)\n","  f1_mlp_ensemble.append(f1_mlp)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2155: FutureWarning:\n","\n","The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\n"],"name":"stderr"},{"output_type":"stream","text":["weighted_metrics (0.4738990675461966, 0.4736410816716744, 0.4737700394881485)\n","macro_metrics (0.4772264238131928, 0.43169819034607365, 0.45332204747485083)\n","    DONE.\n","weighted_metrics (0.4794963203312509, 0.4796503687517072, 0.47957333217063103)\n","macro_metrics (0.47986628326697645, 0.43883180985682835, 0.4584326258026502)\n","    DONE.\n","weighted_metrics (0.47126938133508806, 0.4759694156198798, 0.4736077381008455)\n","macro_metrics (0.4662151294886595, 0.4336042850894695, 0.4493187736221538)\n","    DONE.\n","weighted_metrics (0.4772486322995243, 0.4811578372474058, 0.4791952622253813)\n","macro_metrics (0.4742321237658554, 0.4396306484959126, 0.4562763304008572)\n","    DONE.\n","weighted_metrics (0.4798977756814006, 0.479792463134899, 0.47984511362986215)\n","macro_metrics (0.4763985188022729, 0.43958316017615146, 0.45725099356113563)\n","    DONE.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eLO3BUOEKMpm","colab":{"base_uri":"https://localhost:8080/"},"outputId":"693b7050-78bf-4221-ceaa-48e62aa0b50e"},"source":["print(f1_bo_ensemble)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0.5072805461600827, 0.5136024136257048, 0.5033756191635819, 0.5089557987624489, 0.5088839917661918]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"71jVD2AAcMPe","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4398fb84-76bd-4332-bf75-dd0070d2bce4"},"source":["f1_mlp_ensemble"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.4737700394881485,\n"," 0.47957333217063103,\n"," 0.4736077381008455,\n"," 0.4791952622253813,\n"," 0.47984511362986215]"]},"metadata":{"tags":[]},"execution_count":124}]},{"cell_type":"code","metadata":{"id":"87jSlrNZJsfZ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"845d5158-9075-4da3-91d5-4b1ba9ba43b1"},"source":["from scipy import stats\n","stats.ttest_rel(f1_bo_ensemble,f1_mlp_ensemble)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Ttest_relResult(statistic=29.678620611924487, pvalue=7.675319950610268e-06)"]},"metadata":{"tags":[]},"execution_count":125}]},{"cell_type":"markdown","metadata":{"id":"UDxKTJqNSnFp"},"source":["Now for macro f1"]},{"cell_type":"code","metadata":{"id":"31_jaZqASy6P","colab":{"base_uri":"https://localhost:8080/"},"outputId":"977ff9bc-3a15-4024-a33e-b1dc39371cb1"},"source":["print(macro_f1_bo_ensemble)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0.48046315570722753, 0.48588314423228, 0.473905996256458, 0.48367720025589533, 0.479588364738323]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IoqkoAbaSy6Q","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7548f39c-0807-4386-fc39-89357d4285d0"},"source":["macro_f1_mlp_ensemble"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.45332204747485083,\n"," 0.4584326258026502,\n"," 0.4493187736221538,\n"," 0.4562763304008572,\n"," 0.45725099356113563]"]},"metadata":{"tags":[]},"execution_count":121}]},{"cell_type":"code","metadata":{"id":"ManKEa4kSy6R","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d5dec8b3-7eec-47d1-c3b5-0d11a0e87606"},"source":["from scipy import stats\n","stats.ttest_rel(macro_f1_bo_ensemble,macro_f1_mlp_ensemble)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Ttest_relResult(statistic=25.439251378782533, pvalue=1.417990297502261e-05)"]},"metadata":{"tags":[]},"execution_count":122}]},{"cell_type":"code","metadata":{"id":"uWyojbiQ902u"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i42EkBD4kuVm"},"source":["import pandas as pd\n","x = pd.read_csv(\"train_skill_name_difficulty.csv\")\n","qa=x[\"question_answer\"].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Iz5Jrn3PlIK5"},"source":["def get_questions(qa1) :\n","  if \"?\" in qa1:\n","    qa1 = qa1.split(\"?\")[0]\n","  elif \"known as\" in qa1:\n","    qa1 = qa1.split(\"known as\")[0]\n","  # elif \":\" in qa1:\n","  #   qa1 = qa1.split(\":\")[0]\n","  # elif \". \" in qa1:\n","  #   qa1 = qa1.split(\". \")[0]\n","  return qa1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EwrEe0P2lrqW"},"source":["x[\"questions\"] = x[\"question_answer\"].apply(lambda x: get_questions(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"56awUdTZmDjl","outputId":"7e2c2fa8-67b1-4b42-b7a1-0d8ebf943588"},"source":["x[\"questions\"].values"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([' Among the following, freshwater fish is rohu Rohu is a fresh water fish. Other common freshwater fish are catla, common carp.',\n","       ' Which of the following statement is true',\n","       ' The process of using multiple constructors with the same name but with different parameters is ',\n","       ...,\n","       'The horizontal line in the graph is denoted as the X-axis. The horizontal line points in the horizontal direction and is denoted as the X-axis in the graph.',\n","       ' SI unit of force is newton The SI unit of force is Newton (N), named after famous scientist Isaac Newton who discovered force of gravitation.',\n","       ' In machines sliding frictions is replaced to rolling by use of ball bearings Ball bearing roll to produce rolling;friction.'],\n","      dtype=object)"]},"metadata":{"tags":[]},"execution_count":186}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zYjzYcfem7Kv","outputId":"7c4847eb-3940-4bfa-805e-2ba063d66888"},"source":["x[\"question_answer\"].values"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([' Among the following, freshwater fish is rohu Rohu is a fresh water fish. Other common freshwater fish are catla, common carp.',\n","       ' Which of the following statement is true? Sound requires a medium for propagation. Sound travels through a medium (solid, liquid or gas). It cannot travel through vacuum.',\n","       ' The process of using multiple constructors with the same name but with different parameters is known as: Constructor overloading Constructor overloading is a technique in Java in which a class can have any number of constructors that differ in parameter lists.',\n","       ...,\n","       'The horizontal line in the graph is denoted as the X-axis. The horizontal line points in the horizontal direction and is denoted as the X-axis in the graph.',\n","       ' SI unit of force is newton The SI unit of force is Newton (N), named after famous scientist Isaac Newton who discovered force of gravitation.',\n","       ' In machines sliding frictions is replaced to rolling by use of ball bearings Ball bearing roll to produce rolling;friction.'],\n","      dtype=object)"]},"metadata":{"tags":[]},"execution_count":187}]},{"cell_type":"code","metadata":{"id":"FjS-OYlyvWv2"},"source":["x.to_csv(\"train_skill_difficulty_question_only.csv\",index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rBigR355Vf8N"},"source":[""],"execution_count":null,"outputs":[]}]}