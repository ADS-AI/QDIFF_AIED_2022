{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"multi_task_qdiff_interactive_attention_BERT_data_2_final.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"widgets":{"application/vnd.jupyter.widget-state+json":{"aca83b08638d400487e9d1e4362e0581":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_88b9437bde85450d9e006d3e02075f6d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f97201aca72f413c81b4804dd7a89261","IPY_MODEL_2224beba045d493da73bd727b36343eb","IPY_MODEL_06540715278341b590fe724e17157601"]}},"88b9437bde85450d9e006d3e02075f6d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f97201aca72f413c81b4804dd7a89261":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_91ff315db32c4d2c92e0be280506ce2c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1c189ea102614e9eb5206b9b982e2230"}},"2224beba045d493da73bd727b36343eb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_2a153b539dec40339f4e3cbc94d74265","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6512c4a556bf4216aa327153a12dff9a"}},"06540715278341b590fe724e17157601":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_fbfedfa6d5c44e6f8a83b98403f8fafb","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:00&lt;00:00, 1.20MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e21ac2e146c3499299944f0c29c96420"}},"91ff315db32c4d2c92e0be280506ce2c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1c189ea102614e9eb5206b9b982e2230":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2a153b539dec40339f4e3cbc94d74265":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6512c4a556bf4216aa327153a12dff9a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fbfedfa6d5c44e6f8a83b98403f8fafb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e21ac2e146c3499299944f0c29c96420":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sR9av2JU3kf6","outputId":"c8209a0c-f0e7-418c-a42f-9d5d66278e01"},"source":["import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","import torch\n","import logging\n","logging.basicConfig(level=logging.ERROR)\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla K80\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BzKeqoCs3kgA","outputId":"8a1e382b-43f7-478b-c0d5-9d3b72d2dc35"},"source":["!pip install transformers==3.2.0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers==3.2.0\n","  Downloading transformers-3.2.0-py3-none-any.whl (1.0 MB)\n","\u001b[K     |████████████████████████████████| 1.0 MB 5.2 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 36.6 MB/s \n","\u001b[?25hCollecting tokenizers==0.8.1.rc2\n","  Downloading tokenizers-0.8.1rc2-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n","\u001b[K     |████████████████████████████████| 3.0 MB 20.6 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.2.0) (21.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.2.0) (2019.12.20)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.2.0) (1.19.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.2.0) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.2.0) (4.62.0)\n","Collecting sentencepiece!=0.1.92\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 49.7 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.2.0) (3.0.12)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.2.0) (2.4.7)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.2.0) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.2.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.2.0) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.2.0) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.2.0) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.2.0) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.2.0) (7.1.2)\n","Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n","Successfully installed sacremoses-0.0.45 sentencepiece-0.1.96 tokenizers-0.8.1rc2 transformers-3.2.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GsADhaO93kgD","colab":{"base_uri":"https://localhost:8080/","height":580},"outputId":"50023c1b-e803-4bb5-a840-1329874b36f0"},"source":["import pandas as pd\n","final_data = pd.read_csv(\"/content/train_qdiff_data_2_soft_labeled.csv\")\n","# final_data = final_data[[\"Question\",\"Answer\",\"DifficultyFromAnswerer\"]]\n","final_data"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Question</th>\n","      <th>Answer</th>\n","      <th>DifficultyFromAnswerer</th>\n","      <th>question_answer</th>\n","      <th>difficulty_label</th>\n","      <th>skill_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Is the dialect spoken in Jeju located in fact ...</td>\n","      <td>The dialect spoken in Jeju is in fact classifi...</td>\n","      <td>hard</td>\n","      <td>Is the dialect spoken in Jeju located in fact ...</td>\n","      <td>1</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>What cello manufacturer should I buy from if I...</td>\n","      <td>Luis &amp; Clark</td>\n","      <td>hard</td>\n","      <td>What cello manufacturer should I buy from if I...</td>\n","      <td>1</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Does it have a border with Norway?</td>\n","      <td>yes</td>\n","      <td>medium</td>\n","      <td>Does it have a border with Norway? yes</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>How many people use the bus network daily?</td>\n","      <td>More than 2.78 million people.</td>\n","      <td>easy</td>\n","      <td>How many people use the bus network daily? Mor...</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Who founded Montevideo?</td>\n","      <td>The Spanish.</td>\n","      <td>medium</td>\n","      <td>Who founded Montevideo? The Spanish.</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2763</th>\n","      <td>Did he become a professor before the revolutio...</td>\n","      <td>yes</td>\n","      <td>hard</td>\n","      <td>Did he become a professor before the revolutio...</td>\n","      <td>1</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2764</th>\n","      <td>Does Vietnamese borrow from Latin and Greek?</td>\n","      <td>No, Vietnamese does not borrow from Latin and ...</td>\n","      <td>medium</td>\n","      <td>Does Vietnamese borrow from Latin and Greek? N...</td>\n","      <td>2</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2765</th>\n","      <td>Where is San Francisco?</td>\n","      <td>San Francisco is in California.</td>\n","      <td>medium</td>\n","      <td>Where is San Francisco? San Francisco is in Ca...</td>\n","      <td>2</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2766</th>\n","      <td>What is the primary item in an otter's diet?</td>\n","      <td>fish</td>\n","      <td>medium</td>\n","      <td>What is the primary item in an otter's diet? fish</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2767</th>\n","      <td>Where are turtle eggs layed?</td>\n","      <td>Turtles lay eggs on land.</td>\n","      <td>hard</td>\n","      <td>Where are turtle eggs layed? Turtles lay eggs ...</td>\n","      <td>1</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2768 rows × 6 columns</p>\n","</div>"],"text/plain":["                                               Question  ... skill_label\n","0     Is the dialect spoken in Jeju located in fact ...  ...           3\n","1     What cello manufacturer should I buy from if I...  ...           3\n","2                    Does it have a border with Norway?  ...           2\n","3            How many people use the bus network daily?  ...           2\n","4                               Who founded Montevideo?  ...           2\n","...                                                 ...  ...         ...\n","2763  Did he become a professor before the revolutio...  ...           3\n","2764       Does Vietnamese borrow from Latin and Greek?  ...           3\n","2765                            Where is San Francisco?  ...           3\n","2766       What is the primary item in an otter's diet?  ...           2\n","2767                       Where are turtle eggs layed?  ...           3\n","\n","[2768 rows x 6 columns]"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"2EkZ-3TZ5UBJ"},"source":["def clean_sentence(question):\n","  # print(question)\n","  question = re.sub('<[^>]*>', ' ',question)\n","  question = re.sub(' +', ' ', question)\n","  question = re.sub('\\xa0','',question)\n","  question = question.rstrip()\n","  question = re.sub('nan','',question)\n","  question = re.sub(u'\\u2004','',question)\n","  question = re.sub(u'\\u2009','',question)\n","\n","  # question = question.decode(\"utf-8\")\n","  # question = question.replace(u'\\u200\\d*','').encode(\"utf-8\")\n","  question = re.sub('&nbsp','',question)\n","  question = re.sub('&ndash','',question)\n","  question = re.sub('\\r','',question)\n","  question = re.sub('\\t','',question)\n","  question = re.sub('\\n',' ',question)\n","\n","  question = re.sub('MathType@.*','',question)\n","  question = re.sub('&thinsp','',question)\n","  question = re.sub('&times','',question)\n","  question = re.sub('\\u200b','',question)\n","  question = re.sub('&rarr;;;','',question)\n","\n","  return question"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IMCV9mxDnaK0","colab":{"base_uri":"https://localhost:8080/"},"outputId":"97f49a64-11b9-4c96-8c49-b898b4d9cbf7"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9exlBELH5oq9"},"source":["!cp \"/content/drive/My Drive/research_skill_name_prediction/label_encoder_difficulty_Lstm\"  /content"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pUqcGtXwBIyl"},"source":["!cp \"/content/drive/MyDrive/research_skill_name_prediction/label_encoder_skill_lstm\" /content"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rj-ow-6cqFyn"},"source":["!cp -r \"/content/drive/MyDrive/research_skill_name_prediction/model_bert_multi_task_interactive_data_2_final\" /content"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JYbSa7ZKAkfz","outputId":"f38a1eff-5e17-4ff2-d749-7e89ad1640ac"},"source":["import joblib\n","LE_skill = joblib.load('label_encoder_skill_lstm')\n","LE_skill.classes_"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['Analysing', 'Applying', 'Knowledge & understanding',\n","       'Remembering', 'Understanding'], dtype=object)"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"Jq7TeCSBPSwT"},"source":["from sklearn.preprocessing import LabelEncoder\n","LE_diff = LabelEncoder()\n","final_data[\"difficulty_label\"] = LE_diff.fit_transform(final_data[\"DifficultyFromAnswerer\"].values)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":580},"id":"9RPR93qiPkoJ","outputId":"e7889512-612a-4ea2-945a-04335db5f761"},"source":["final_data"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Question</th>\n","      <th>Answer</th>\n","      <th>DifficultyFromAnswerer</th>\n","      <th>question_answer</th>\n","      <th>difficulty_label</th>\n","      <th>skill_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Is the dialect spoken in Jeju located in fact ...</td>\n","      <td>The dialect spoken in Jeju is in fact classifi...</td>\n","      <td>hard</td>\n","      <td>Is the dialect spoken in Jeju located in fact ...</td>\n","      <td>1</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>What cello manufacturer should I buy from if I...</td>\n","      <td>Luis &amp; Clark</td>\n","      <td>hard</td>\n","      <td>What cello manufacturer should I buy from if I...</td>\n","      <td>1</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Does it have a border with Norway?</td>\n","      <td>yes</td>\n","      <td>medium</td>\n","      <td>Does it have a border with Norway? yes</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>How many people use the bus network daily?</td>\n","      <td>More than 2.78 million people.</td>\n","      <td>easy</td>\n","      <td>How many people use the bus network daily? Mor...</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Who founded Montevideo?</td>\n","      <td>The Spanish.</td>\n","      <td>medium</td>\n","      <td>Who founded Montevideo? The Spanish.</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2763</th>\n","      <td>Did he become a professor before the revolutio...</td>\n","      <td>yes</td>\n","      <td>hard</td>\n","      <td>Did he become a professor before the revolutio...</td>\n","      <td>1</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2764</th>\n","      <td>Does Vietnamese borrow from Latin and Greek?</td>\n","      <td>No, Vietnamese does not borrow from Latin and ...</td>\n","      <td>medium</td>\n","      <td>Does Vietnamese borrow from Latin and Greek? N...</td>\n","      <td>2</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2765</th>\n","      <td>Where is San Francisco?</td>\n","      <td>San Francisco is in California.</td>\n","      <td>medium</td>\n","      <td>Where is San Francisco? San Francisco is in Ca...</td>\n","      <td>2</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2766</th>\n","      <td>What is the primary item in an otter's diet?</td>\n","      <td>fish</td>\n","      <td>medium</td>\n","      <td>What is the primary item in an otter's diet? fish</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2767</th>\n","      <td>Where are turtle eggs layed?</td>\n","      <td>Turtles lay eggs on land.</td>\n","      <td>hard</td>\n","      <td>Where are turtle eggs layed? Turtles lay eggs ...</td>\n","      <td>1</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2768 rows × 6 columns</p>\n","</div>"],"text/plain":["                                               Question  ... skill_label\n","0     Is the dialect spoken in Jeju located in fact ...  ...           3\n","1     What cello manufacturer should I buy from if I...  ...           3\n","2                    Does it have a border with Norway?  ...           2\n","3            How many people use the bus network daily?  ...           2\n","4                               Who founded Montevideo?  ...           2\n","...                                                 ...  ...         ...\n","2763  Did he become a professor before the revolutio...  ...           3\n","2764       Does Vietnamese borrow from Latin and Greek?  ...           3\n","2765                            Where is San Francisco?  ...           3\n","2766       What is the primary item in an otter's diet?  ...           2\n","2767                       Where are turtle eggs layed?  ...           3\n","\n","[2768 rows x 6 columns]"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"-OBarOLBz2nO"},"source":["def get_labels(prediction):\n","    predicted_label =  LE_diff.inverse_transform([prediction])\n","    return predicted_label[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":281},"id":"XqWem79lbn1J","outputId":"11ab406a-9c18-4c47-c7fa-0c4e94bb1bd1"},"source":["final_data['difficulty_label'].value_counts().sort_values(ascending=False).plot(kind='bar')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7fad03f34e90>"]},"metadata":{},"execution_count":13},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAX0AAAD1CAYAAAC87SVQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAM8klEQVR4nO3dfaie9X3H8fenZnZrC8aHQ7AnsUcwW3Ebm3KwFmGUZrQ+lMU/WrGUGSSQf+zTHMxs/wgbDIUxpzBkoXGLo9hKVkhoxSJRKWOYeqziU9Z5cGoSfDit0c1JabN+98f5Zb17mpic+z657+jv/YLDua7f9bvv63c48D5XrnPfJ6kqJEl9eN+kFyBJGh+jL0kdMfqS1BGjL0kdMfqS1BGjL0kdWTXpBbyTc845p2ZmZia9DEl6V3nsscd+VFVTRzt2Skd/ZmaGubm5SS9Dkt5Vkrx4rGPe3pGkjhh9SeqI0Zekjhh9SeqI0Zekjhh9SeqI0Zekjhh9SerIKf3mrHGb2fqdSS/hpHrhlqsmvQRJE+aVviR1xOhLUkeMviR1xOhLUkeMviR1xOhLUkeMviR1xOhLUkeMviR1xOhLUkeMviR1xOhLUkeMviR1xOhLUkeOG/0kdyV5LcnTA2NnJXkgyXPt85ltPEnuSDKf5MkkFw88ZlOb/1ySTSfny5EkvZMTudL/J+DyJWNbgT1VtR7Y0/YBrgDWt48twJ2w+EMCuBn4GHAJcPORHxSSpPE5bvSr6nvA60uGNwI72vYO4OqB8btr0SPA6iTnAp8GHqiq16vqEPAAv/qDRJJ0kg17T39NVb3ctl8B1rTtaWD/wLwDbexY45KkMRr5v0usqkpSK7EYgCRbWLw1xHnnnbdST6sO+N9dSsc37JX+q+22De3za238ILBuYN7aNnas8V9RVduqaraqZqempoZcniTpaIaN/m7gyCtwNgG7Bsava6/iuRR4s90G+i7wqSRntl/gfqqNSZLG6Li3d5LcA3wCOCfJARZfhXMLcG+SzcCLwDVt+n3AlcA88DZwPUBVvZ7kr4BH27y/rKqlvxyWJJ1kx41+VX3+GIc2HGVuATcc43nuAu5a1uokSSvKd+RKUkeMviR1xOhLUkeMviR1xOhLUkeMviR1xOhLUkeMviR1xOhLUkeMviR1xOhLUkeMviR1xOhLUkeMviR1xOhLUkeMviR1xOhLUkeMviR1xOhLUkeMviR1xOhLUkeMviR1xOhLUkeMviR1xOhLUkeMviR1xOhLUkeMviR1xOhLUkdGin6SP0nyTJKnk9yT5NeTnJ9kb5L5JN9Mcnqb+/62P9+Oz6zEFyBJOnFDRz/JNPBlYLaqfgc4DbgWuBW4raouAA4Bm9tDNgOH2vhtbZ4kaYxGvb2zCviNJKuADwAvA58EdrbjO4Cr2/bGtk87viFJRjy/JGkZho5+VR0E/gZ4icXYvwk8BrxRVYfbtAPAdNueBva3xx5u888e9vySpOUb5fbOmSxevZ8PfBj4IHD5qAtKsiXJXJK5hYWFUZ9OkjRglNs7fwj8Z1UtVNXPgG8BlwGr2+0egLXAwbZ9EFgH0I6fAfx46ZNW1baqmq2q2ampqRGWJ0laapTovwRcmuQD7d78BuBZ4CHgs23OJmBX297d9mnHH6yqGuH8kqRlGuWe/l4WfyH7A+Cp9lzbgJuAG5PMs3jPfnt7yHbg7DZ+I7B1hHVLkoaw6vhTjq2qbgZuXjL8PHDJUeb+BPjcKOeTJI3Gd+RKUkeMviR1xOhLUkeMviR1xOhLUkeMviR1xOhLUkeMviR1xOhLUkeMviR1xOhLUkeMviR1xOhLUkeMviR1xOhLUkeMviR1xOhLUkeMviR1xOhLUkeMviR1ZKT/GF2SVsLM1u9Megkn1Qu3XDXpJfw/r/QlqSNGX5I6YvQlqSNGX5I6YvQlqSNGX5I6YvQlqSNGX5I6MlL0k6xOsjPJvyfZl+TjSc5K8kCS59rnM9vcJLkjyXySJ5NcvDJfgiTpRI16pX87cH9VfRT4PWAfsBXYU1XrgT1tH+AKYH372ALcOeK5JUnLNHT0k5wB/AGwHaCqflpVbwAbgR1t2g7g6ra9Ebi7Fj0CrE5y7tArlyQt2yhX+ucDC8A/Jnk8ydeSfBBYU1UvtzmvAGva9jSwf+DxB9rYL0myJclckrmFhYURlidJWmqU6K8CLgburKqLgP/hF7dyAKiqAmo5T1pV26pqtqpmp6amRlieJGmpUaJ/ADhQVXvb/k4Wfwi8euS2Tfv8Wjt+EFg38Pi1bUySNCZDR7+qXgH2J/mtNrQBeBbYDWxqY5uAXW17N3BdexXPpcCbA7eBJEljMOrf0/8S8PUkpwPPA9ez+IPk3iSbgReBa9rc+4ArgXng7TZXkjRGI0W/qp4AZo9yaMNR5hZwwyjnkySNxnfkSlJHjL4kdcToS1JHjL4kdcToS1JHjL4kdcToS1JHjL4kdcToS1JHjL4kdcToS1JHjL4kdcToS1JHjL4kdcToS1JHjL4kdcToS1JHjL4kdcToS1JHjL4kdcToS1JHjL4kdcToS1JHjL4kdcToS1JHjL4kdcToS1JHjL4kdWTk6Cc5LcnjSb7d9s9PsjfJfJJvJjm9jb+/7c+34zOjnluStDwrcaX/FWDfwP6twG1VdQFwCNjcxjcDh9r4bW2eJGmMRop+krXAVcDX2n6ATwI725QdwNVte2Pbpx3f0OZLksZk1Cv9vwP+DPh52z8beKOqDrf9A8B0254G9gO042+2+ZKkMRk6+kk+A7xWVY+t4HpIsiXJXJK5hYWFlXxqSereKFf6lwF/lOQF4Bss3ta5HVidZFWbsxY42LYPAusA2vEzgB8vfdKq2lZVs1U1OzU1NcLyJElLDR39qvrzqlpbVTPAtcCDVfUF4CHgs23aJmBX297d9mnHH6yqGvb8kqTlOxmv078JuDHJPIv37Le38e3A2W38RmDrSTi3JOkdrDr+lOOrqoeBh9v288AlR5nzE+BzK3E+SdJwfEeuJHXE6EtSR4y+JHXE6EtSR4y+JHXE6EtSR4y+JHXE6EtSR4y+JHXE6EtSR4y+JHXE6EtSR4y+JHXE6EtSR4y+JHXE6EtSR4y+JHXE6EtSR4y+JHXE6EtSR4y+JHXE6EtSR4y+JHXE6EtSR4y+JHXE6EtSR4y+JHXE6EtSR4y+JHVk6OgnWZfkoSTPJnkmyVfa+FlJHkjyXPt8ZhtPkjuSzCd5MsnFK/VFSJJOzChX+oeBP62qC4FLgRuSXAhsBfZU1XpgT9sHuAJY3z62AHeOcG5J0hCGjn5VvVxVP2jb/w3sA6aBjcCONm0HcHXb3gjcXYseAVYnOXfolUuSlm1F7uknmQEuAvYCa6rq5XboFWBN254G9g887EAbkySNycjRT/Ih4F+Ar1bVfw0eq6oCapnPtyXJXJK5hYWFUZcnSRowUvST/BqLwf96VX2rDb965LZN+/xaGz8IrBt4+No29kuqaltVzVbV7NTU1CjLkyQtMcqrdwJsB/ZV1d8OHNoNbGrbm4BdA+PXtVfxXAq8OXAbSJI0BqtGeOxlwB8DTyV5oo39BXALcG+SzcCLwDXt2H3AlcA88DZw/QjnliQNYejoV9W/AjnG4Q1HmV/ADcOeT5I0Ot+RK0kdMfqS1BGjL0kdMfqS1BGjL0kdMfqS1BGjL0kdMfqS1BGjL0kdMfqS1BGjL0kdMfqS1BGjL0kdMfqS1BGjL0kdMfqS1BGjL0kdMfqS1BGjL0kdMfqS1BGjL0kdMfqS1BGjL0kdMfqS1BGjL0kdMfqS1BGjL0kdMfqS1BGjL0kdGXv0k1ye5IdJ5pNsHff5JalnY41+ktOAvweuAC4EPp/kwnGuQZJ6Nu4r/UuA+ap6vqp+CnwD2DjmNUhSt1aN+XzTwP6B/QPAxwYnJNkCbGm7byX54ZjWNgnnAD8a18ly67jO1A2/f+9e7/Xv3UeOdWDc0T+uqtoGbJv0OsYhyVxVzU56HRqO3793r56/d+O+vXMQWDewv7aNSZLGYNzRfxRYn+T8JKcD1wK7x7wGSerWWG/vVNXhJF8EvgucBtxVVc+Mcw2nmC5uY72H+f179+r2e5eqmvQaJElj4jtyJakjRl+SOmL0Jakjp9zr9N/LknyUxXcgT7ehg8Duqto3uVXpRLTv3TSwt6reGhi/vKrun9zKpOXxSn9MktzE4p+dCPD99hHgHv/w3KktyZeBXcCXgKeTDP7pkL+ezKq0EpJcP+k1jJuv3hmTJP8B/HZV/WzJ+OnAM1W1fjIr0/EkeQr4eFW9lWQG2An8c1XdnuTxqrpoogvU0JK8VFXnTXod4+TtnfH5OfBh4MUl4+e2Yzp1ve/ILZ2qeiHJJ4CdST7C4r/WdApL8uSxDgFrxrmWU4HRH5+vAnuSPMcv/ujcecAFwBcntiqdiFeT/H5VPQHQrvg/A9wF/O5kl6YTsAb4NHBoyXiAfxv/cibL6I9JVd2f5DdZ/PPSg7/IfbSq/ndyK9MJuA44PDhQVYeB65L8w2SWpGX4NvChIz+0ByV5ePzLmSzv6UtSR3z1jiR1xOhLUkeMviR1xOhLUkeMviR15P8AzU7vo7l73a8AAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"RxliBQEJ9eTG"},"source":["val = pd.read_csv(\"/content/val_qdiff_data_2_soft_labeled.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":580},"id":"K7RksCIUQIaU","outputId":"0f7958f5-64b5-4fcf-be6f-d225f6b321a1"},"source":["val"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Question</th>\n","      <th>Answer</th>\n","      <th>DifficultyFromAnswerer</th>\n","      <th>difficulty_label</th>\n","      <th>question_answer</th>\n","      <th>skill_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>What are turtle eggs covered in when they incu...</td>\n","      <td>mud or sand</td>\n","      <td>hard</td>\n","      <td>1</td>\n","      <td>What are turtle eggs covered in when they incu...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>What is given for the number of native speakers?</td>\n","      <td>No figure is given for the number of native sp...</td>\n","      <td>hard</td>\n","      <td>1</td>\n","      <td>What is given for the number of native speaker...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>How many long was Lincoln's formal education?</td>\n","      <td>18 months</td>\n","      <td>easy</td>\n","      <td>0</td>\n","      <td>How many long was Lincoln's formal education? ...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>who was his mentor?</td>\n","      <td>John 'Mad Jack' Fuller</td>\n","      <td>medium</td>\n","      <td>2</td>\n","      <td>who was his mentor? John 'Mad Jack' Fuller</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Can black swans swim with only one leg?</td>\n","      <td>yes</td>\n","      <td>easy</td>\n","      <td>0</td>\n","      <td>Can black swans swim with only one leg? yes</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>303</th>\n","      <td>Is Berlin the capital city of Germany?</td>\n","      <td>Berlin is the capital city of Germany.</td>\n","      <td>easy</td>\n","      <td>0</td>\n","      <td>Is Berlin the capital city of Germany? Berlin ...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>304</th>\n","      <td>Who did James Monroe live with in New York City?</td>\n","      <td>His daughter Maria Hester Monroe Gouverneur</td>\n","      <td>medium</td>\n","      <td>2</td>\n","      <td>Who did James Monroe live with in New York Cit...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>305</th>\n","      <td>What is one of the challenges of re-establishi...</td>\n","      <td>roadkill deaths</td>\n","      <td>hard</td>\n","      <td>1</td>\n","      <td>What is one of the challenges of re-establishi...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>306</th>\n","      <td>Is Santiago the national capital of a country?</td>\n","      <td>Yes</td>\n","      <td>easy</td>\n","      <td>0</td>\n","      <td>Is Santiago the national capital of a country?...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>307</th>\n","      <td>Why do some people believe that left-handed pe...</td>\n","      <td>to standardise the instrument</td>\n","      <td>hard</td>\n","      <td>1</td>\n","      <td>Why do some people believe that left-handed pe...</td>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>308 rows × 6 columns</p>\n","</div>"],"text/plain":["                                              Question  ... skill_label\n","0    What are turtle eggs covered in when they incu...  ...           2\n","1     What is given for the number of native speakers?  ...           3\n","2        How many long was Lincoln's formal education?  ...           3\n","3                                  who was his mentor?  ...           3\n","4              Can black swans swim with only one leg?  ...           3\n","..                                                 ...  ...         ...\n","303             Is Berlin the capital city of Germany?  ...           3\n","304   Who did James Monroe live with in New York City?  ...           3\n","305  What is one of the challenges of re-establishi...  ...           2\n","306     Is Santiago the national capital of a country?  ...           3\n","307  Why do some people believe that left-handed pe...  ...           4\n","\n","[308 rows x 6 columns]"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":545},"id":"g8tVsjiWj-cF","outputId":"7e3dc5e2-24d6-4805-9597-4638955a4778"},"source":["test = pd.read_csv(\"/content/test_qdiff_data_2_soft_labeled.csv\")\n","# test[\"difficulty_label\"] = LE_diff.transform(test[\"DifficultyFromAnswerer\"].values)\n","# test = test[[\"Question\",\"Answer\",\"DifficultyFromAnswerer\",\"difficulty_label\"]]\n","# test[\"question_answer\"] = test[\"Question\"]+\" \" + test[\"Answer\"]\n","\n","test"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Question</th>\n","      <th>Answer</th>\n","      <th>DifficultyFromAnswerer</th>\n","      <th>difficulty_label</th>\n","      <th>question_answer</th>\n","      <th>skill_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>How are western-style xylophones characterised?</td>\n","      <td>by a bright, sharp tone and high register</td>\n","      <td>medium</td>\n","      <td>2</td>\n","      <td>How are western-style xylophones characterised...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Is Nairobi the capital of Kenya?</td>\n","      <td>Yes</td>\n","      <td>easy</td>\n","      <td>0</td>\n","      <td>Is Nairobi the capital of Kenya? Yes</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>How many sister cities does the City of Melbou...</td>\n","      <td>six</td>\n","      <td>medium</td>\n","      <td>2</td>\n","      <td>How many sister cities does the City of Melbou...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Is the electric eel a true eel?</td>\n","      <td>No</td>\n","      <td>easy</td>\n","      <td>0</td>\n","      <td>Is the electric eel a true eel? No</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Does Swedish use the perfect participle to for...</td>\n","      <td>No.</td>\n","      <td>easy</td>\n","      <td>0</td>\n","      <td>Does Swedish use the perfect participle to for...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>337</th>\n","      <td>Where was there a vast swarm of butterflies?</td>\n","      <td>In Kyoto there was a vast swarm of butterflies.</td>\n","      <td>medium</td>\n","      <td>2</td>\n","      <td>Where was there a vast swarm of butterflies? I...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>338</th>\n","      <td>What is the most common romanization standard ...</td>\n","      <td>Hanyu Pinyin</td>\n","      <td>medium</td>\n","      <td>2</td>\n","      <td>What is the most common romanization standard ...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>339</th>\n","      <td>Is Jakarta the 12th largest city in the world?</td>\n","      <td>yes</td>\n","      <td>medium</td>\n","      <td>2</td>\n","      <td>Is Jakarta the 12th largest city in the world?...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>340</th>\n","      <td>What sort of turtles are ectothermic?</td>\n","      <td>all of them</td>\n","      <td>medium</td>\n","      <td>2</td>\n","      <td>What sort of turtles are ectothermic? all of them</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>341</th>\n","      <td>Was Gellu Naum the leader of the surrealist mo...</td>\n","      <td>Yes</td>\n","      <td>easy</td>\n","      <td>0</td>\n","      <td>Was Gellu Naum the leader of the surrealist mo...</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>342 rows × 6 columns</p>\n","</div>"],"text/plain":["                                              Question  ... skill_label\n","0      How are western-style xylophones characterised?  ...           2\n","1                     Is Nairobi the capital of Kenya?  ...           3\n","2    How many sister cities does the City of Melbou...  ...           3\n","3                      Is the electric eel a true eel?  ...           3\n","4    Does Swedish use the perfect participle to for...  ...           3\n","..                                                 ...  ...         ...\n","337       Where was there a vast swarm of butterflies?  ...           3\n","338  What is the most common romanization standard ...  ...           3\n","339     Is Jakarta the 12th largest city in the world?  ...           2\n","340              What sort of turtles are ectothermic?  ...           2\n","341  Was Gellu Naum the leader of the surrealist mo...  ...           3\n","\n","[342 rows x 6 columns]"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"P8xfcPLU5sR5"},"source":["\n","# import re\n","\n","# test[\"question_answer\"] = test[\"question_answer\"].apply(lambda x : clean_sentence(x))\n","# test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EoXpzPoXelGC","outputId":"a7555290-a578-4df4-cf5f-d90f4bd073c0"},"source":["LE_skill.inverse_transform([3])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['Remembering'], dtype=object)"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"saWcQIAZgaLM","outputId":"41bc8792-ddca-423c-8dd9-011c13f328c8"},"source":["LE_skill.inverse_transform([2])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['Knowledge & understanding'], dtype=object)"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"PgDZi9yLfKvA"},"source":["import re\n","val[\"question_answer\"] = val[\"question_answer\"].apply(lambda x : clean_sentence(str(x)))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"29a3aeZMRuvD"},"source":["# Interactive attention difficulty prediction training"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["aca83b08638d400487e9d1e4362e0581","88b9437bde85450d9e006d3e02075f6d","f97201aca72f413c81b4804dd7a89261","2224beba045d493da73bd727b36343eb","06540715278341b590fe724e17157601","91ff315db32c4d2c92e0be280506ce2c","1c189ea102614e9eb5206b9b982e2230","2a153b539dec40339f4e3cbc94d74265","6512c4a556bf4216aa327153a12dff9a","fbfedfa6d5c44e6f8a83b98403f8fafb","e21ac2e146c3499299944f0c29c96420"]},"id":"FIrS5sxE3kgk","outputId":"b96d3c30-e8ab-4bf5-b2c1-8ebbda29f369"},"source":["from transformers import BertTokenizer\n","\n","# Load the BERT tokenizer.\n","print('Loading BERT tokenizer...')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading BERT tokenizer...\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aca83b08638d400487e9d1e4362e0581","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"wp64MkNB3kg1"},"source":["def get_labels(prediction):\n","    predicted_label =  LE_diff.inverse_transform([prediction])\n","    return predicted_label[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":580},"id":"I_UpqLMG3kg9","outputId":"f85ee870-318f-4c45-e70c-7f0cdb6c3898"},"source":["final_data"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Question</th>\n","      <th>Answer</th>\n","      <th>DifficultyFromAnswerer</th>\n","      <th>question_answer</th>\n","      <th>difficulty_label</th>\n","      <th>skill_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Is the dialect spoken in Jeju located in fact ...</td>\n","      <td>The dialect spoken in Jeju is in fact classifi...</td>\n","      <td>hard</td>\n","      <td>Is the dialect spoken in Jeju located in fact ...</td>\n","      <td>1</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>What cello manufacturer should I buy from if I...</td>\n","      <td>Luis &amp; Clark</td>\n","      <td>hard</td>\n","      <td>What cello manufacturer should I buy from if I...</td>\n","      <td>1</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Does it have a border with Norway?</td>\n","      <td>yes</td>\n","      <td>medium</td>\n","      <td>Does it have a border with Norway? yes</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>How many people use the bus network daily?</td>\n","      <td>More than 2.78 million people.</td>\n","      <td>easy</td>\n","      <td>How many people use the bus network daily? Mor...</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Who founded Montevideo?</td>\n","      <td>The Spanish.</td>\n","      <td>medium</td>\n","      <td>Who founded Montevideo? The Spanish.</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2763</th>\n","      <td>Did he become a professor before the revolutio...</td>\n","      <td>yes</td>\n","      <td>hard</td>\n","      <td>Did he become a professor before the revolutio...</td>\n","      <td>1</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2764</th>\n","      <td>Does Vietnamese borrow from Latin and Greek?</td>\n","      <td>No, Vietnamese does not borrow from Latin and ...</td>\n","      <td>medium</td>\n","      <td>Does Vietnamese borrow from Latin and Greek? N...</td>\n","      <td>2</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2765</th>\n","      <td>Where is San Francisco?</td>\n","      <td>San Francisco is in California.</td>\n","      <td>medium</td>\n","      <td>Where is San Francisco? San Francisco is in Ca...</td>\n","      <td>2</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2766</th>\n","      <td>What is the primary item in an otter's diet?</td>\n","      <td>fish</td>\n","      <td>medium</td>\n","      <td>What is the primary item in an otter's diet? fish</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2767</th>\n","      <td>Where are turtle eggs layed?</td>\n","      <td>Turtles lay eggs on land.</td>\n","      <td>hard</td>\n","      <td>Where are turtle eggs layed? Turtles lay eggs ...</td>\n","      <td>1</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2768 rows × 6 columns</p>\n","</div>"],"text/plain":["                                               Question  ... skill_label\n","0     Is the dialect spoken in Jeju located in fact ...  ...           3\n","1     What cello manufacturer should I buy from if I...  ...           3\n","2                    Does it have a border with Norway?  ...           2\n","3            How many people use the bus network daily?  ...           2\n","4                               Who founded Montevideo?  ...           2\n","...                                                 ...  ...         ...\n","2763  Did he become a professor before the revolutio...  ...           3\n","2764       Does Vietnamese borrow from Latin and Greek?  ...           3\n","2765                            Where is San Francisco?  ...           3\n","2766       What is the primary item in an otter's diet?  ...           2\n","2767                       Where are turtle eggs layed?  ...           3\n","\n","[2768 rows x 6 columns]"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"MHdVe13Fr3vt"},"source":["new_data = final_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AkyM7gqv3khI"},"source":["question_answer = new_data[\"question_answer\"].values\n","categories = new_data[\"difficulty_label\"].values\n","skill_category = new_data[\"skill_label\"].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ndpw0p1SBUoZ","outputId":"6fc2a765-1811-4f3b-d1d5-8d870204924f"},"source":["question_answer"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['Is the dialect spoken in Jeju located in fact classified as a different language by all Korean linguists? The dialect spoken in Jeju is in fact classified as a different language by some Korean linguists.',\n","       'What cello manufacturer should I buy from if I want to play outside? Luis & Clark',\n","       'Does it have a border with Norway? yes', ...,\n","       'Where is San Francisco? San Francisco is in California.',\n","       \"What is the primary item in an otter's diet? fish\",\n","       'Where are turtle eggs layed? Turtles lay eggs on land.'],\n","      dtype=object)"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"tFkS_H_83khL","outputId":"1ce90cff-2c92-4ac7-c2fd-382ae258396f"},"source":["question_answer[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Is the dialect spoken in Jeju located in fact classified as a different language by all Korean linguists? The dialect spoken in Jeju is in fact classified as a different language by some Korean linguists.'"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ian7gSDE3khR","outputId":"bdfbceec-4585-42c9-8609-ad8a89eaf63b"},"source":["len(categories)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2768"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y_ZeuHc63khU","outputId":"0a1479eb-392e-4ff5-e8b0-ab48ae51dde7"},"source":["input_ids = []\n","attention_masks = []\n","\n","for sent in question_answer:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens.\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 128,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        truncation=True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","# Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', question_answer[0])\n","print('Token IDs:', input_ids[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Original:  Is the dialect spoken in Jeju located in fact classified as a different language by all Korean linguists? The dialect spoken in Jeju is in fact classified as a different language by some Korean linguists.\n","Token IDs: tensor([  101,  2003,  1996,  9329,  5287,  1999, 15333,  9103,  2284,  1999,\n","         2755,  6219,  2004,  1037,  2367,  2653,  2011,  2035,  4759, 22978,\n","         2015,  1029,  1996,  9329,  5287,  1999, 15333,  9103,  2003,  1999,\n","         2755,  6219,  2004,  1037,  2367,  2653,  2011,  2070,  4759, 22978,\n","         2015,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iVGvVZb13kha","outputId":"2bd30795-d04a-45c0-d107-82f102e8e135"},"source":["print('Original: ', len(question_answer[1]))\n","print('Token IDs:', len(input_ids[1]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Original:  81\n","Token IDs: 128\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"u4wgF9bRMhNL"},"source":["val = val.dropna(subset=[\"question_answer\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5nmRiaBbA9OH"},"source":["val_text = val[\"question_answer\"].values\n","val_labels = val[\"difficulty_label\"].values\n","val_skill_labels = val[\"skill_label\"].values\n","test_text = test[\"question_answer\"].values\n","test_labels = test[\"difficulty_label\"].values\n","test_skill_labels = test[\"skill_label\"].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JcHwFZofMgIN"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E-s_H1WdyvCw","outputId":"81cd6283-09fd-443b-94a3-2841685b22e2"},"source":["test_labels"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([2, 0, 2, 0, 0, 0, 1, 2, 1, 2, 2, 1, 0, 0, 1, 1, 1, 0, 2, 2, 0, 2,\n","       1, 1, 1, 2, 0, 0, 0, 0, 2, 2, 2, 0, 0, 2, 1, 0, 1, 2, 1, 2, 2, 0,\n","       2, 0, 2, 2, 0, 2, 2, 1, 0, 2, 2, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 2,\n","       0, 0, 2, 2, 2, 2, 0, 0, 2, 2, 0, 2, 0, 2, 2, 2, 2, 0, 0, 0, 0, 2,\n","       0, 1, 0, 0, 0, 2, 2, 0, 1, 0, 0, 0, 0, 1, 1, 2, 2, 0, 2, 2, 0, 2,\n","       0, 0, 0, 1, 0, 2, 2, 1, 1, 1, 1, 2, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n","       0, 1, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 2, 0, 0, 1, 0, 2, 2, 2,\n","       2, 2, 0, 1, 2, 2, 0, 2, 2, 2, 1, 1, 2, 2, 1, 0, 0, 0, 2, 1, 1, 0,\n","       0, 0, 0, 0, 1, 2, 0, 2, 2, 2, 1, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0,\n","       2, 0, 0, 2, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0, 2, 2, 0, 0, 2, 2, 2, 1,\n","       2, 2, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 0, 1, 2, 2, 1, 0, 0, 0, 1,\n","       2, 0, 0, 2, 0, 2, 0, 0, 2, 1, 1, 2, 2, 0, 2, 0, 2, 2, 1, 2, 2, 2,\n","       0, 1, 2, 0, 1, 1, 2, 2, 0, 2, 1, 0, 2, 1, 0, 1, 1, 2, 0, 1, 1, 1,\n","       1, 2, 1, 0, 2, 0, 0, 2, 0, 1, 0, 1, 1, 0, 2, 0, 1, 2, 0, 0, 1, 0,\n","       1, 2, 1, 1, 1, 2, 2, 1, 2, 0, 2, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 2,\n","       2, 2, 1, 0, 1, 1, 2, 2, 2, 2, 2, 0])"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YF-mKCC1CUjD","outputId":"3e992498-1576-49bc-e154-2e036891d6c8"},"source":["val_labels"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 1, 0, 2, 0, 0, 1, 2, 2, 0, 2, 0, 1, 0, 0, 0, 0, 2, 2, 2, 0, 0,\n","       2, 1, 1, 0, 1, 2, 2, 1, 1, 0, 1, 2, 1, 1, 0, 0, 0, 1, 1, 2, 2, 0,\n","       1, 0, 1, 2, 0, 0, 0, 2, 0, 2, 0, 2, 0, 1, 0, 1, 0, 0, 2, 2, 1, 1,\n","       2, 1, 0, 1, 2, 0, 2, 2, 2, 1, 2, 2, 1, 2, 1, 1, 0, 2, 0, 1, 0, 0,\n","       1, 2, 2, 0, 0, 0, 2, 2, 1, 1, 0, 0, 1, 0, 2, 0, 1, 0, 2, 0, 0, 2,\n","       2, 1, 2, 2, 0, 1, 0, 0, 2, 0, 0, 0, 0, 2, 0, 2, 0, 2, 0, 2, 0, 0,\n","       0, 1, 0, 1, 2, 0, 1, 2, 1, 0, 1, 2, 2, 2, 2, 1, 2, 1, 0, 1, 2, 0,\n","       0, 1, 0, 2, 0, 2, 2, 0, 0, 1, 1, 2, 1, 1, 0, 2, 2, 0, 1, 1, 2, 0,\n","       0, 0, 1, 2, 2, 2, 1, 0, 1, 2, 2, 0, 0, 1, 0, 2, 2, 1, 0, 0, 0, 1,\n","       2, 1, 2, 1, 1, 2, 0, 1, 2, 0, 1, 1, 2, 1, 2, 0, 0, 1, 0, 0, 2, 1,\n","       0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 2, 2, 2, 1, 2, 1, 1, 0, 1, 0, 2, 1,\n","       1, 0, 2, 0, 0, 0, 1, 2, 0, 1, 0, 0, 2, 1, 2, 0, 1, 2, 0, 0, 0, 0,\n","       0, 0, 2, 0, 0, 2, 1, 1, 0, 2, 0, 0, 2, 0, 2, 2, 2, 2, 2, 0, 2, 0,\n","       0, 0, 0, 2, 2, 0, 0, 1, 1, 2, 1, 1, 2, 1, 2, 0, 0, 0, 2, 1, 0, 1])"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sOQuDahhAzOO","outputId":"c52ed346-2c53-472f-f654-4bb244028337"},"source":["val_input_ids = []\n","val_attention_masks = []\n","\n","for sent in val_text:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens.\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 128,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        truncation=True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    val_input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    val_attention_masks.append(encoded_dict['attention_mask'])\n","# Convert the lists into tensors.\n","val_input_ids = torch.cat(val_input_ids, dim=0)\n","val_attention_masks = torch.cat(val_attention_masks, dim=0)\n","\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', val_text[0])\n","print('Token IDs:', val_attention_masks[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Original:  What are turtle eggs covered in when they incubate? mud or sand\n","Token IDs: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0])\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Siskea7qDLUG","outputId":"a4c8d982-f8f0-4be9-e36b-552b8bf847c5"},"source":["print('Original: ', val_text[1])\n","print('Token IDs:', val_input_ids[1])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Original:  What is given for the number of native speakers? No figure is given for the number of native speakers.\n","Token IDs: tensor([ 101, 2054, 2003, 2445, 2005, 1996, 2193, 1997, 3128, 7492, 1029, 2053,\n","        3275, 2003, 2445, 2005, 1996, 2193, 1997, 3128, 7492, 1012,  102,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"irMTimjf3khd"},"source":["labels = torch.tensor(categories)\n","skill_category = torch.tensor(skill_category)\n","val_labels = torch.tensor(val_labels)\n","val_skill_labels = torch.tensor(val_skill_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PdOgWP_LKTHi","outputId":"e3993b80-cdc7-4378-e490-907adc6d3dda"},"source":["val_labels"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1, 1, 0, 2, 0, 0, 1, 2, 2, 0, 2, 0, 1, 0, 0, 0, 0, 2, 2, 2, 0, 0, 2, 1,\n","        1, 0, 1, 2, 2, 1, 1, 0, 1, 2, 1, 1, 0, 0, 0, 1, 1, 2, 2, 0, 1, 0, 1, 2,\n","        0, 0, 0, 2, 0, 2, 0, 2, 0, 1, 0, 1, 0, 0, 2, 2, 1, 1, 2, 1, 0, 1, 2, 0,\n","        2, 2, 2, 1, 2, 2, 1, 2, 1, 1, 0, 2, 0, 1, 0, 0, 1, 2, 2, 0, 0, 0, 2, 2,\n","        1, 1, 0, 0, 1, 0, 2, 0, 1, 0, 2, 0, 0, 2, 2, 1, 2, 2, 0, 1, 0, 0, 2, 0,\n","        0, 0, 0, 2, 0, 2, 0, 2, 0, 2, 0, 0, 0, 1, 0, 1, 2, 0, 1, 2, 1, 0, 1, 2,\n","        2, 2, 2, 1, 2, 1, 0, 1, 2, 0, 0, 1, 0, 2, 0, 2, 2, 0, 0, 1, 1, 2, 1, 1,\n","        0, 2, 2, 0, 1, 1, 2, 0, 0, 0, 1, 2, 2, 2, 1, 0, 1, 2, 2, 0, 0, 1, 0, 2,\n","        2, 1, 0, 0, 0, 1, 2, 1, 2, 1, 1, 2, 0, 1, 2, 0, 1, 1, 2, 1, 2, 0, 0, 1,\n","        0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 2, 2, 2, 1, 2, 1, 1, 0, 1, 0,\n","        2, 1, 1, 0, 2, 0, 0, 0, 1, 2, 0, 1, 0, 0, 2, 1, 2, 0, 1, 2, 0, 0, 0, 0,\n","        0, 0, 2, 0, 0, 2, 1, 1, 0, 2, 0, 0, 2, 0, 2, 2, 2, 2, 2, 0, 2, 0, 0, 0,\n","        0, 2, 2, 0, 0, 1, 1, 2, 1, 1, 2, 1, 2, 0, 0, 0, 2, 1, 0, 1])"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","metadata":{"id":"OJJ0I8Ud3khf"},"source":["get_labels(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z1ZAbQRfiG63","outputId":"00cb7378-214e-4810-d6bf-fb3378d40868"},"source":["len(set(final_data[\"question_answer\"].values).intersection(val[\"question_answer\"].values))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["46"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BNDW74Ny3khj","outputId":"ce2699a8-c093-4ed5-8cfd-46ca019173a3"},"source":["num_classes = len(list(set(categories)))\n","list(set(categories))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0, 1, 2]"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jOBErYPYEUrl","outputId":"19604ad3-4271-4974-970f-25c9c70a6a20"},"source":["skill_label_count = len(list(set(new_data[\"skill_label\"].values)))\n","skill_label_count"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jYFIJaRPE20c","outputId":"1239b20e-b4e7-45d5-9146-bf2a6f9f9371"},"source":["list(set(new_data[\"skill_label\"].values))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0, 1, 2, 3, 4]"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","metadata":{"id":"ZmaLk5Ab3khl"},"source":["from torch.utils.data import TensorDataset, random_split\n","# train_poincare_tensor = torch.tensor(poincare_embeddings_final,dtype=torch.float)\n","# difficulty_tensor = torch.tensor(difficulty_level_vectors,dtype=torch.float)\n","# Combine the training inputs into a TensorDataset.\n","dataset = TensorDataset(input_ids, attention_masks, labels,skill_category)\n","val_dataset = TensorDataset(val_input_ids, val_attention_masks,val_labels,val_skill_labels) \n","# Create a 90-10train-validation split.\n","\n","# Calculate the number of samples to include in each set.\n","# train_size = int(0.90 * len(dataset))\n","# val_size = len(dataset) - train_size\n","\n","# # Divide the dataset by randomly selecting samples.\n","# train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","# print('{:>5,} training samples'.format(train_size))\n","# # print('{:>5,} validation samples'.format(val_size))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D_lTinod3kho"},"source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","batch_size = 32\n","train_dataloader = DataLoader(\n","            dataset,  # The training samples.\n","            sampler = RandomSampler(dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","\n","validation_dataloader = DataLoader(\n","            val_dataset, # The validation samples.\n","            sampler = SequentialSampler(val_dataset), \n","            batch_size = batch_size \n","        )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_2tmAMlw3khr"},"source":["from transformers import BertModel, AdamW, BertConfig\n","\n","# # Loads BertForSequenceClassification, the pretrained BERT model with a single \n","# model = BertModel.from_pretrained(\n","#     \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n","# )\n","\n","# # Tell pytorch to run this model on the GPU.\n","# model.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DkDmTZhVChN6","outputId":"195a8678-082d-4676-8ddc-a2557926445f"},"source":["set(test[\"question_answer\"].values).intersection(set(final_data[\"question_answer\"].values))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'Approximately how many species of Testudines are alive today? 300',\n"," 'Are all dialects of Korean similar to each other? Yes',\n"," 'Are pocket trumpets compact B trumpets? yes',\n"," 'Are wolves built for stamina? Yes',\n"," 'Around how many recognized octopus species are there? There are around 300 recognized octopus species.',\n"," 'Copenhagen is the capital of what country? Denmark',\n"," 'Did Lincoln ever represent Alton & Sangamon Railroad? Yes',\n"," \"Did Monroe' wedding happen at the Trinity Church in New York? Yes\",\n"," 'Do linguists often view Chinese as a language family? Yes, linguists often view Chinese as a language family.',\n"," 'Does Modern Standard Arabic continue to evolve like other languages? yes',\n"," 'Does Theodore Roosevelt have a brother? Yes',\n"," 'Does every drumhead make the same sound? no',\n"," 'Does the octopus have a hard beak? Yes, the octopus has a hard beak.',\n"," 'Have cymbals been used historically to suggest bacchanal? Yes',\n"," 'How many children did Avogadro have? six',\n"," 'How many species of otter are there? 13',\n"," 'How old is the oldest known representation of a guitar-like intrument being played? 3,300 years old',\n"," 'How old was Celsius when he died? 42',\n"," \"Is English Ghana's official language? yes\",\n"," 'Is Liechtenstein heavily urbanized? No',\n"," 'Is Liechtenstein the smallest German-speaking country in the world? Yes',\n"," 'Is it a disadvantage for something to be unsafe to handle? yes',\n"," 'Is polar bear a mammal? Yes',\n"," 'Is the SI unit for radioactivity named after him? Yes',\n"," 'Was Abraham Lincoln the first President of the United States? No',\n"," 'Was Grover Cleveland the twenty-seventh president of the United States? No.',\n"," \"Was Henri Becquerel first in his family to occupy the physics chair at the Museum National d'Histoire Naturelle? No\",\n"," \"Was Isaac Newton educated at The King's Schol, Grantham? yes\",\n"," 'Was Thedore Roosevelt  a member of the Republican Party? Yes',\n"," 'Was the Italian 10.000 lira banknote created before the euro? yes',\n"," 'Were trumpet players heavily guarded? yes',\n"," \"What are the elephant's ears important for? temperature regulation\",\n"," \"What company administers Leichtenstein's railways? Austrian Federal Railways\",\n"," 'What do river otters eat? a variety of fish and shellfish, as well as small land mammals and birds',\n"," 'What is the life expectancy for men in Finland? 75 years',\n"," 'What is the most common romanization standard for Standard Mandarin today? Hanyu Pinyin',\n"," 'What is the smallest suborder of turtles? Pleurodira',\n"," \"What was Grant's political affiliation? Republican\",\n"," 'What year did Coolidge open his own law office? 1898',\n"," 'When did Isaac Newton discover the generalized binomial theorem? In 1665.',\n"," 'When did Roosevelt die? On January 6, 1919, Roosevelt died in his sleep.',\n"," 'When was the Six Day War? 1967',\n"," 'Where is Finland located? Northern Europe',\n"," 'Where was Isaac Newton born? Woolsthorpe Manor in Woolsthorpe-by-Colsterworth',\n"," 'Where was James Monroe born? Westmoreland County, Virginia',\n"," 'Where was the League of Nations created? Paris',\n"," 'Which type of beetle is a pest of potato plants? Colorado potato beetle',\n"," 'Who appointed Harlan Fiske Stone to the Supreme Court? Coolidge',\n"," \"Who is the mayor of Ottawa? Larry O'Brien\",\n"," 'Who was President when Wilson finished Congressional Government? Grover Cleveland',\n"," 'With what party did Adams run for presidency? The Federalist Party'}"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-AX-SagSE8CS","outputId":"847d9507-a314-476b-88c6-ceb86ccaeba6"},"source":["num_classes"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","metadata":{"id":"aHmQK0OP9mv-"},"source":["from torch import nn\n","# for plottign attentions\n","class MultiClassClassifier(nn.Module):\n","    def __init__(self, bert_model_path, labels_count,skill_label_count, hidden_dim=768, mlp_dim=500, extras_dim=140, dropout=0.1, freeze_bert=False):\n","        super().__init__()\n","\n","        self.bert = BertModel.from_pretrained(bert_model_path,output_hidden_states=True,output_attentions=True)\n","        self.dropout = nn.Dropout(dropout)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(hidden_dim , mlp_dim),\n","            nn.ReLU(),\n","            # nn.Linear(mlp_dim, mlp_dim),\n","            # nn.ReLU(),\n","            # nn.Linear(mlp_dim, mlp_dim),\n","            # nn.ReLU(),            \n","            nn.Linear(mlp_dim, labels_count)\n","        )\n","        self.mlp2 = nn.Sequential(\n","            nn.Linear(hidden_dim , mlp_dim),\n","            nn.ReLU(),         \n","            nn.Linear(mlp_dim, skill_label_count)\n","        )\n","        # self.softmax = nn.LogSoftmax(dim=1)\n","        if freeze_bert:\n","            print(\"Freezing layers\")\n","            for param in self.bert.parameters():\n","                param.requires_grad = False\n","\n","    def forward(self, tokens, masks):\n","        _, pooled_output,hidden_states,attentions = self.bert(tokens, attention_mask=masks)\n","        dropout_output = self.dropout(pooled_output)\n","        concat_output = dropout_output\n","        mlp_output = self.mlp(concat_output)\n","        skill_output = self.mlp2(concat_output)\n","        # proba = self.sigmoid(mlp_output)\n","        # proba = self.softmax(mlp_output)\n","\n","        return mlp_output,skill_output,attentions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZAgBxwiDbcIj"},"source":["from torch import nn\n","\n","\n","class Attention(nn.Module):\n","  def __init__(self,vector_1_dim,vector_2_dim):\n","    super(Attention, self).__init__()\n","    self.Weights = nn.Parameter(torch.rand(vector_2_dim,vector_1_dim))\n","    self.bias = nn.Parameter(torch.zeros(1))\n","\n","  def forward(self,vector_1,vector_2):\n","    #(batch_size,vector_2_dim,vector_1_dim)\n","    weights = self.Weights.repeat(vector_2.size(0),1,1)\n","    vector_1 = vector_1.unsqueeze(-1)  # (batch_size,vector_2_dim,vector_1_dim)\n","    weights = weights.matmul(vector_1) # results in (batch_size,vector_2_dim,1)\n","    weights = weights.repeat(vector_2.size(1),1,1,1).transpose(0,1)\n","    vector_2 = vector_2.unsqueeze(-2)\n","    attention_weights = torch.tanh(vector_2.matmul(weights).squeeze() + self.bias) # batch_size, vector_2_dim.size(0)\n","    if len(attention_weights.shape) ==1:\n","      attention_weights = attention_weights.squeeze()\n","      attention_weights = attention_weights.reshape(1,-1)\n","    attention_weights = attention_weights.squeeze()\n","    # print(\"torch.exp(attention_weights)\",torch.exp(attention_weights).shape,attention_weights.shape,torch.exp(attention_weights).sum(dim=1).shape)\n","    attention_weights = torch.exp(attention_weights)/ torch.exp(attention_weights).sum(dim=1,keepdim=True)\n","\n","    return attention_weights\n","\n","# bloom interactive attention\n","class MultiClassClassifier(nn.Module):\n","    def __init__(self, bert_model_path, labels_count,skill_label_count, hidden_dim=768, mlp_dim=500, extras_dim=140, dropout=0.1, freeze_bert=False):\n","        super().__init__()\n","\n","        self.bert = BertModel.from_pretrained(bert_model_path,output_hidden_states=True,output_attentions=True)\n","        self.dropout = nn.Dropout(dropout)\n","        self.bloom_attention = Attention(768, 768)\n","\n","        self.mlp = nn.Sequential(\n","            nn.Linear(hidden_dim , mlp_dim),\n","            nn.ReLU(),\n","            # nn.Linear(mlp_dim, mlp_dim),\n","            # nn.ReLU(),\n","            # nn.Linear(mlp_dim, mlp_dim),\n","            # nn.ReLU(),            \n","            nn.Linear(mlp_dim, labels_count)\n","        )\n","        self.mlp2 = nn.Sequential(  \n","            nn.Linear(hidden_dim , mlp_dim),\n","            nn.ReLU(),         \n","            nn.Linear(mlp_dim, skill_label_count)\n","        )\n","        # self.softmax = nn.LogSoftmax(dim=1)\n","        if freeze_bert:\n","            print(\"Freezing layers\")\n","            for param in self.bert.parameters():\n","                param.requires_grad = False\n","\n","    def forward(self, tokens, masks):\n","        _, pooled_output,hidden_states,attentions = self.bert(tokens, attention_mask=masks)\n","        dropout_output = self.dropout(pooled_output)\n","        concat_output = dropout_output\n","\n","        # mlp_output = self.mlp(concat_output)\n","        skill_output_probas = self.mlp2(concat_output)\n","        skill_output = torch.argmax(skill_output_probas,axis=1).cpu().numpy()\n","        skill_output = LE_skill.inverse_transform(skill_output)\n","        skill_input_ids = []\n","        skill_attention_masks = []\n","        for skill_text in skill_output:\n","          encoded_skill_output = tokenizer.encode_plus(\n","                          skill_text,                      # Sentence to encode.\n","                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                          max_length = 128,           # Pad & truncate all sentences.\n","                          pad_to_max_length = True,\n","                          truncation=True,\n","                          return_attention_mask = True,   # Construct attn. masks.\n","                          return_tensors = 'pt',     # Return pytorch tensors.\n","                    )\n","          skill_input_ids.append(encoded_skill_output['input_ids'])\n","          skill_attention_masks.append(encoded_skill_output['attention_mask'])\n","        skill_input_ids = torch.cat(skill_input_ids,dim=0).cuda()\n","        skill_attention_masks = torch.cat(skill_attention_masks,dim=0).cuda()\n","        _,_,hidden_states_skill,_ = self.bert(skill_input_ids,skill_attention_masks)\n","\n","        skill_hidden_averaged =  torch.sum(hidden_states_skill[12],dim=1)/hidden_states_skill[12].shape[1]\n","\n","        bloom_attention_weights = self.bloom_attention(skill_hidden_averaged, hidden_states[12])\n","\n","        bloom_attention_weights = bloom_attention_weights.unsqueeze(-2)\n","        # print(\"context_attention_weights\",context_attention_weights.shape,context_out.shape)\n","        input_attended_vector = bloom_attention_weights.matmul(hidden_states[12]).squeeze()\n","\n","        mlp_output = self.mlp(input_attended_vector)\n","\n","        # print(\"bloom attention weights\", bloom_attention_weights.shape)\n","        # print(\"_hidden_states\",hidden_states_skill[12].shape, hidden_states[12].shape,skill_hidden_averaged.shape)\n","\n","        # proba = self.sigmoid(mlp_output)\n","        # proba = self.softmax(mlp_output)\n","\n","        return mlp_output,skill_output_probas"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aGU0MvlQ97pt","outputId":"bff10ce3-7dd0-4bbf-c070-44e445892168"},"source":["model = MultiClassClassifier('bert-base-uncased',num_classes, skill_label_count,768,500,140,dropout=0.1,freeze_bert=False)\n","model.load_state_dict(torch.load(\"model_bert_multi_task_interactive_data_2_final/model_weights\"))\n","model.cuda()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MultiClassClassifier(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (bloom_attention): Attention()\n","  (mlp): Sequential(\n","    (0): Linear(in_features=768, out_features=500, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=500, out_features=3, bias=True)\n","  )\n","  (mlp2): Sequential(\n","    (0): Linear(in_features=768, out_features=500, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=500, out_features=5, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":175}]},{"cell_type":"code","metadata":{"id":"awQ2Y9Jb3kht"},"source":["optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Ys-M4-e3khv"},"source":["from transformers import get_linear_schedule_with_warmup\n","\n","\n","epochs = 20\n","\n","# Total number of training steps is [number of batches] x [number of epochs]. \n","total_steps = len(train_dataloader) * epochs\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EenVUl0iDyc1"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QrYqErOD3khx","outputId":"ce6e6822-b55d-42c4-f0ed-0088be6ff3b1"},"source":["len(train_dataloader) "],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["87"]},"metadata":{},"execution_count":104}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EWVSE9LM3kh0","outputId":"8664717d-f1f7-4f57-a121-80f96867f037"},"source":["1935 * 32"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["61920"]},"metadata":{},"execution_count":105}]},{"cell_type":"code","metadata":{"id":"rcvxVVi63kh3"},"source":["scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GUw3zm6g3kh5"},"source":["import numpy as np\n","\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ta6zfUTa3kh7"},"source":["import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GFq9gd5kQSHb"},"source":["import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y-rKyCrwE7N4","outputId":"4e763352-e3c9-4e1e-fbaf-aa835c3dc324"},"source":["# model.to(device)\n","model"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MultiClassClassifier(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (bloom_attention): Attention()\n","  (mlp): Sequential(\n","    (0): Linear(in_features=768, out_features=500, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=500, out_features=3, bias=True)\n","  )\n","  (mlp2): Sequential(\n","    (0): Linear(in_features=768, out_features=500, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=500, out_features=5, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":110}]},{"cell_type":"code","metadata":{"id":"1NuM6yRptFtS"},"source":["for param in model.bert.encoder.layer[0:5].parameters():\n","    param.requires_grad=False\n","for param in model.bert.embeddings.parameters():\n","    param.requires_grad=False\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a_nmuoSgQ5t3"},"source":["class EarlyStopping:\n","    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n","    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n","        \"\"\"\n","        Args:\n","            patience (int): How long to wait after last time validation loss improved.\n","                            Default: 7\n","            verbose (bool): If True, prints a message for each validation loss improvement. \n","                            Default: False\n","            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n","                            Default: 0\n","            path (str): Path for the checkpoint to be saved to.\n","                            Default: 'checkpoint.pt'\n","            trace_func (function): trace print function.\n","                            Default: print            \n","        \"\"\"\n","        self.patience = patience\n","        self.verbose = verbose\n","        self.counter = 0\n","        self.best_score = None\n","        self.early_stop = False\n","        self.val_loss_min = np.Inf\n","        self.delta = delta\n","        self.path = path\n","        self.trace_func = trace_func\n","    def __call__(self, val_loss, model):\n","\n","        score = -val_loss\n","\n","        if self.best_score is None:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","        elif score < self.best_score + self.delta:\n","            self.counter += 1\n","            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","            self.counter = 0\n","\n","    def save_checkpoint(self, val_loss, model):\n","        '''Saves model when validation loss decrease.'''\n","        if self.verbose:\n","            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n","        torch.save(model.state_dict(), self.path)\n","        self.val_loss_min = val_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E1rDO58zMfc8"},"source":["loss_func = nn.CrossEntropyLoss()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JNFL0393HQZc"},"source":[""]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6LhAy2hZ3kh9","outputId":"10a8eba6-2854-431c-b8da-ceddb02d3ef7"},"source":["import random\n","import numpy as np\n","\n","# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","# Set the seed value all over the place to make this reproducible.\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# We'll store a number of quantities such as training and validation loss, \n","# validation accuracy, and timings.\n","training_stats = []\n","\n","# Measure the total training time for the whole run.\n","total_t0 = time.time()\n","early_stopping = EarlyStopping(patience=2, verbose=True)\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_train_accuracy = 0\n","    total_train_loss = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questimport gensim.downloader as api\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","        # Progress update every 40 batches.\n","        if step % 40 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels\n","         \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        # b_poincare = batch[2].to(device)\n","        # b_difficulty = batch[3].to(device)\n","        b_labels = batch[2].to(device)\n","        skill_labels = batch[3].to(device)\n","\n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because \n","        # accumulating the gradients is \"convenient while training RNNs\". \n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        model.zero_grad()        \n","\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        probas, skill_probs = model(b_input_ids,b_input_mask)\n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","        loss_1 = loss_func(probas, b_labels)\n","        skill_loss = loss_func(skill_probs,skill_labels)\n","        loss = loss_1 + skill_loss\n","        total_train_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        # scheduler.step()\n","        logits = probas.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        total_train_accuracy += flat_accuracy(logits, label_ids)\n","    avg_train_accuracy = total_train_accuracy / len(train_dataloader)\n","    print(\" Train Accuracy: {0:.2f}\".format(avg_train_accuracy))\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(train_dataloader) \n","\n","            \n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(training_time))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n","    # Evaluate data for one epoch\n","    for batch in validation_dataloader:\n","        \n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using \n","        # the `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        # b_poincare = batch[2].to(device)\n","        # b_difficulty = batch[3].to(device)\n","        b_labels = batch[2].to(device)\n","        skill_labels = batch[3].to(device)\n","        \n","        # Tell pytorch not to bother with constructing the compute graph during\n","        # the forward pass, since this is only needed for backprop (training).\n","        with torch.no_grad():        \n","\n","            # Forward pass, calculate logit predictions.\n","\n","          logits, skill_logits = model(b_input_ids,b_input_mask)\n","            \n","        # Accumulate the validation loss.\n","        loss_1 = loss_func(logits, b_labels)\n","        skill_loss = loss_func(skill_logits,skill_labels)\n","        loss = loss_1 + skill_loss\n","        total_eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        # Calculate the accuracy for this batch of test sentences, and\n","        # accumulate it over all batches.\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","        \n","\n","    # Report the final accuracy for this validation run.\n","    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(validation_dataloader)\n","    early_stopping(avg_val_loss, model)\n","    if early_stopping.early_stop:\n","      print(\"Early stopping\")\n","      break    \n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","    \n","    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","    output_dir = 'model_bert_multi_task_interactive_data_2_final_2/'\n","    if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","    if early_stopping.counter==0:\n","      print(\"Saving model to %s\" % output_dir)\n","      tokenizer.save_pretrained(output_dir)\n","      torch.save(model.state_dict(), os.path.join(output_dir, 'model_weights'))\n","\n","      !rm -rf \"/content/drive/My Drive/research_skill_name_prediction/model_bert_multi_task_interactive_data_2_final_2\"\n","      !mv model_bert_multi_task_interactive_data_2_final_2 \"/content/drive/My Drive/research_skill_name_prediction/\"\n","    # Record all statistics from this epoch.\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i + 1,\n","            'Training Loss': avg_train_loss,\n","            'Valid. Loss': avg_val_loss,\n","            'Valid. Accur.': avg_val_accuracy,\n","            'Training Time': training_time,\n","            'Validation Time': validation_time\n","        }\n","    )\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"],"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","======== Epoch 1 / 20 ========\n","Training...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","text":["  Batch    40  of     87.    Elapsed: 0:01:20.\n","  Batch    80  of     87.    Elapsed: 0:02:39.\n"," Train Accuracy: 0.62\n","\n","  Average training loss: 1.81\n","  Training epcoh took: 0:02:52\n","\n","Running Validation...\n","  Accuracy: 0.64\n","Validation loss decreased (inf --> 1.623065).  Saving model ...\n","  Validation Loss: 1.62\n","  Validation took: 0:00:11\n","Saving model to model_bert_multi_task_interactive_data_2_final_2/\n","\n","======== Epoch 2 / 20 ========\n","Training...\n","  Batch    40  of     87.    Elapsed: 0:01:19.\n","  Batch    80  of     87.    Elapsed: 0:02:38.\n"," Train Accuracy: 0.68\n","\n","  Average training loss: 1.43\n","  Training epcoh took: 0:02:51\n","\n","Running Validation...\n","  Accuracy: 0.64\n","Validation loss decreased (1.623065 --> 1.356278).  Saving model ...\n","  Validation Loss: 1.36\n","  Validation took: 0:00:11\n","Saving model to model_bert_multi_task_interactive_data_2_final_2/\n","\n","======== Epoch 3 / 20 ========\n","Training...\n","  Batch    40  of     87.    Elapsed: 0:01:19.\n","  Batch    80  of     87.    Elapsed: 0:02:38.\n"," Train Accuracy: 0.71\n","\n","  Average training loss: 1.21\n","  Training epcoh took: 0:02:51\n","\n","Running Validation...\n","  Accuracy: 0.69\n","Validation loss decreased (1.356278 --> 1.259210).  Saving model ...\n","  Validation Loss: 1.26\n","  Validation took: 0:00:11\n","Saving model to model_bert_multi_task_interactive_data_2_final_2/\n","\n","======== Epoch 4 / 20 ========\n","Training...\n","  Batch    40  of     87.    Elapsed: 0:01:19.\n","  Batch    80  of     87.    Elapsed: 0:02:38.\n"," Train Accuracy: 0.73\n","\n","  Average training loss: 1.04\n","  Training epcoh took: 0:02:51\n","\n","Running Validation...\n","  Accuracy: 0.70\n","Validation loss decreased (1.259210 --> 1.207267).  Saving model ...\n","  Validation Loss: 1.21\n","  Validation took: 0:00:11\n","Saving model to model_bert_multi_task_interactive_data_2_final_2/\n","\n","======== Epoch 5 / 20 ========\n","Training...\n","  Batch    40  of     87.    Elapsed: 0:01:19.\n","  Batch    80  of     87.    Elapsed: 0:02:38.\n"," Train Accuracy: 0.76\n","\n","  Average training loss: 0.87\n","  Training epcoh took: 0:02:51\n","\n","Running Validation...\n","  Accuracy: 0.71\n","Validation loss decreased (1.207267 --> 1.189889).  Saving model ...\n","  Validation Loss: 1.19\n","  Validation took: 0:00:11\n","Saving model to model_bert_multi_task_interactive_data_2_final_2/\n","\n","======== Epoch 6 / 20 ========\n","Training...\n","  Batch    40  of     87.    Elapsed: 0:01:19.\n","  Batch    80  of     87.    Elapsed: 0:02:39.\n"," Train Accuracy: 0.79\n","\n","  Average training loss: 0.74\n","  Training epcoh took: 0:02:51\n","\n","Running Validation...\n","  Accuracy: 0.74\n","Validation loss decreased (1.189889 --> 1.152561).  Saving model ...\n","  Validation Loss: 1.15\n","  Validation took: 0:00:11\n","Saving model to model_bert_multi_task_interactive_data_2_final_2/\n","\n","======== Epoch 7 / 20 ========\n","Training...\n","  Batch    40  of     87.    Elapsed: 0:01:19.\n","  Batch    80  of     87.    Elapsed: 0:02:38.\n"," Train Accuracy: 0.82\n","\n","  Average training loss: 0.60\n","  Training epcoh took: 0:02:51\n","\n","Running Validation...\n","  Accuracy: 0.72\n","EarlyStopping counter: 1 out of 2\n","  Validation Loss: 1.24\n","  Validation took: 0:00:09\n","\n","======== Epoch 8 / 20 ========\n","Training...\n","  Batch    40  of     87.    Elapsed: 0:01:19.\n","  Batch    80  of     87.    Elapsed: 0:02:38.\n"," Train Accuracy: 0.85\n","\n","  Average training loss: 0.53\n","  Training epcoh took: 0:02:51\n","\n","Running Validation...\n","  Accuracy: 0.71\n","EarlyStopping counter: 2 out of 2\n","Early stopping\n","\n","Training complete!\n","Total training took 0:24:42 (h:mm:ss)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6RACcsko3kh_","colab":{"base_uri":"https://localhost:8080/","height":258},"outputId":"e07d06d0-993c-4026-e14e-aa604e5fa39e"},"source":["import pandas as pd\n","\n","# Display floats with two decimal places.\n","pd.set_option('precision', 2)\n","\n","# Create a DataFrame from our training statistics.\n","df_stats = pd.DataFrame(data=training_stats)\n","\n","# Use the 'epoch' as the row index.\n","df_stats = df_stats.set_index('epoch')\n","\n","# A hack to force the column headers to wrap.\n","#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n","\n","# Display the table.\n","df_stats"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Training Loss</th>\n","      <th>Valid. Loss</th>\n","      <th>Valid. Accur.</th>\n","      <th>Training Time</th>\n","      <th>Validation Time</th>\n","    </tr>\n","    <tr>\n","      <th>epoch</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>1.70</td>\n","      <td>1.46</td>\n","      <td>0.63</td>\n","      <td>0:02:54</td>\n","      <td>0:00:11</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1.36</td>\n","      <td>1.37</td>\n","      <td>0.62</td>\n","      <td>0:02:54</td>\n","      <td>0:00:11</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.20</td>\n","      <td>1.30</td>\n","      <td>0.67</td>\n","      <td>0:02:54</td>\n","      <td>0:00:11</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.06</td>\n","      <td>1.28</td>\n","      <td>0.70</td>\n","      <td>0:02:53</td>\n","      <td>0:00:11</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.93</td>\n","      <td>1.24</td>\n","      <td>0.70</td>\n","      <td>0:02:53</td>\n","      <td>0:00:11</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0.82</td>\n","      <td>1.30</td>\n","      <td>0.69</td>\n","      <td>0:02:52</td>\n","      <td>0:00:10</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n","epoch                                                                         \n","1               1.70         1.46           0.63       0:02:54         0:00:11\n","2               1.36         1.37           0.62       0:02:54         0:00:11\n","3               1.20         1.30           0.67       0:02:54         0:00:11\n","4               1.06         1.28           0.70       0:02:53         0:00:11\n","5               0.93         1.24           0.70       0:02:53         0:00:11\n","6               0.82         1.30           0.69       0:02:52         0:00:10"]},"metadata":{},"execution_count":67}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":427},"id":"o5TicdiP3kiC","outputId":"b2b58e1a-b7b6-421a-a12c-aabfab1794c8"},"source":["import matplotlib.pyplot as plt\n","% matplotlib inline\n","\n","import seaborn as sns\n","\n","# Use plot styling from seaborn.\n","sns.set(style='darkgrid')\n","\n","# Increase the plot size and font size.\n","sns.set(font_scale=1.5)\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","# Plot the learning curve.\n","plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n","plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n","\n","# Label the plot.\n","plt.title(\"Training & Validation Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.xticks([1, 2, 3, 4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20])\n","\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 864x432 with 1 Axes>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DbiTDpVv3kiF","outputId":"be3d9578-794c-46c7-ce81-822c2a4295e9"},"source":["import os\n","\n","\n","output_dir = 'model_bert_multi_task_interactive_data_2_final/'\n","\n","# Create output directory if needed\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","print(\"Saving model to %s\" % output_dir)\n","\n","\n","# model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n","# model_to_save.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Saving model to model_bert_multi_task_interactive_data_2_final/\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["('model_bert_multi_task_interactive_data_2_final/vocab.txt',\n"," 'model_bert_multi_task_interactive_data_2_final/special_tokens_map.json',\n"," 'model_bert_multi_task_interactive_data_2_final/added_tokens.json')"]},"metadata":{},"execution_count":69}]},{"cell_type":"code","metadata":{"id":"Kq9ByBbSjJlx"},"source":["  import json\n","  torch.save(model.state_dict(), os.path.join(output_dir, 'model_weights'))\n","  # with open(os.path.join(output_dir, 'model_config.json'), 'w') as f:\n","  #     json.dump(model.config, f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1U2UQ29a3kiI"},"source":["# !pip install joblib\n","# import joblib\n","# joblib.dump(LE, \"label_encoder_BLOOM_LATEST\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4GFnEkPP3kiP"},"source":["from google.colab import files\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dpGY8vSDI6u4","outputId":"3ed708a4-543f-40b1-cf62-b0fda18516c8"},"source":["!zip -r model_bert_multi_task_interactive_data_2_final_2.zip model_bert_multi_task_interactive_data_2_final_2\n","# files.download('model_bert_difficulty_prediction.zip')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  adding: model_bert_multi_task_interactive_data_2_final_2/ (stored 0%)\n","  adding: model_bert_multi_task_interactive_data_2_final_2/vocab.txt (deflated 53%)\n","  adding: model_bert_multi_task_interactive_data_2_final_2/tokenizer_config.json (stored 0%)\n","  adding: model_bert_multi_task_interactive_data_2_final_2/special_tokens_map.json (deflated 40%)\n","  adding: model_bert_multi_task_interactive_data_2_final_2/model_weights (deflated 7%)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HvFDCDIxKDOf"},"source":["# !zip -r label_encoder_BLOOM_LATEST.zip label_encoder_BLOOM_LATEST\n","# files.download('label_encoder_BLOOM_LATEST.zip')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":545},"id":"FcSwwzAFyE7S","outputId":"6a68ab43-c0e0-458f-f44c-51e4a730ddd2"},"source":["\n","test"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Question</th>\n","      <th>Answer</th>\n","      <th>DifficultyFromAnswerer</th>\n","      <th>difficulty_label</th>\n","      <th>question_answer</th>\n","      <th>skill_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>How are western-style xylophones characterised?</td>\n","      <td>by a bright, sharp tone and high register</td>\n","      <td>medium</td>\n","      <td>2</td>\n","      <td>How are western-style xylophones characterised...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Is Nairobi the capital of Kenya?</td>\n","      <td>Yes</td>\n","      <td>easy</td>\n","      <td>0</td>\n","      <td>Is Nairobi the capital of Kenya? Yes</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>How many sister cities does the City of Melbou...</td>\n","      <td>six</td>\n","      <td>medium</td>\n","      <td>2</td>\n","      <td>How many sister cities does the City of Melbou...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Is the electric eel a true eel?</td>\n","      <td>No</td>\n","      <td>easy</td>\n","      <td>0</td>\n","      <td>Is the electric eel a true eel? No</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Does Swedish use the perfect participle to for...</td>\n","      <td>No.</td>\n","      <td>easy</td>\n","      <td>0</td>\n","      <td>Does Swedish use the perfect participle to for...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>337</th>\n","      <td>Where was there a vast swarm of butterflies?</td>\n","      <td>In Kyoto there was a vast swarm of butterflies.</td>\n","      <td>medium</td>\n","      <td>2</td>\n","      <td>Where was there a vast swarm of butterflies? I...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>338</th>\n","      <td>What is the most common romanization standard ...</td>\n","      <td>Hanyu Pinyin</td>\n","      <td>medium</td>\n","      <td>2</td>\n","      <td>What is the most common romanization standard ...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>339</th>\n","      <td>Is Jakarta the 12th largest city in the world?</td>\n","      <td>yes</td>\n","      <td>medium</td>\n","      <td>2</td>\n","      <td>Is Jakarta the 12th largest city in the world?...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>340</th>\n","      <td>What sort of turtles are ectothermic?</td>\n","      <td>all of them</td>\n","      <td>medium</td>\n","      <td>2</td>\n","      <td>What sort of turtles are ectothermic? all of them</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>341</th>\n","      <td>Was Gellu Naum the leader of the surrealist mo...</td>\n","      <td>Yes</td>\n","      <td>easy</td>\n","      <td>0</td>\n","      <td>Was Gellu Naum the leader of the surrealist mo...</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>342 rows × 6 columns</p>\n","</div>"],"text/plain":["                                              Question  ... skill_label\n","0      How are western-style xylophones characterised?  ...           2\n","1                     Is Nairobi the capital of Kenya?  ...           3\n","2    How many sister cities does the City of Melbou...  ...           3\n","3                      Is the electric eel a true eel?  ...           3\n","4    Does Swedish use the perfect participle to for...  ...           3\n","..                                                 ...  ...         ...\n","337       Where was there a vast swarm of butterflies?  ...           3\n","338  What is the most common romanization standard ...  ...           3\n","339     Is Jakarta the 12th largest city in the world?  ...           2\n","340              What sort of turtles are ectothermic?  ...           2\n","341  Was Gellu Naum the leader of the surrealist mo...  ...           3\n","\n","[342 rows x 6 columns]"]},"metadata":{},"execution_count":115}]},{"cell_type":"code","metadata":{"id":"4178_yLFMWmx"},"source":["test_features = test[\"question_answer\"].values\n","test_labels = test[\"difficulty_label\"].values\n","test_skill_labels = test[\"skill_label\"].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DggP9Sxdv_-l","outputId":"f8485466-e7fc-4b08-df31-f6c1f2cfcfb9"},"source":["test_labels"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([2, 0, 2, 0, 0, 0, 1, 2, 1, 2, 2, 1, 0, 0, 1, 1, 1, 0, 2, 2, 0, 2,\n","       1, 1, 1, 2, 0, 0, 0, 0, 2, 2, 2, 0, 0, 2, 1, 0, 1, 2, 1, 2, 2, 0,\n","       2, 0, 2, 2, 0, 2, 2, 1, 0, 2, 2, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 2,\n","       0, 0, 2, 2, 2, 2, 0, 0, 2, 2, 0, 2, 0, 2, 2, 2, 2, 0, 0, 0, 0, 2,\n","       0, 1, 0, 0, 0, 2, 2, 0, 1, 0, 0, 0, 0, 1, 1, 2, 2, 0, 2, 2, 0, 2,\n","       0, 0, 0, 1, 0, 2, 2, 1, 1, 1, 1, 2, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n","       0, 1, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 2, 0, 0, 1, 0, 2, 2, 2,\n","       2, 2, 0, 1, 2, 2, 0, 2, 2, 2, 1, 1, 2, 2, 1, 0, 0, 0, 2, 1, 1, 0,\n","       0, 0, 0, 0, 1, 2, 0, 2, 2, 2, 1, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0,\n","       2, 0, 0, 2, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0, 2, 2, 0, 0, 2, 2, 2, 1,\n","       2, 2, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 0, 1, 2, 2, 1, 0, 0, 0, 1,\n","       2, 0, 0, 2, 0, 2, 0, 0, 2, 1, 1, 2, 2, 0, 2, 0, 2, 2, 1, 2, 2, 2,\n","       0, 1, 2, 0, 1, 1, 2, 2, 0, 2, 1, 0, 2, 1, 0, 1, 1, 2, 0, 1, 1, 1,\n","       1, 2, 1, 0, 2, 0, 0, 2, 0, 1, 0, 1, 1, 0, 2, 0, 1, 2, 0, 0, 1, 0,\n","       1, 2, 1, 1, 1, 2, 2, 1, 2, 0, 2, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 2,\n","       2, 2, 1, 0, 1, 1, 2, 2, 2, 2, 2, 0])"]},"metadata":{},"execution_count":117}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"MlOvANUwprAw","outputId":"30eb8280-8383-4bd6-c008-b0be153eda1d"},"source":["test_features[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'How are western-style xylophones characterised? by a bright, sharp tone and high register'"]},"metadata":{},"execution_count":119}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"-nCVhlaoXaXM","outputId":"82e117bc-feb2-4352-cc9c-756694461894"},"source":["test_features[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'How are western-style xylophones characterised? by a bright, sharp tone and high register'"]},"metadata":{},"execution_count":120}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qe4qYkV2C4fX","outputId":"69355ac0-399e-461f-b8b3-7ea10ba0445a"},"source":["input_ids = []\n","attention_masks = []\n","for sent in test_features:\n","\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 128,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        truncation=True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","test_labels = torch.tensor(test_labels)\n","test_skill_labels = torch.tensor(test_skill_labels)\n","\n","# Set the batch size.  \n","batch_size = 34\n","# test_poincare_tensor = torch.tensor(poincare_embeddings_final,dtype=torch.float)\n","# print(test_poincare_tensor.shape)\n","# difficulty_tensor = torch.tensor(difficulty_level_vectors,dtype=torch.float)\n","# print(\"difficulty_tensor\",difficulty_tensor.shape)\n","# Combine the training inputs into a TensorDataset.\n","prediction_data = TensorDataset(input_ids, attention_masks, test_labels,test_skill_labels)\n","# Create the DataLoader.\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mPCktQT9DVT4","outputId":"4c8c8e9d-6562-46c4-c244-f3f6c5bf4a83"},"source":["# Prediction on test set\n","\n","print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions,skill_predictions , true_labels, true_skill_labels = [], [], [], []\n","\n","# Predict ea\n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  \n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels,skill_labels = batch\n","  \n","  # Telling the model not to compute or store gradients, saving memory and \n","  # speeding up prediction\n","  # print(\"b_input_ids\",b_input_ids.shape)\n","  with torch.no_grad():\n","      # Forward pass, calculate logit predictions\n","      outputs,skill_ouputs = model(b_input_ids,b_input_mask)\n","\n","  logits = outputs\n","  skill_logits = skill_ouputs\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  skill_logits = skill_logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  skill_labels = skill_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  skill_predictions.append(skill_logits)\n","  true_labels.append(label_ids)\n","  true_skill_labels.append(skill_labels)\n","\n","\n","print('    DONE.')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Predicting labels for 342 test sentences...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["    DONE.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"U_WchmXtDspr"},"source":["print('Positive samples: %d of %d (%.2f%%)' % (final_data.label.sum(), len(final_data.label), (final_data.label.sum() / len(final_data.label) * 100.0)))"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7vbdVvUXDxgf","outputId":"f45959e2-f9e7-4de2-82bb-c9bf77a1493e"},"source":["true_skill_labels[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([2, 3, 3, 3, 3, 2, 3, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 3, 3, 3,\n","       4, 3, 3, 3, 2, 3, 3, 2, 3, 2, 2, 2])"]},"metadata":{},"execution_count":177}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s5BoY2hKGb7_","outputId":"3973187c-b1b1-41b7-8a28-8ee5e722f629"},"source":["import numpy as np\n","pred =  np.argmax(predictions[0],axis=1).flatten()\n","pred"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([2, 0, 2, 0, 0, 0, 1, 0, 1, 2, 2, 2, 0, 1, 1, 0, 1, 0, 2, 2, 0, 2,\n","       2, 1, 0, 2, 0, 0, 0, 0, 2, 0, 2, 1])"]},"metadata":{},"execution_count":178}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tPuK0-vzGp3R","outputId":"dede2ca1-abfd-4f0d-95c8-92e7d7fcda06"},"source":["from sklearn.metrics import matthews_corrcoef\n","\n","matthews_set = []\n","\n","# Evaluate each test batch using Matthew's correlation coefficient\n","print('Calculating Matthews Corr. Coef. for each batch...')\n","\n","# For each input batch...\n","for i in range(len(true_labels)):\n","  \n","  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n","  # and one column for \"1\"). Pick the label with the highest value and turn this\n","  # in to a list of 0s and 1s.\n","  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n","  \n","  # Calculate and store the coef for this batch.  \n","  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n","  matthews_set.append(matthews)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Calculating Matthews Corr. Coef. for each batch...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i5MKFS0iXm7l","outputId":"b9e2285e-1f58-4ce4-924e-b8a02ce09c84"},"source":["import numpy as np\n","flat_predictions = np.concatenate(predictions, axis=0)\n","\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","\n","# Combine the correct labels for each batch into a single list.\n","flat_true_labels = np.concatenate(true_labels, axis=0)\n","\n","# Calculate the MCC\n","mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n","\n","print('Total MCC: %.3f' % mcc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total MCC: 0.552\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nOTqUYPmEUCn","outputId":"0a3b7ca7-7051-4a37-aecd-0d0782d04ed5"},"source":["flat_skill_predictions = np.concatenate(skill_predictions, axis=0)\n","\n","flat_skill_predictions = np.argmax(flat_skill_predictions, axis=1).flatten()\n","\n","# Combine the correct labels for each batch into a single list.\n","flat_true_skill_labels = np.concatenate(true_skill_labels, axis=0)\n","\n","# Calculate the MCC\n","mcc = matthews_corrcoef(flat_true_skill_labels, flat_skill_predictions)\n","\n","print('Total MCC: %.3f' % mcc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total MCC: 0.655\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FD3aaycBz4DR","outputId":"57ef417e-93dd-4b06-f546-a9738ba4fc46"},"source":["len(flat_true_skill_labels)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["342"]},"metadata":{},"execution_count":182}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"EdmAmoA0Y4zS","outputId":"aef9e5aa-efa9-49cb-c3dd-19fac72f8bba"},"source":["question_answer[30]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Did Mainland Nova Scotia not come under British rule with the Treaty of Utrecht    (: ; ? no!'"]},"metadata":{},"execution_count":184}]},{"cell_type":"code","metadata":{"id":"zlqpVfk-NW_F"},"source":["def accuracy_per_class(preds_flat, labels_flat):\n","\n","    for label in np.unique(labels_flat):\n","        y_preds = preds_flat[labels_flat==label]\n","        y_true = labels_flat[labels_flat==label]\n","        print(f'Class: {get_labels(label)}')\n","        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o29QuEYW-mzm","outputId":"42104c29-277a-4309-eaf8-9ae955a7b133"},"source":["accuracy_per_class(flat_predictions,flat_true_labels)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Class: easy\n","Accuracy: 115/134\n","\n","Class: hard\n","Accuracy: 43/85\n","\n","Class: medium\n","Accuracy: 84/123\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ed7_zfiDNaOv","outputId":"19ca8316-7824-4df1-e5d0-98c3cf194280"},"source":["accuracy_per_class(flat_predictions,flat_true_labels)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Class: easy\n","Accuracy: 115/134\n","\n","Class: hard\n","Accuracy: 43/85\n","\n","Class: medium\n","Accuracy: 84/123\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v48rDl4JHmhv","outputId":"89b0d5e6-3bd5-4ec3-8cca-e017110c5b32"},"source":["flat_predictions = np.concatenate(predictions, axis=0)\n","\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","\n","# Combine the correct labels for each batch into a single list.\n","flat_true_labels = np.concatenate(true_labels, axis=0)\n","\n","# Calculate the MCC\n","mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n","\n","print('Total MCC: %.3f' % mcc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total MCC: 0.552\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O5f5p01wfLM6","outputId":"27d117a0-0b39-4248-c840-596a678a331a"},"source":["flat_skill_predictions"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([2, 2, 3, 2, 3, 2, 3, 3, 2, 2, 2, 3, 2, 3, 2, 3, 2, 3, 3, 2, 3, 3,\n","       3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2,\n","       3, 3, 3, 2, 3, 3, 4, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3,\n","       3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3,\n","       3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 2, 3, 2, 2, 1, 2, 2, 3, 3, 3, 3, 2,\n","       3, 3, 2, 3, 3, 2, 3, 4, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3, 2, 2, 3,\n","       3, 3, 2, 3, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2, 4, 3, 3, 3, 2, 3, 3,\n","       3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2, 2,\n","       3, 3, 2, 2, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3,\n","       3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3,\n","       3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 4, 3, 2, 2, 3, 2, 2, 4, 3, 2, 3, 2,\n","       3, 2, 2, 3, 3, 2, 2, 3, 2, 2, 3, 2, 3, 2, 3, 2, 3, 3, 4, 3, 2, 3,\n","       3, 3, 3, 2, 2, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4, 2, 3, 3,\n","       3, 2, 2, 3, 3, 3, 1, 3, 2, 3, 3, 2, 3, 2, 3, 2, 3, 2, 2, 3, 3, 3,\n","       2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3,\n","       3, 2, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3])"]},"metadata":{},"execution_count":190}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MI8-HrJ6MSEk","outputId":"83902012-9081-448c-b4d1-79558087283e"},"source":["list_bool = (flat_true_labels==flat_predictions)\n","print(list_bool)\n","print(len([i for i, val in enumerate(list_bool) if val]))\n","len(flat_predictions)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[ True  True  True  True  True  True  True False  True  True  True False\n","  True False  True False  True  True  True  True  True  True False  True\n"," False  True  True  True  True  True  True False  True False  True  True\n","  True  True False  True False  True  True False  True  True  True False\n","  True False False False  True False  True  True  True False False False\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True False  True\n","  True False  True  True  True False  True  True  True False  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True False  True  True  True  True False  True\n","  True  True  True  True  True False False False  True  True  True  True\n","  True  True False  True  True  True  True  True  True  True  True  True\n","  True  True  True  True False  True False False False  True False False\n","  True False False  True  True  True  True False False False False  True\n","  True  True  True  True False False False  True False  True  True  True\n","  True  True  True False False  True False  True  True  True False  True\n","  True  True False False  True  True  True  True  True False False  True\n","  True False  True  True False  True  True  True  True  True False False\n","  True  True False False  True  True  True False False False  True False\n"," False  True False False  True False  True False  True  True False  True\n","  True False  True  True  True  True  True False  True  True  True  True\n","  True False  True  True  True  True False  True False  True False False\n"," False  True  True  True False False False False  True False  True  True\n","  True  True  True  True False  True  True False  True False  True  True\n","  True  True  True False  True False  True  True  True False False  True\n","  True  True  True  True  True  True False  True  True  True False False\n","  True  True  True False  True  True False  True  True  True False  True\n","  True  True False  True False  True  True  True  True  True  True  True\n","  True False  True False False  True]\n","242\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["342"]},"metadata":{},"execution_count":191}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f8K0OFM-c7Fv","outputId":"249d526f-4719-4308-d8cd-c89e464fbcd1"},"source":["from sklearn.metrics import precision_recall_fscore_support\n","#difficulty\n","print(precision_recall_fscore_support(flat_true_labels, flat_predictions, average='micro'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(0.7076023391812866, 0.7076023391812866, 0.7076023391812865, None)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"44lMA088c7Fv","outputId":"c0d13f88-825b-4381-9427-e9fea15d79f9"},"source":["from sklearn.metrics import precision_recall_fscore_support\n","#difficulty\n","print(precision_recall_fscore_support(flat_true_labels, flat_predictions, average='macro'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(0.6843559944841658, 0.68233937914445, 0.6825522557788003, None)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ul7CnYqIYjmH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"33e66712-2511-4035-c28a-8d4df277a5d8"},"source":["from sklearn.metrics import precision_recall_fscore_support\n","#difficulty\n","print(precision_recall_fscore_support(flat_true_labels, flat_predictions, average='weighted'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(0.7015667037101829, 0.7076023391812866, 0.7038898023481908, None)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TBpMQFrz9Als","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5fedaf00-30e3-4b73-cec3-5d9bb5f7bf7e"},"source":["from sklearn.metrics import precision_recall_fscore_support\n","#skill_name\n","print(precision_recall_fscore_support(flat_true_skill_labels, flat_skill_predictions, average='micro'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(0.8304093567251462, 0.8304093567251462, 0.8304093567251462, None)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hcaAMXG69Als","outputId":"18fc9e9b-9967-400e-8ac5-f9f817be08a0"},"source":["from sklearn.metrics import precision_recall_fscore_support\n","#skill name\n","print(precision_recall_fscore_support(flat_true_skill_labels, flat_skill_predictions, average='macro'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(0.493588195841717, 0.41955091937765204, 0.4387014788953139, None)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zivypLKV9Als","outputId":"e5279319-81e7-433e-ae74-17eda24160bf"},"source":["from sklearn.metrics import precision_recall_fscore_support\n","#skill name\n","print(precision_recall_fscore_support(flat_true_skill_labels, flat_skill_predictions, average='weighted'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(0.840865269590251, 0.8304093567251462, 0.8299514442085165, None)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"zFediYEjlKjX"},"source":["def get_confusion_matrix(predicted,actual):\n","    conf_matrix = np.zeros((5, 5))\n","    for pred,act in zip(predicted,actual):\n","        conf_matrix[act,pred]+=1\n","    return conf_matrix\n","        \n","def get_TP(confusion_matrix,label):\n","    tp = confusion_matrix[label][label]\n","    return tp\n","\n","def get_FN(confusion_matrix,label):\n","    row = confusion_matrix[label,]\n","    row_truepositives = row[label]\n","    fn = row.sum() - row_truepositives\n","    return fn\n","\n","def get_FP(confusion_matrix,tag):\n","    col = confusion_matrix[:,tag]\n","    col_tp = col[tag]\n","    #  sum of all values in column except tp\n","    fp = col.sum() - col_tp\n","    return fp\n","def Precision(conf_matrix):\n","    precision = 0.0\n","    for label in [0,1,2,3,4]:\n","        dividor= get_TP(conf_matrix,label)+get_FP(conf_matrix,label)\n","        if dividor != 0.0:\n","            precision += (get_TP(conf_matrix,label))/dividor\n","    return (precision / 5)\n","\n","def Recall(conf_matrix):\n","    recall = 0.0\n","    for label in [0,1,2,3,4]:\n","        dividor=get_TP(conf_matrix,label)+get_FN(conf_matrix,label)\n","        if dividor != 0.0:\n","            recall += (get_TP(conf_matrix,label))/dividor\n","    return (recall / 5)\n","\n","def F1(precision,recall):\n","    return (2*precision*recall)/(precision+recall)\n","def accuracy_per_class(preds_flat, labels_flat):\n","\n","    for label in np.unique(labels_flat):\n","        y_preds = preds_flat[labels_flat==label]\n","        y_true = labels_flat[labels_flat==label]\n","        print(f'Class: {label}')\n","        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')\n","def print_metrics(predictions,test_labels):\n","    conf_matrix = get_confusion_matrix(predictions,test_labels)\n","    precision = Precision(conf_matrix)\n","    recall = Recall(conf_matrix)\n","    f1_score = F1(precision,recall)\n","    print(\"Macro : Precision:{}, Recall: {}, F1: {}\".format(precision,recall,f1_score))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aFEi23ejPfOm"},"source":["def Precision_macro_weighted(conf_matrix,test_samples):\n","    accum =0\n","    label_wise_precision = dict()\n","    for label in [0,1,2,3,4]:\n","        true_sample = [sample for sample in test_samples if sample==label ]\n","        if (get_TP(conf_matrix,label)+get_FP(conf_matrix,label))!=0:\n","            accum+= float(len(true_sample)) *(get_TP(conf_matrix,label)/(get_TP(conf_matrix,label)+get_FP(conf_matrix,label)))\n","            label_wise_precision[label] = get_TP(conf_matrix,label)/(get_TP(conf_matrix,label)+get_FP(conf_matrix,label))\n","\n","    \n","    precision =  accum/len(test_samples)\n","            \n","    return precision\n","\n","\n","def Recall_macro_weighted(conf_matrix,test_samples):\n","    accum =0\n","    label_wise_recall = dict()\n","    for label in [0,1,2,3,4]:\n","        true_sample = [sample for sample in test_samples if sample==label ]\n","\n","        if (get_TP(conf_matrix,label)+get_FN(conf_matrix,label))!=0:\n","            accum+= float(len(true_sample)) * (get_TP(conf_matrix,label)/(get_TP(conf_matrix,label)+get_FN(conf_matrix,label)))\n","            label_wise_recall[label] = get_TP(conf_matrix,label)/(get_TP(conf_matrix,label)+get_FN(conf_matrix,label))\n","\n","    \n","    recall =  accum/len(test_samples)\n","    return recall\n","def print_weighted_metrics(predictions,test_labels):\n","    conf_matrix = get_confusion_matrix(predictions,test_labels)\n","    precision = Precision_macro_weighted(conf_matrix,test_labels)\n","    recall = Recall_macro_weighted(conf_matrix,test_labels)\n","    f1_score = F1(precision,recall)\n","    print(\"Macro : Precision:{}, Recall: {}, F1: {}\".format(precision,recall,f1_score))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ylgUKDwyRT0B"},"source":["def get_confusion_matrix(predicted,actual):\n","    conf_matrix = np.zeros((3, 3))\n","    for pred,act in zip(predicted,actual):\n","        conf_matrix[act,pred]+=1\n","    return conf_matrix\n","        \n","def get_TP(confusion_matrix,label):\n","    tp = confusion_matrix[label][label]\n","    return tp\n","\n","def get_FN(confusion_matrix,label):\n","    row = confusion_matrix[label,]\n","    row_truepositives = row[label]\n","    fn = row.sum() - row_truepositives\n","    return fn\n","\n","def get_FP(confusion_matrix,tag):\n","    col = confusion_matrix[:,tag]\n","    col_tp = col[tag]\n","    #  sum of all values in column except tp\n","    fp = col.sum() - col_tp\n","    return fp\n","def Precision(conf_matrix):\n","    precision = 0.0\n","    for label in [0,1,2]:\n","        dividor= get_TP(conf_matrix,label)+get_FP(conf_matrix,label)\n","        if dividor != 0.0:\n","            precision += (get_TP(conf_matrix,label))/dividor\n","    return (precision / 3)\n","\n","def Recall(conf_matrix):\n","    recall = 0.0\n","    for label in [0,1,2]:\n","        dividor=get_TP(conf_matrix,label)+get_FN(conf_matrix,label)\n","        if dividor != 0.0:\n","            recall += (get_TP(conf_matrix,label))/dividor\n","    return (recall / 3)\n","\n","def F1(precision,recall):\n","    return (2*precision*recall)/(precision+recall)\n","def accuracy_per_class(preds_flat, labels_flat):\n","\n","    for label in np.unique(labels_flat):\n","        y_preds = preds_flat[labels_flat==label]\n","        y_true = labels_flat[labels_flat==label]\n","        print(f'Class: {label}')\n","        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')\n","def print_metrics(predictions,test_labels):\n","    conf_matrix = get_confusion_matrix(predictions,test_labels)\n","    precision = Precision(conf_matrix)\n","    recall = Recall(conf_matrix)\n","    f1_score = F1(precision,recall)\n","    print(\"Macro : Precision:{}, Recall: {}, F1: {}\".format(precision,recall,f1_score))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4zyjYkYZRT0D"},"source":["def Precision_macro_weighted(conf_matrix,test_samples):\n","    accum =0\n","    label_wise_precision = dict()\n","    for label in [0,1,2]:\n","        true_sample = [sample for sample in test_samples if sample==label ]\n","        if (get_TP(conf_matrix,label)+get_FP(conf_matrix,label))!=0:\n","            accum+= float(len(true_sample)) *(get_TP(conf_matrix,label)/(get_TP(conf_matrix,label)+get_FP(conf_matrix,label)))\n","            label_wise_precision[label] = get_TP(conf_matrix,label)/(get_TP(conf_matrix,label)+get_FP(conf_matrix,label))\n","\n","    \n","    precision =  accum/len(test_samples)\n","            \n","    return precision\n","\n","\n","def Recall_macro_weighted(conf_matrix,test_samples):\n","    accum =0\n","    label_wise_recall = dict()\n","    for label in [0,1,2]:\n","        true_sample = [sample for sample in test_samples if sample==label ]\n","\n","        if (get_TP(conf_matrix,label)+get_FN(conf_matrix,label))!=0:\n","            accum+= float(len(true_sample)) * (get_TP(conf_matrix,label)/(get_TP(conf_matrix,label)+get_FN(conf_matrix,label)))\n","            label_wise_recall[label] = get_TP(conf_matrix,label)/(get_TP(conf_matrix,label)+get_FN(conf_matrix,label))\n","\n","    \n","    recall =  accum/len(test_samples)\n","    return recall\n","def print_weighted_metrics(predictions,test_labels):\n","    conf_matrix = get_confusion_matrix(predictions,test_labels)\n","    precision = Precision_macro_weighted(conf_matrix,test_labels)\n","    recall = Recall_macro_weighted(conf_matrix,test_labels)\n","    f1_score = F1(precision,recall)\n","    print(\"Macro : Precision:{}, Recall: {}, F1: {}\".format(precision,recall,f1_score))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pbe2BbA4E20b","outputId":"988a961d-e0d4-4c53-eb3d-4daba9e8836d"},"source":["#difficuty macro\n","print_metrics(flat_predictions,flat_true_labels)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Macro : Precision:0.6843559944841658, Recall: 0.68233937914445, F1: 0.6833461990147928\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nNp29QbiA1B8","outputId":"2c7d3ab7-00f1-486f-b252-e4b1d3dc5378"},"source":["#difficulty\n","print_weighted_metrics(flat_predictions,flat_true_labels)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Macro : Precision:0.7015667037101829, Recall: 0.7076023391812866, F1: 0.7045715957801719\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1hNCTTaiK9NU","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6b62ec0f-e963-491a-e612-956d93504230"},"source":["#skill name\n","print_metrics(flat_skill_predictions,flat_true_skill_labels)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Macro : Precision:0.493588195841717, Recall: 0.41955091937765204, F1: 0.453568088164966\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PN7dldMoEMq0","outputId":"3d0dbdd4-a203-4ded-c409-716ce91acc22"},"source":["#skill name\n","print_weighted_metrics(flat_skill_predictions,flat_true_skill_labels)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Macro : Precision:0.840865269590251, Recall: 0.8304093567251462, F1: 0.8356046057521885\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lbxR7mbxBCBP"},"source":["def accuracy_per_class(preds_flat, labels_flat):\n","\n","    for label in np.unique(labels_flat):\n","        y_preds = preds_flat[labels_flat==label]\n","        y_true = labels_flat[labels_flat==label]\n","        print(f'Class: {get_labels(label)}')\n","        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xXoacYx9BD_v","outputId":"c7394b35-96c6-40bd-8ad2-cf1f3879129e"},"source":["accuracy_per_class(flat_predictions,flat_true_labels)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Class: easy\n","Accuracy: 115/134\n","\n","Class: hard\n","Accuracy: 43/85\n","\n","Class: medium\n","Accuracy: 84/123\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LI4KDGLx3yh6"},"source":["def accuracy_per_class(preds_flat, labels_flat):\n","\n","    for label in np.unique(labels_flat):\n","        y_preds = preds_flat[labels_flat==label]\n","        y_true = labels_flat[labels_flat==label]\n","        print(f'Class: {LE_skill.inverse_transform([label])}')\n","        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0DaR_83RQ9Fp","outputId":"a4389701-896e-494e-a4b7-88be57998108"},"source":["accuracy_per_class(flat_skill_predictions,flat_true_skill_labels)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Class: ['Analysing']\n","Accuracy: 0/1\n","\n","Class: ['Applying']\n","Accuracy: 0/1\n","\n","Class: ['Knowledge & understanding']\n","Accuracy: 88/106\n","\n","Class: ['Remembering']\n","Accuracy: 192/219\n","\n","Class: ['Understanding']\n","Accuracy: 5/15\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vu6lbpuxxbxk"},"source":["!cp -r /content/model_bert_multi_task_interactive_data_2_final.zip \"/content/drive/My Drive/research_skill_name_prediction/\" "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nv242vUH68jm"},"source":["!cp -r \"/content/drive/My Drive/research_skill_name_prediction/model_bert_multi_task_prediction.zip\" /content/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PNaswzbJ7EOw","outputId":"33fcba65-ff87-4bc7-88e8-1890bb03b6d8"},"source":["!unzip model_bert_multi_task_prediction.zip"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Archive:  model_bert_multi_task_prediction.zip\n","   creating: model_bert_multi_task_prediction/\n","  inflating: model_bert_multi_task_prediction/model_weights  \n","  inflating: model_bert_multi_task_prediction/tokenizer_config.json  \n","  inflating: model_bert_multi_task_prediction/special_tokens_map.json  \n","  inflating: model_bert_multi_task_prediction/vocab.txt  \n"]}]},{"cell_type":"code","metadata":{"id":"uSK1y-sQ8UAq"},"source":["!cp -r /content/model_bert_multi_task_interactive \"/content/drive/My Drive/research_skill_name_prediction/\" "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qykI9yk5Ocwn"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J2x-6JL3cNPz"},"source":["\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UV4-0Wdzc0-L"},"source":[""],"execution_count":null,"outputs":[]}]}