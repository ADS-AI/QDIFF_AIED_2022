{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"difficulty_ensemble_statistical_significance_data_2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"cells":[{"cell_type":"code","metadata":{"id":"sR9av2JU3kf6","colab":{"base_uri":"https://localhost:8080/"},"outputId":"978edec0-6c94-474e-ef74-d089c67ec615"},"source":["import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","import torch\n","import logging\n","logging.basicConfig(level=logging.ERROR)\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla K80\n"]}]},{"cell_type":"code","metadata":{"id":"zXHEZXXXIrDD","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b9dc65c4-b34d-408d-99fe-90b9fe823a3c"},"source":["!pip install torchtext==0.3.1"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchtext==0.3.1 in /usr/local/lib/python3.7/dist-packages (0.3.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.3.1) (2.23.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.3.1) (4.62.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.3.1) (1.19.5)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.3.1) (1.9.0+cu102)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.3.1) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.3.1) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.3.1) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.3.1) (3.0.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.3.1) (3.7.4.3)\n"]}]},{"cell_type":"code","metadata":{"id":"ATfPIQGkqp_u","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d7e7b005-461c-41bd-f8d0-6f40be5ac24f"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"BzKeqoCs3kgA","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b8c6988b-2bb5-4e78-a4b8-75ffddcaf0e4"},"source":["\n","!pip install transformers==3.2.0"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers==3.2.0 in /usr/local/lib/python3.7/dist-packages (3.2.0)\n","Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.2.0) (0.1.96)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.2.0) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.2.0) (3.0.12)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.2.0) (1.19.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.2.0) (4.62.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.2.0) (0.0.45)\n","Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.7/dist-packages (from transformers==3.2.0) (0.8.1rc2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.2.0) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.2.0) (21.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.2.0) (2.4.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.2.0) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.2.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.2.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.2.0) (2.10)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.2.0) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.2.0) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.2.0) (1.15.0)\n"]}]},{"cell_type":"code","metadata":{"id":"GsADhaO93kgD","colab":{"base_uri":"https://localhost:8080/","height":632},"outputId":"cde6889c-0d16-4afa-82e4-05fcbbb9a6c3"},"source":["import pandas as pd\n","\n","final_data = pd.read_csv(\"/content/train_qdiff_data_2_soft_labeled.csv\")\n","final_data"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Question</th>\n","      <th>Answer</th>\n","      <th>DifficultyFromAnswerer</th>\n","      <th>question_answer</th>\n","      <th>difficulty_label</th>\n","      <th>skill_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Is the dialect spoken in Jeju located in fact ...</td>\n","      <td>The dialect spoken in Jeju is in fact classifi...</td>\n","      <td>hard</td>\n","      <td>Is the dialect spoken in Jeju located in fact ...</td>\n","      <td>1</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>What cello manufacturer should I buy from if I...</td>\n","      <td>Luis &amp; Clark</td>\n","      <td>hard</td>\n","      <td>What cello manufacturer should I buy from if I...</td>\n","      <td>1</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Does it have a border with Norway?</td>\n","      <td>yes</td>\n","      <td>medium</td>\n","      <td>Does it have a border with Norway? yes</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>How many people use the bus network daily?</td>\n","      <td>More than 2.78 million people.</td>\n","      <td>easy</td>\n","      <td>How many people use the bus network daily? Mor...</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Who founded Montevideo?</td>\n","      <td>The Spanish.</td>\n","      <td>medium</td>\n","      <td>Who founded Montevideo? The Spanish.</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2763</th>\n","      <td>Did he become a professor before the revolutio...</td>\n","      <td>yes</td>\n","      <td>hard</td>\n","      <td>Did he become a professor before the revolutio...</td>\n","      <td>1</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2764</th>\n","      <td>Does Vietnamese borrow from Latin and Greek?</td>\n","      <td>No, Vietnamese does not borrow from Latin and ...</td>\n","      <td>medium</td>\n","      <td>Does Vietnamese borrow from Latin and Greek? N...</td>\n","      <td>2</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2765</th>\n","      <td>Where is San Francisco?</td>\n","      <td>San Francisco is in California.</td>\n","      <td>medium</td>\n","      <td>Where is San Francisco? San Francisco is in Ca...</td>\n","      <td>2</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2766</th>\n","      <td>What is the primary item in an otter's diet?</td>\n","      <td>fish</td>\n","      <td>medium</td>\n","      <td>What is the primary item in an otter's diet? fish</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2767</th>\n","      <td>Where are turtle eggs layed?</td>\n","      <td>Turtles lay eggs on land.</td>\n","      <td>hard</td>\n","      <td>Where are turtle eggs layed? Turtles lay eggs ...</td>\n","      <td>1</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2768 rows × 6 columns</p>\n","</div>"],"text/plain":["                                               Question  ... skill_label\n","0     Is the dialect spoken in Jeju located in fact ...  ...           3\n","1     What cello manufacturer should I buy from if I...  ...           3\n","2                    Does it have a border with Norway?  ...           2\n","3            How many people use the bus network daily?  ...           2\n","4                               Who founded Montevideo?  ...           2\n","...                                                 ...  ...         ...\n","2763  Did he become a professor before the revolutio...  ...           3\n","2764       Does Vietnamese borrow from Latin and Greek?  ...           3\n","2765                            Where is San Francisco?  ...           3\n","2766       What is the primary item in an otter's diet?  ...           2\n","2767                       Where are turtle eggs layed?  ...           3\n","\n","[2768 rows x 6 columns]"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"A0Ja4jiFhmdg","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0b6e87e0-811f-4ccf-d9e1-4032dab5e7dd"},"source":["final_data[\"question_answer\"].values"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['Is the dialect spoken in Jeju located in fact classified as a different language by all Korean linguists? The dialect spoken in Jeju is in fact classified as a different language by some Korean linguists.',\n","       'What cello manufacturer should I buy from if I want to play outside? Luis & Clark',\n","       'Does it have a border with Norway? yes', ...,\n","       'Where is San Francisco? San Francisco is in California.',\n","       \"What is the primary item in an otter's diet? fish\",\n","       'Where are turtle eggs layed? Turtles lay eggs on land.'],\n","      dtype=object)"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"iQhO6qqt6lge","colab":{"base_uri":"https://localhost:8080/"},"outputId":"de0cab6e-d896-4e14-ea77-9ca4169fe9d7"},"source":["final_data['skill_label'].value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3    1813\n","2     801\n","4     120\n","1      29\n","0       5\n","Name: skill_label, dtype: int64"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"9exlBELH5oq9"},"source":["\n","\n","!cp \"/content/drive/My Drive/research_skill_name_prediction/label_encoder_difficulty_Lstm\"  /content"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IMBf0kH7TOyd"},"source":["!cp \"/content/drive/My Drive/research_skill_name_prediction/label_encoder_skill_lstm\"  /content"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YlpZo0VvTSMb"},"source":["import joblib\n","LE_skill = joblib.load(\"label_encoder_skill_lstm\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-OBarOLBz2nO"},"source":["def get_labels(prediction):\n","    predicted_label =  LE.inverse_transform([prediction])\n","    return predicted_label[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XqWem79lbn1J","colab":{"base_uri":"https://localhost:8080/","height":281},"outputId":"a1ddcc8c-6938-4de4-990c-560e86799a8b"},"source":["final_data['difficulty_label'].value_counts().sort_values(ascending=False).plot(kind='bar')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7f56ad24fad0>"]},"metadata":{},"execution_count":12},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"RxliBQEJ9eTG"},"source":["val = pd.read_csv(\"/content/val_qdiff_data_2_soft_labeled.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"07eBaI9wA2hL","colab":{"base_uri":"https://localhost:8080/","height":753},"outputId":"42249a6c-bda3-4635-9972-741f56f0b34d"},"source":["val"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Question</th>\n","      <th>Answer</th>\n","      <th>DifficultyFromAnswerer</th>\n","      <th>difficulty_label</th>\n","      <th>question_answer</th>\n","      <th>skill_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>What are turtle eggs covered in when they incu...</td>\n","      <td>mud or sand</td>\n","      <td>hard</td>\n","      <td>1</td>\n","      <td>What are turtle eggs covered in when they incu...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>What is given for the number of native speakers?</td>\n","      <td>No figure is given for the number of native sp...</td>\n","      <td>hard</td>\n","      <td>1</td>\n","      <td>What is given for the number of native speaker...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>How many long was Lincoln's formal education?</td>\n","      <td>18 months</td>\n","      <td>easy</td>\n","      <td>0</td>\n","      <td>How many long was Lincoln's formal education? ...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>who was his mentor?</td>\n","      <td>John 'Mad Jack' Fuller</td>\n","      <td>medium</td>\n","      <td>2</td>\n","      <td>who was his mentor? John 'Mad Jack' Fuller</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Can black swans swim with only one leg?</td>\n","      <td>yes</td>\n","      <td>easy</td>\n","      <td>0</td>\n","      <td>Can black swans swim with only one leg? yes</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>303</th>\n","      <td>Is Berlin the capital city of Germany?</td>\n","      <td>Berlin is the capital city of Germany.</td>\n","      <td>easy</td>\n","      <td>0</td>\n","      <td>Is Berlin the capital city of Germany? Berlin ...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>304</th>\n","      <td>Who did James Monroe live with in New York City?</td>\n","      <td>His daughter Maria Hester Monroe Gouverneur</td>\n","      <td>medium</td>\n","      <td>2</td>\n","      <td>Who did James Monroe live with in New York Cit...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>305</th>\n","      <td>What is one of the challenges of re-establishi...</td>\n","      <td>roadkill deaths</td>\n","      <td>hard</td>\n","      <td>1</td>\n","      <td>What is one of the challenges of re-establishi...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>306</th>\n","      <td>Is Santiago the national capital of a country?</td>\n","      <td>Yes</td>\n","      <td>easy</td>\n","      <td>0</td>\n","      <td>Is Santiago the national capital of a country?...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>307</th>\n","      <td>Why do some people believe that left-handed pe...</td>\n","      <td>to standardise the instrument</td>\n","      <td>hard</td>\n","      <td>1</td>\n","      <td>Why do some people believe that left-handed pe...</td>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>308 rows × 6 columns</p>\n","</div>"],"text/plain":["                                              Question  ... skill_label\n","0    What are turtle eggs covered in when they incu...  ...           2\n","1     What is given for the number of native speakers?  ...           3\n","2        How many long was Lincoln's formal education?  ...           3\n","3                                  who was his mentor?  ...           3\n","4              Can black swans swim with only one leg?  ...           3\n","..                                                 ...  ...         ...\n","303             Is Berlin the capital city of Germany?  ...           3\n","304   Who did James Monroe live with in New York City?  ...           3\n","305  What is one of the challenges of re-establishi...  ...           2\n","306     Is Santiago the national capital of a country?  ...           3\n","307  Why do some people believe that left-handed pe...  ...           4\n","\n","[308 rows x 6 columns]"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"g8tVsjiWj-cF","colab":{"base_uri":"https://localhost:8080/","height":735},"outputId":"19c5f6a4-ca67-4b93-ef86-84b1a876b8ad"},"source":["test = pd.read_csv(\"/content/test_qdiff_data_2_soft_labeled.csv\")\n","test"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Question</th>\n","      <th>Answer</th>\n","      <th>DifficultyFromAnswerer</th>\n","      <th>difficulty_label</th>\n","      <th>question_answer</th>\n","      <th>skill_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>How are western-style xylophones characterised?</td>\n","      <td>by a bright, sharp tone and high register</td>\n","      <td>medium</td>\n","      <td>2</td>\n","      <td>How are western-style xylophones characterised...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Is Nairobi the capital of Kenya?</td>\n","      <td>Yes</td>\n","      <td>easy</td>\n","      <td>0</td>\n","      <td>Is Nairobi the capital of Kenya? Yes</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>How many sister cities does the City of Melbou...</td>\n","      <td>six</td>\n","      <td>medium</td>\n","      <td>2</td>\n","      <td>How many sister cities does the City of Melbou...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Is the electric eel a true eel?</td>\n","      <td>No</td>\n","      <td>easy</td>\n","      <td>0</td>\n","      <td>Is the electric eel a true eel? No</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Does Swedish use the perfect participle to for...</td>\n","      <td>No.</td>\n","      <td>easy</td>\n","      <td>0</td>\n","      <td>Does Swedish use the perfect participle to for...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>337</th>\n","      <td>Where was there a vast swarm of butterflies?</td>\n","      <td>In Kyoto there was a vast swarm of butterflies.</td>\n","      <td>medium</td>\n","      <td>2</td>\n","      <td>Where was there a vast swarm of butterflies? I...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>338</th>\n","      <td>What is the most common romanization standard ...</td>\n","      <td>Hanyu Pinyin</td>\n","      <td>medium</td>\n","      <td>2</td>\n","      <td>What is the most common romanization standard ...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>339</th>\n","      <td>Is Jakarta the 12th largest city in the world?</td>\n","      <td>yes</td>\n","      <td>medium</td>\n","      <td>2</td>\n","      <td>Is Jakarta the 12th largest city in the world?...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>340</th>\n","      <td>What sort of turtles are ectothermic?</td>\n","      <td>all of them</td>\n","      <td>medium</td>\n","      <td>2</td>\n","      <td>What sort of turtles are ectothermic? all of them</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>341</th>\n","      <td>Was Gellu Naum the leader of the surrealist mo...</td>\n","      <td>Yes</td>\n","      <td>easy</td>\n","      <td>0</td>\n","      <td>Was Gellu Naum the leader of the surrealist mo...</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>342 rows × 6 columns</p>\n","</div>"],"text/plain":["                                              Question  ... skill_label\n","0      How are western-style xylophones characterised?  ...           2\n","1                     Is Nairobi the capital of Kenya?  ...           3\n","2    How many sister cities does the City of Melbou...  ...           3\n","3                      Is the electric eel a true eel?  ...           3\n","4    Does Swedish use the perfect participle to for...  ...           3\n","..                                                 ...  ...         ...\n","337       Where was there a vast swarm of butterflies?  ...           3\n","338  What is the most common romanization standard ...  ...           3\n","339     Is Jakarta the 12th largest city in the world?  ...           2\n","340              What sort of turtles are ectothermic?  ...           2\n","341  Was Gellu Naum the leader of the surrealist mo...  ...           3\n","\n","[342 rows x 6 columns]"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"FIrS5sxE3kgk","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c789ba8e-a212-4b39-8e55-7cff84d244d9"},"source":["from transformers import BertTokenizer\n","\n","# Load the BERT tokenizer.\n","print('Loading BERT tokenizer...')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading BERT tokenizer...\n"]}]},{"cell_type":"code","metadata":{"id":"wp64MkNB3kg1"},"source":["\n","def get_labels(prediction):\n","    predicted_label =  LE.inverse_transform([prediction])\n","    return predicted_label[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uPgTmJPS3kg4","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"7ca6251b-9c7a-4002-bf9e-336ed7b81dae"},"source":["import joblib\n","from sklearn.preprocessing import LabelEncoder\n","\n","LE = LabelEncoder()\n","LE = joblib.load('label_encoder_difficulty_Lstm')\n","\n","get_labels(0)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Difficult'"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"I_UpqLMG3kg9","colab":{"base_uri":"https://localhost:8080/","height":632},"outputId":"e790cf4c-5c83-466f-c8f3-30e8c16945c0"},"source":["final_data"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Question</th>\n","      <th>Answer</th>\n","      <th>DifficultyFromAnswerer</th>\n","      <th>question_answer</th>\n","      <th>difficulty_label</th>\n","      <th>skill_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Is the dialect spoken in Jeju located in fact ...</td>\n","      <td>The dialect spoken in Jeju is in fact classifi...</td>\n","      <td>hard</td>\n","      <td>Is the dialect spoken in Jeju located in fact ...</td>\n","      <td>1</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>What cello manufacturer should I buy from if I...</td>\n","      <td>Luis &amp; Clark</td>\n","      <td>hard</td>\n","      <td>What cello manufacturer should I buy from if I...</td>\n","      <td>1</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Does it have a border with Norway?</td>\n","      <td>yes</td>\n","      <td>medium</td>\n","      <td>Does it have a border with Norway? yes</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>How many people use the bus network daily?</td>\n","      <td>More than 2.78 million people.</td>\n","      <td>easy</td>\n","      <td>How many people use the bus network daily? Mor...</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Who founded Montevideo?</td>\n","      <td>The Spanish.</td>\n","      <td>medium</td>\n","      <td>Who founded Montevideo? The Spanish.</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2763</th>\n","      <td>Did he become a professor before the revolutio...</td>\n","      <td>yes</td>\n","      <td>hard</td>\n","      <td>Did he become a professor before the revolutio...</td>\n","      <td>1</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2764</th>\n","      <td>Does Vietnamese borrow from Latin and Greek?</td>\n","      <td>No, Vietnamese does not borrow from Latin and ...</td>\n","      <td>medium</td>\n","      <td>Does Vietnamese borrow from Latin and Greek? N...</td>\n","      <td>2</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2765</th>\n","      <td>Where is San Francisco?</td>\n","      <td>San Francisco is in California.</td>\n","      <td>medium</td>\n","      <td>Where is San Francisco? San Francisco is in Ca...</td>\n","      <td>2</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2766</th>\n","      <td>What is the primary item in an otter's diet?</td>\n","      <td>fish</td>\n","      <td>medium</td>\n","      <td>What is the primary item in an otter's diet? fish</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2767</th>\n","      <td>Where are turtle eggs layed?</td>\n","      <td>Turtles lay eggs on land.</td>\n","      <td>hard</td>\n","      <td>Where are turtle eggs layed? Turtles lay eggs ...</td>\n","      <td>1</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2768 rows × 6 columns</p>\n","</div>"],"text/plain":["                                               Question  ... skill_label\n","0     Is the dialect spoken in Jeju located in fact ...  ...           3\n","1     What cello manufacturer should I buy from if I...  ...           3\n","2                    Does it have a border with Norway?  ...           2\n","3            How many people use the bus network daily?  ...           2\n","4                               Who founded Montevideo?  ...           2\n","...                                                 ...  ...         ...\n","2763  Did he become a professor before the revolutio...  ...           3\n","2764       Does Vietnamese borrow from Latin and Greek?  ...           3\n","2765                            Where is San Francisco?  ...           3\n","2766       What is the primary item in an otter's diet?  ...           2\n","2767                       Where are turtle eggs layed?  ...           3\n","\n","[2768 rows x 6 columns]"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"MHdVe13Fr3vt"},"source":["new_data = final_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AkyM7gqv3khI"},"source":["question_answer = new_data[\"question_answer\"].values\n","categories = new_data[\"difficulty_label\"].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ndpw0p1SBUoZ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d4188098-336c-43de-a3d0-da319414448b"},"source":["question_answer"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['Is the dialect spoken in Jeju located in fact classified as a different language by all Korean linguists? The dialect spoken in Jeju is in fact classified as a different language by some Korean linguists.',\n","       'What cello manufacturer should I buy from if I want to play outside? Luis & Clark',\n","       'Does it have a border with Norway? yes', ...,\n","       'Where is San Francisco? San Francisco is in California.',\n","       \"What is the primary item in an otter's diet? fish\",\n","       'Where are turtle eggs layed? Turtles lay eggs on land.'],\n","      dtype=object)"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"tFkS_H_83khL","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"eb257a72-1922-49f5-cd39-1f896b3ab144"},"source":["question_answer[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Is the dialect spoken in Jeju located in fact classified as a different language by all Korean linguists? The dialect spoken in Jeju is in fact classified as a different language by some Korean linguists.'"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"ian7gSDE3khR","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9f560257-e7b6-4c5a-df3b-6e94a850e4a0"},"source":["len(categories)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2768"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"Y_ZeuHc63khU","colab":{"base_uri":"https://localhost:8080/"},"outputId":"65794e2c-ccf3-4dc7-94e0-ef55118650d3"},"source":["input_ids = []\n","attention_masks = []\n","\n","for sent in question_answer:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens.\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 128,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        truncation=True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","# Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', question_answer[0])\n","print('Token IDs:', input_ids[0])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Original:  Is the dialect spoken in Jeju located in fact classified as a different language by all Korean linguists? The dialect spoken in Jeju is in fact classified as a different language by some Korean linguists.\n","Token IDs: tensor([  101,  2003,  1996,  9329,  5287,  1999, 15333,  9103,  2284,  1999,\n","         2755,  6219,  2004,  1037,  2367,  2653,  2011,  2035,  4759, 22978,\n","         2015,  1029,  1996,  9329,  5287,  1999, 15333,  9103,  2003,  1999,\n","         2755,  6219,  2004,  1037,  2367,  2653,  2011,  2070,  4759, 22978,\n","         2015,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0])\n"]}]},{"cell_type":"code","metadata":{"id":"iVGvVZb13kha","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bfcea38b-5fbd-47e0-8e47-51c364c08f71"},"source":["print('Original: ', len(question_answer[1]))\n","print('Token IDs:', len(input_ids[1]))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original:  81\n","Token IDs: 128\n"]}]},{"cell_type":"code","metadata":{"id":"SyHFXhD2R32G"},"source":["val = val.dropna(subset=[\"question_answer\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5nmRiaBbA9OH"},"source":["val_text = val[\"question_answer\"].values\n","val_labels = val[\"difficulty_label\"].values\n","test_text = test[\"question_answer\"].values\n","test_labels = test[\"difficulty_label\"].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E-s_H1WdyvCw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f775b6e7-28d8-41bd-866c-b566724bc863"},"source":["test_labels"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([2, 0, 2, 0, 0, 0, 1, 2, 1, 2, 2, 1, 0, 0, 1, 1, 1, 0, 2, 2, 0, 2,\n","       1, 1, 1, 2, 0, 0, 0, 0, 2, 2, 2, 0, 0, 2, 1, 0, 1, 2, 1, 2, 2, 0,\n","       2, 0, 2, 2, 0, 2, 2, 1, 0, 2, 2, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 2,\n","       0, 0, 2, 2, 2, 2, 0, 0, 2, 2, 0, 2, 0, 2, 2, 2, 2, 0, 0, 0, 0, 2,\n","       0, 1, 0, 0, 0, 2, 2, 0, 1, 0, 0, 0, 0, 1, 1, 2, 2, 0, 2, 2, 0, 2,\n","       0, 0, 0, 1, 0, 2, 2, 1, 1, 1, 1, 2, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n","       0, 1, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 2, 0, 0, 1, 0, 2, 2, 2,\n","       2, 2, 0, 1, 2, 2, 0, 2, 2, 2, 1, 1, 2, 2, 1, 0, 0, 0, 2, 1, 1, 0,\n","       0, 0, 0, 0, 1, 2, 0, 2, 2, 2, 1, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0,\n","       2, 0, 0, 2, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0, 2, 2, 0, 0, 2, 2, 2, 1,\n","       2, 2, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 0, 1, 2, 2, 1, 0, 0, 0, 1,\n","       2, 0, 0, 2, 0, 2, 0, 0, 2, 1, 1, 2, 2, 0, 2, 0, 2, 2, 1, 2, 2, 2,\n","       0, 1, 2, 0, 1, 1, 2, 2, 0, 2, 1, 0, 2, 1, 0, 1, 1, 2, 0, 1, 1, 1,\n","       1, 2, 1, 0, 2, 0, 0, 2, 0, 1, 0, 1, 1, 0, 2, 0, 1, 2, 0, 0, 1, 0,\n","       1, 2, 1, 1, 1, 2, 2, 1, 2, 0, 2, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 2,\n","       2, 2, 1, 0, 1, 1, 2, 2, 2, 2, 2, 0])"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"YF-mKCC1CUjD","colab":{"base_uri":"https://localhost:8080/"},"outputId":"716c01d2-669b-4a23-c6d3-76fa4b38c221"},"source":["val_labels"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 1, 0, 2, 0, 0, 1, 2, 2, 0, 2, 0, 1, 0, 0, 0, 0, 2, 2, 2, 0, 0,\n","       2, 1, 1, 0, 1, 2, 2, 1, 1, 0, 1, 2, 1, 1, 0, 0, 0, 1, 1, 2, 2, 0,\n","       1, 0, 1, 2, 0, 0, 0, 2, 0, 2, 0, 2, 0, 1, 0, 1, 0, 0, 2, 2, 1, 1,\n","       2, 1, 0, 1, 2, 0, 2, 2, 2, 1, 2, 2, 1, 2, 1, 1, 0, 2, 0, 1, 0, 0,\n","       1, 2, 2, 0, 0, 0, 2, 2, 1, 1, 0, 0, 1, 0, 2, 0, 1, 0, 2, 0, 0, 2,\n","       2, 1, 2, 2, 0, 1, 0, 0, 2, 0, 0, 0, 0, 2, 0, 2, 0, 2, 0, 2, 0, 0,\n","       0, 1, 0, 2, 0, 1, 2, 1, 0, 1, 2, 2, 2, 2, 1, 2, 1, 0, 1, 2, 0, 0,\n","       1, 0, 2, 0, 2, 2, 0, 0, 1, 1, 2, 1, 1, 0, 2, 2, 0, 1, 1, 2, 0, 0,\n","       0, 1, 2, 2, 2, 1, 0, 1, 2, 2, 0, 0, 1, 0, 2, 2, 1, 0, 0, 0, 1, 2,\n","       1, 2, 1, 1, 2, 0, 1, 2, 0, 1, 1, 2, 1, 2, 0, 0, 1, 0, 0, 2, 1, 0,\n","       0, 0, 0, 0, 0, 2, 0, 1, 0, 2, 2, 2, 1, 2, 1, 1, 0, 1, 0, 2, 1, 1,\n","       0, 2, 0, 0, 0, 1, 2, 0, 1, 0, 0, 2, 1, 2, 0, 1, 2, 0, 0, 0, 0, 0,\n","       0, 2, 0, 0, 2, 1, 1, 0, 2, 0, 0, 2, 0, 2, 2, 2, 2, 2, 0, 2, 0, 0,\n","       0, 0, 2, 2, 0, 0, 1, 1, 2, 1, 1, 2, 1, 2, 0, 0, 0, 2, 1, 0, 1])"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"sOQuDahhAzOO","colab":{"base_uri":"https://localhost:8080/"},"outputId":"84a315f1-a5ce-4f30-c7b9-c41bbbffd981"},"source":["val_input_ids = []\n","val_attention_masks = []\n","\n","for sent in val_text:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens.\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 128,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        truncation=True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    val_input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    val_attention_masks.append(encoded_dict['attention_mask'])\n","# Convert the lists into tensors.\n","val_input_ids = torch.cat(val_input_ids, dim=0)\n","val_attention_masks = torch.cat(val_attention_masks, dim=0)\n","\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', val_text[0])\n","print('Token IDs:', val_attention_masks[0])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original:  What are turtle eggs covered in when they incubate? mud or sand\n","Token IDs: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0])\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}]},{"cell_type":"code","metadata":{"id":"Siskea7qDLUG","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5d89faeb-4e2d-4ea3-c898-6fb096e2dc18"},"source":["print('Original: ', val_text[1])\n","print('Token IDs:', val_input_ids[1])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original:  What is given for the number of native speakers? No figure is given for the number of native speakers.\n","Token IDs: tensor([ 101, 2054, 2003, 2445, 2005, 1996, 2193, 1997, 3128, 7492, 1029, 2053,\n","        3275, 2003, 2445, 2005, 1996, 2193, 1997, 3128, 7492, 1012,  102,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0])\n"]}]},{"cell_type":"code","metadata":{"id":"irMTimjf3khd"},"source":["labels = torch.tensor(categories)\n","val_labels = torch.tensor(val_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PdOgWP_LKTHi","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3cf90caf-0a41-4ff9-d6ff-3724106e99c5"},"source":["val_labels"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1, 1, 0, 2, 0, 0, 1, 2, 2, 0, 2, 0, 1, 0, 0, 0, 0, 2, 2, 2, 0, 0, 2, 1,\n","        1, 0, 1, 2, 2, 1, 1, 0, 1, 2, 1, 1, 0, 0, 0, 1, 1, 2, 2, 0, 1, 0, 1, 2,\n","        0, 0, 0, 2, 0, 2, 0, 2, 0, 1, 0, 1, 0, 0, 2, 2, 1, 1, 2, 1, 0, 1, 2, 0,\n","        2, 2, 2, 1, 2, 2, 1, 2, 1, 1, 0, 2, 0, 1, 0, 0, 1, 2, 2, 0, 0, 0, 2, 2,\n","        1, 1, 0, 0, 1, 0, 2, 0, 1, 0, 2, 0, 0, 2, 2, 1, 2, 2, 0, 1, 0, 0, 2, 0,\n","        0, 0, 0, 2, 0, 2, 0, 2, 0, 2, 0, 0, 0, 1, 0, 2, 0, 1, 2, 1, 0, 1, 2, 2,\n","        2, 2, 1, 2, 1, 0, 1, 2, 0, 0, 1, 0, 2, 0, 2, 2, 0, 0, 1, 1, 2, 1, 1, 0,\n","        2, 2, 0, 1, 1, 2, 0, 0, 0, 1, 2, 2, 2, 1, 0, 1, 2, 2, 0, 0, 1, 0, 2, 2,\n","        1, 0, 0, 0, 1, 2, 1, 2, 1, 1, 2, 0, 1, 2, 0, 1, 1, 2, 1, 2, 0, 0, 1, 0,\n","        0, 2, 1, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 2, 2, 2, 1, 2, 1, 1, 0, 1, 0, 2,\n","        1, 1, 0, 2, 0, 0, 0, 1, 2, 0, 1, 0, 0, 2, 1, 2, 0, 1, 2, 0, 0, 0, 0, 0,\n","        0, 2, 0, 0, 2, 1, 1, 0, 2, 0, 0, 2, 0, 2, 2, 2, 2, 2, 0, 2, 0, 0, 0, 0,\n","        2, 2, 0, 0, 1, 1, 2, 1, 1, 2, 1, 2, 0, 0, 0, 2, 1, 0, 1])"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"OJJ0I8Ud3khf","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"c2ef3ef0-0000-48dd-8582-b312b25c67b6"},"source":["get_labels(1)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Easy'"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"Z1ZAbQRfiG63","colab":{"base_uri":"https://localhost:8080/"},"outputId":"30d3fb2d-569b-4f7b-a8a7-883ff5c1a0ef"},"source":["len(set(final_data[\"question_answer\"].values).intersection(val[\"question_answer\"].values))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["46"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","metadata":{"id":"BNDW74Ny3khj","colab":{"base_uri":"https://localhost:8080/"},"outputId":"363d4bf8-1fb0-457d-ea4a-0fdfe91b1a04"},"source":["num_classes = len(list(set(categories)))\n","list(set(categories))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0, 1, 2]"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","metadata":{"id":"ZmaLk5Ab3khl"},"source":["from torch.utils.data import TensorDataset, random_split\n","# train_poincare_tensor = torch.tensor(poincare_embeddings_final,dtype=torch.float)\n","# difficulty_tensor = torch.tensor(difficulty_level_vectors,dtype=torch.float)\n","# Combine the training inputs into a TensorDataset.\n","dataset = TensorDataset(input_ids, attention_masks, labels)\n","val_dataset = TensorDataset(val_input_ids, val_attention_masks,val_labels) \n","# Create a 90-10train-validation split.\n","\n","# Calculate the number of samples to include in each set.\n","# train_size = int(0.90 * len(dataset))\n","# val_size = len(dataset) - train_size\n","\n","# # Divide the dataset by randomly selecting samples.\n","# train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","# print('{:>5,} training samples'.format(train_size))\n","# # print('{:>5,} validation samples'.format(val_size))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D_lTinod3kho"},"source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","batch_size = 34\n","train_dataloader = DataLoader(\n","            dataset,  # The training samples.\n","            sampler = RandomSampler(dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","\n","validation_dataloader = DataLoader(\n","            val_dataset, # The validation samples.\n","            sampler = SequentialSampler(val_dataset), \n","            batch_size = batch_size \n","        )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_2tmAMlw3khr"},"source":["from transformers import BertModel, AdamW, BertConfig\n","\n","# # Loads BertForSequenceClassification, the pretrained BERT model with a single \n","# model = BertModel.from_pretrained(\n","#     \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n","# )\n","\n","# # Tell pytorch to run this model on the GPU.\n","# model.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DkDmTZhVChN6","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1a5caf24-0c83-41fb-a32b-8bcccf5a024a"},"source":["\n","\n","\n","\n","set(test[\"question_answer\"].values).intersection(set(final_data[\"question_answer\"].values))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'Approximately how many species of Testudines are alive today? 300',\n"," 'Are all dialects of Korean similar to each other? Yes',\n"," 'Are pocket trumpets compact B trumpets? yes',\n"," 'Are wolves built for stamina? Yes',\n"," 'Around how many recognized octopus species are there? There are around 300 recognized octopus species.',\n"," 'Copenhagen is the capital of what country? Denmark',\n"," 'Did Lincoln ever represent Alton & Sangamon Railroad? Yes',\n"," \"Did Monroe' wedding happen at the Trinity Church in New York? Yes\",\n"," 'Do linguists often view Chinese as a language family? Yes, linguists often view Chinese as a language family.',\n"," 'Does Modern Standard Arabic continue to evolve like other languages? yes',\n"," 'Does Theodore Roosevelt have a brother? Yes',\n"," 'Does every drumhead make the same sound? no',\n"," 'Does the octopus have a hard beak? Yes, the octopus has a hard beak.',\n"," 'Have cymbals been used historically to suggest bacchanal? Yes',\n"," 'How many children did Avogadro have? six',\n"," 'How many species of otter are there? 13',\n"," 'How old is the oldest known representation of a guitar-like intrument being played? 3,300 years old',\n"," 'How old was Celsius when he died? 42',\n"," \"Is English Ghana's official language? yes\",\n"," 'Is Liechtenstein heavily urbanized? No',\n"," 'Is Liechtenstein the smallest German-speaking country in the world? Yes',\n"," 'Is it a disadvantage for something to be unsafe to handle? yes',\n"," 'Is polar bear a mammal? Yes',\n"," 'Is the SI unit for radioactivity named after him? Yes',\n"," 'Was Abraham Lincoln the first President of the United States? No',\n"," 'Was Grover Cleveland the twenty-seventh president of the United States? No.',\n"," \"Was Henri Becquerel first in his family to occupy the physics chair at the Museum National d'Histoire Naturelle? No\",\n"," \"Was Isaac Newton educated at The King's Schol, Grantham? yes\",\n"," 'Was Thedore Roosevelt  a member of the Republican Party? Yes',\n"," 'Was the Italian 10.000 lira banknote created before the euro? yes',\n"," 'Were trumpet players heavily guarded? yes',\n"," \"What are the elephant's ears important for? temperature regulation\",\n"," \"What company administers Leichtenstein's railways? Austrian Federal Railways\",\n"," 'What do river otters eat? a variety of fish and shellfish, as well as small land mammals and birds',\n"," 'What is the life expectancy for men in Finland? 75 years',\n"," 'What is the most common romanization standard for Standard Mandarin today? Hanyu Pinyin',\n"," 'What is the smallest suborder of turtles? Pleurodira',\n"," \"What was Grant's political affiliation? Republican\",\n"," 'What year did Coolidge open his own law office? 1898',\n"," 'When did Isaac Newton discover the generalized binomial theorem? In 1665.',\n"," 'When did Roosevelt die? On January 6, 1919, Roosevelt died in his sleep.',\n"," 'When was the Six Day War? 1967',\n"," 'Where is Finland located? Northern Europe',\n"," 'Where was Isaac Newton born? Woolsthorpe Manor in Woolsthorpe-by-Colsterworth',\n"," 'Where was James Monroe born? Westmoreland County, Virginia',\n"," 'Where was the League of Nations created? Paris',\n"," 'Which type of beetle is a pest of potato plants? Colorado potato beetle',\n"," 'Who appointed Harlan Fiske Stone to the Supreme Court? Coolidge',\n"," \"Who is the mayor of Ottawa? Larry O'Brien\",\n"," 'Who was President when Wilson finished Congressional Government? Grover Cleveland',\n"," 'With what party did Adams run for presidency? The Federalist Party'}"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","metadata":{"id":"JJvEax7_uTEh"},"source":["!unzip \"/content/drive/MyDrive/research_skill_name_prediction/model_bert_skill_prediction_data_2.zip\"\n","!cp -r \"/content/drive/MyDrive/research_skill_name_prediction/model_bert_difficulty_prediction_data_2\" /content"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8CFbt2CWqSLz"},"source":["!cp -r \"/content/drive/MyDrive/research_skill_name_prediction/model_bert_multi_task_prediction_data_2\" /content"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MJ9iUjJiHI7k"},"source":["!cp -r \"/content/drive/MyDrive/research_skill_name_prediction/model_bert_multi_task_interactive_data_2_final\" /content"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"drz7amvtT4ju"},"source":["!cp -r \"/content/drive/MyDrive/research_skill_name_prediction/model_bert_multi_task_interactive_pre_trained_skill_bert_data_2\" /content"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lJjikk9efqJP","colab":{"base_uri":"https://localhost:8080/"},"outputId":"71437abf-d00c-4051-9ac0-0869cf145a37"},"source":["!pip install torchtext\n","import torchtext"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (0.3.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext) (2.23.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.9.0+cu102)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.19.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext) (4.62.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2.10)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext) (3.7.4.3)\n"]}]},{"cell_type":"code","metadata":{"id":"fsD6CUxrvnL0","colab":{"base_uri":"https://localhost:8080/"},"outputId":"06c43575-0e1d-49fe-a3fd-f13cf6d00394"},"source":["!cp -r  \"/content/drive/MyDrive/research_skill_name_prediction/model_bert_skill_prediction_cascade_data_2\" /content\n","!cp -r \"/content/drive/MyDrive/research_skill_name_prediction/model_save_BLOOM_difficulty_Lstm_data_2\" /content\n","!cp -r \"/content/drive/MyDrive/research_skill_name_prediction/model_save_gru_difficulty_name.zip\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cp: missing destination file operand after '/content/drive/MyDrive/research_skill_name_prediction/model_save_gru_difficulty_name.zip'\n","Try 'cp --help' for more information.\n"]}]},{"cell_type":"code","metadata":{"id":"YBoO8oWNFf2z"},"source":["!cp -r \"/content/drive/MyDrive/research_skill_name_prediction/model_bert_difficulty_cascade_data_2\" /content\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-7yA8H4kHJvv"},"source":["\n","text = torchtext.data.Field(lower=True, batch_first=True, tokenize='spacy', include_lengths=True)\n","target = torchtext.data.Field(sequential=False, use_vocab=False, is_target=True)\n","# use field objects to read training, validation and test sets\n","train = torchtext.data.TabularDataset(path='/content/train_qdiff_data_2_soft_labeled.csv', format='csv',\n","                                      fields={'question_answer': ('text',text),\n","                                              'difficulty_label': ('target',target)})\n","val = torchtext.data.TabularDataset(path='/content/val_qdiff_data_2_soft_labeled.csv', format='csv',\n","                                    fields={'question_answer': ('text',text),\n","                                              'difficulty_label': ('target',target)})\n","test_text = torchtext.data.TabularDataset(path='/content/test_qdiff_data_2_soft_labeled.csv', format='csv',\n","                                     fields={'difficulty_label': ('label', target),\n","                                             'question_answer': ('text',text)})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NHKLEuKxHL5A"},"source":["batch_size = 34\n","train_iter = torchtext.data.Iterator(dataset=train,\n","                                           batch_size=batch_size,\n","                                           sort_key=lambda x: x.text.__len__(),\n","                                           shuffle=True,\n","                                           sort_within_batch=True) \n","val_iter = torchtext.data.Iterator(dataset=val,\n","                                         batch_size=batch_size,\n","                                         sort_key=lambda x: x.text.__len__(),\n","                                         train=False,\n","                                         sort_within_batch=True)\n","test_iter = torchtext.data.Iterator(dataset=test_text,\n","                                          batch_size=batch_size,\n","                                          sort=False,\n","                                          sort_within_batch=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"miKcDNXuHNop"},"source":["import torchtext.vocab as vocab\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"spmW5b5ufklC"},"source":["text.build_vocab(train, val, test_text,vectors=\"glove.6B.100d\", min_freq=3)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-sLtbTYSfklC","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d6a2ac2e-25bc-40e7-9192-ef79772a456c"},"source":["print(text.vocab.vectors.shape)\n","print(f\"Unique tokens in text vocabulary: {len(text.vocab)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2309, 100])\n","Unique tokens in text vocabulary: 2309\n"]}]},{"cell_type":"code","metadata":{"id":"5a0Hs7nZi8TW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"92c646ab-798d-472a-da11-6b6087fb37ea"},"source":["num_classes"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{},"execution_count":54}]},{"cell_type":"code","metadata":{"id":"ZA_r6iiReO4N"},"source":["# attention layer code inspired from: https://discuss.pytorch.org/t/self-attention-on-words-and-masking/5671/4\n","from torch import nn\n","class Attention(nn.Module):\n","    def __init__(self, hidden_size, batch_first=False):\n","        super(Attention, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","        self.batch_first = batch_first\n","\n","        self.att_weights = nn.Parameter(torch.Tensor(1, hidden_size), requires_grad=True)\n","\n","        stdv = 1.0 / np.sqrt(self.hidden_size)\n","        for weight in self.att_weights:\n","            nn.init.uniform_(weight, -stdv, stdv)\n","\n","    def get_mask(self):\n","        pass\n","\n","    def forward(self, inputs, lengths):\n","        if self.batch_first:\n","            batch_size, max_len = inputs.size()[:2]\n","        else:\n","            max_len, batch_size = inputs.size()[:2]\n","            \n","        # apply attention layer\n","        weights = torch.bmm(inputs,\n","                            self.att_weights\n","                            .permute(1, 0)  \n","                            .unsqueeze(0)  \n","                            .repeat(batch_size, 1, 1) \n","                            )\n","    \n","        attentions = torch.softmax(F.relu(weights.squeeze()), dim=-1)\n","\n","        mask = torch.ones(attentions.size(), requires_grad=True).cuda()\n","        for i, l in enumerate(lengths):  # skip the first sentence\n","            if l < max_len:\n","                mask[i, l:] = 0\n","\n","        masked = attentions * mask\n","        _sums = masked.sum(-1).unsqueeze(-1)  # sums per row\n","        \n","        attentions = masked.div(_sums)\n","\n","        weighted = torch.mul(inputs, attentions.unsqueeze(-1).expand_as(inputs))\n","\n","        representations = weighted.sum(1).squeeze()\n","\n","        return representations, attentions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aHmQK0OP9mv-"},"source":["from torch import nn\n","class MultiClassClassifierGRU(nn.Module):\n","  def __init__(self, pretrained_lm, hidden_dim=228, lstm_layer=2, dropout=0.2):\n","        super(MultiClassClassifierGRU, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.embedding = nn.Embedding.from_pretrained(pretrained_lm)\n","        self.embedding.weight.requires_grad = False\n","        self.gru1 = nn.GRU(input_size=self.embedding.embedding_dim,\n","                            hidden_size=hidden_dim,\n","                            num_layers=1, \n","                            bidirectional=True)\n","        self.atten1 = Attention(hidden_dim*2, batch_first=True) # 2 is bidrectional\n","        self.gru2 = nn.GRU(input_size=hidden_dim*2,\n","                            hidden_size=hidden_dim,\n","                            num_layers=1, \n","                            bidirectional=True)\n","        self.atten2 = Attention(hidden_dim*2, batch_first=True)\n","        self.fc1 = nn.Sequential(nn.Linear(hidden_dim*7*2, hidden_dim*7*2),\n","                                 nn.BatchNorm1d(hidden_dim*7*2),\n","                                 nn.ReLU()) \n","        self.fc2 = nn.Linear(hidden_dim*7*2, num_classes)\n","\n","    \n","  def forward(self, x, x_len):\n","        x = self.embedding(x)\n","        x = self.dropout(x)\n","        \n","        x = nn.utils.rnn.pack_padded_sequence(x, x_len, batch_first=True, enforce_sorted=False)\n","        out1, h_n = self.gru1(x)\n","\n","        x, lengths = nn.utils.rnn.pad_packed_sequence(out1, batch_first=True)\n","        lengths_tensor = torch.autograd.Variable(torch.FloatTensor(lengths.float())).view(-1,1)\n","\n","        average_pooling = torch.sum(x,dim=1)/lengths_tensor.to(device)\n","        max_pooling =  torch.nn.functional.adaptive_max_pool1d(x.permute(0,2,1), (1,)).view(x.size()[0],-1)\n","\n","        # print(pooled_output_1.size())\n","\n","        x, _ = self.atten1(x, lengths) # skip connect\n","        if len(x.shape)==1:\n","          x = x.reshape(1,-1)\n","        pooled_output_1 = torch.cat([x,h_n[-1],average_pooling,max_pooling],dim=1)\n","\n","\n","        # print(pooled_output_1.size())\n","\n","        out2, h_n = self.gru2(out1)\n","        y, lengths = nn.utils.rnn.pad_packed_sequence(out2, batch_first=True)\n","\n","        lengths_tensor = torch.autograd.Variable(torch.FloatTensor(lengths.float())).view(-1,1)\n","\n","        average_pooling = torch.sum(y,dim=1)/lengths_tensor.to(device)\n","        max_pooling =  torch.nn.functional.adaptive_max_pool1d(y.permute(0,2,1), (1,)).view(y.size()[0],-1)\n","        y, _ = self.atten2(y, lengths)\n","        if len(y.shape)==1:\n","          y = y.reshape(1,-1)\n","        pooled_output_2 = torch.cat([y,h_n[-1],average_pooling,max_pooling],dim=1)\n","\n","        z = torch.cat([pooled_output_1, pooled_output_2], dim=1)\n","\n","        z = self.fc1(self.dropout(z))\n","        z = self.fc2(self.dropout(z))\n","        return z\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VeuaTKv7b8SY"},"source":["import numpy as np\n","model_gru = MultiClassClassifierGRU(text.vocab.vectors, hidden_dim=400, lstm_layer=2, dropout=0.3).cuda()\n","model_gru.load_state_dict(torch.load(\"model_save_gru_difficulty_name/model_weights\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P0Qbs-C7kD0K"},"source":["from torch import nn\n","class MultiClassClassifierLstm(nn.Module):\n","  def __init__(self, pretrained_lm, hidden_dim=128, lstm_layer=2, dropout=0.2):\n","        super(MultiClassClassifierLstm, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.embedding = nn.Embedding.from_pretrained(pretrained_lm)\n","        self.embedding.weight.requires_grad = False\n","        self.lstm1 = nn.LSTM(input_size=self.embedding.embedding_dim,\n","                            hidden_size=hidden_dim,\n","                            num_layers=1, \n","                            bidirectional=True)\n","        self.atten1 = Attention(hidden_dim*2, batch_first=True) # 2 is bidrectional\n","        self.lstm2 = nn.LSTM(input_size=hidden_dim*2,\n","                            hidden_size=hidden_dim,\n","                            num_layers=1, \n","                            bidirectional=True)\n","        self.atten2 = Attention(hidden_dim*2, batch_first=True)\n","        self.fc1 = nn.Sequential(nn.Linear(hidden_dim*lstm_layer*2, hidden_dim*lstm_layer*2),\n","                                 nn.BatchNorm1d(hidden_dim*lstm_layer*2),\n","                                 nn.ReLU()) \n","        self.fc2 = nn.Linear(hidden_dim*lstm_layer*2, 3)\n","\n","    \n","  def forward(self, x, x_len):\n","        x = self.embedding(x)\n","        x = self.dropout(x)\n","        \n","        x = nn.utils.rnn.pack_padded_sequence(x, x_len, batch_first=True, enforce_sorted=False)\n","        out1, (h_n, c_n) = self.lstm1(x)\n","        x, lengths = nn.utils.rnn.pad_packed_sequence(out1, batch_first=True)\n","        x, _ = self.atten1(x, lengths) # skip connect\n","\n","        out2, (h_n, c_n) = self.lstm2(out1)\n","        y, lengths = nn.utils.rnn.pad_packed_sequence(out2, batch_first=True)\n","        y, _ = self.atten2(y, lengths)\n","        \n","        if len(x.shape)==1:\n","          x = x.reshape(1,-1)\n","          y = y.reshape(1,-1)\n","\n","        z = torch.cat([x, y], dim=1)\n","        z = self.fc1(self.dropout(z))\n","        z = self.fc2(self.dropout(z))\n","        return z\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r-lFAuxXXXbk"},"source":["!cp -r \"/content/drive/MyDrive/research_skill_name_prediction/model_save_BLOOM_difficulty_Lstm_data_2\" /content/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lpqEAkGCkt7W","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a228df23-2688-42cc-ca67-bb22c7cb9a46"},"source":["import numpy as np\n","model_lstm = MultiClassClassifierLstm(text.vocab.vectors, hidden_dim=128, lstm_layer=2, dropout=0.3).cuda()\n","model_lstm.load_state_dict(torch.load(\"model_save_BLOOM_difficulty_Lstm_data_2/model_weights\"))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":60}]},{"cell_type":"code","metadata":{"id":"nAdyYUrfRBzQ"},"source":["from torch import nn\n","class MultiClassClassifier(nn.Module):\n","    def __init__(self, bert_model_path, labels_count, hidden_dim=768, mlp_dim=500, extras_dim=100, dropout=0.1, freeze_bert=False):\n","        super().__init__()\n","\n","        self.bert = BertModel.from_pretrained(bert_model_path,output_hidden_states=False,output_attentions=False)\n","        self.dropout = nn.Dropout(dropout)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(hidden_dim, mlp_dim),\n","            nn.ReLU(),\n","            # nn.Linear(mlp_dim, mlp_dim),\n","            # # nn.ReLU(),\n","            # # nn.Linear(mlp_dim, mlp_dim),\n","            # nn.ReLU(),            \n","            nn.Linear(mlp_dim, labels_count)\n","        )\n","        # self.softmax = nn.LogSoftmax(dim=1)\n","        if freeze_bert:\n","            print(\"Freezing layers\")\n","            for param in self.bert.parameters():\n","                param.requires_grad = False\n","\n","    def forward(self, tokens, masks):\n","        _,pooled_output = self.bert(tokens, attention_mask=masks)\n","        dropout_output = self.dropout(pooled_output)\n","        # concat_output = torch.cat((dropout_output, topic_emb), dim=1)\n","        # concat_output = self.dropout(concat_output)\n","        mlp_output = self.mlp(dropout_output)\n","        # proba = self.sigmoid(mlp_output)\n","        # proba = self.softmax(mlp_output)\n","\n","        return mlp_output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HM0xelxGYj_X"},"source":["!cp -r \"/content/drive/MyDrive/research_skill_name_prediction/model_bert_difficulty_prediction_data_2\" /content"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H0TDkaA56dFJ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6017cfb8-f3b4-44e4-e1d5-6ea4f5db47f0"},"source":["from transformers import BertForSequenceClassification, AdamW, BertConfig\n","\n","# Loads BertForSequenceClassification, the pretrained BERT model with a single \n","model = MultiClassClassifier('bert-base-uncased',num_classes, 768,500,140,dropout=0.1,freeze_bert=False)\n","\n","model.load_state_dict(torch.load(\"model_bert_difficulty_prediction_data_2/model_weights\"))\n","\n","# Tell pytorch to run this model on the GPU.\n","model.cuda()\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MultiClassClassifier(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (mlp): Sequential(\n","    (0): Linear(in_features=768, out_features=500, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=500, out_features=3, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":64}]},{"cell_type":"code","metadata":{"id":"_g8cFpXrpo9Q"},"source":["from torch import nn\n","class MultiClassClassifier(nn.Module):\n","    def __init__(self, bert_model_path, labels_count,skill_label_count, hidden_dim=768, mlp_dim=500, extras_dim=140, dropout=0.1, freeze_bert=False):\n","        super().__init__()\n","\n","        self.bert = BertModel.from_pretrained(bert_model_path,output_hidden_states=False,output_attentions=False)\n","        self.dropout = nn.Dropout(dropout)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(hidden_dim , mlp_dim),\n","            nn.ReLU(),\n","            # nn.Linear(mlp_dim, mlp_dim),\n","            # nn.ReLU(),\n","            # nn.Linear(mlp_dim, mlp_dim),\n","            # nn.ReLU(),            \n","            nn.Linear(mlp_dim, labels_count)\n","        )\n","        self.mlp2 = nn.Sequential(\n","            nn.Linear(hidden_dim , mlp_dim),\n","            nn.ReLU(),         \n","            nn.Linear(mlp_dim, skill_label_count)\n","        )\n","        # self.softmax = nn.LogSoftmax(dim=1)\n","        if freeze_bert:\n","            print(\"Freezing layers\")\n","            for param in self.bert.parameters():\n","                param.requires_grad = False\n","\n","    def forward(self, tokens, masks):\n","        _,pooled_output = self.bert(tokens, attention_mask=masks)\n","\n","        dropout_output = self.dropout(pooled_output)\n","        concat_output = dropout_output\n","        mlp_output = self.mlp(concat_output)\n","        skill_output = self.mlp2(concat_output)\n","        # proba = self.sigmoid(mlp_output)\n","        # proba = self.softmax(mlp_output)\n","\n","        return mlp_output,skill_output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PzdVd6lUEfv8","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1126c33a-4202-431f-8ccb-c6680ce60c5f"},"source":["num_classes"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{},"execution_count":66}]},{"cell_type":"code","metadata":{"id":"kctN7UA1bhCx","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9ba7110e-a58a-4503-da34-46a24432a942"},"source":["model_multi_task = MultiClassClassifier('bert-base-uncased',3, 5,768,500,140,dropout=0.1,freeze_bert=False)\n","model_multi_task.load_state_dict(torch.load('model_bert_multi_task_prediction_data_2/model_weights'))\n","model_multi_task.cuda()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MultiClassClassifier(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (mlp): Sequential(\n","    (0): Linear(in_features=768, out_features=500, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=500, out_features=3, bias=True)\n","  )\n","  (mlp2): Sequential(\n","    (0): Linear(in_features=768, out_features=500, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=500, out_features=5, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":67}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6RuioUjMvcHi","outputId":"3bf84a7c-e328-4aa1-e386-809c80828906"},"source":["model_cascade = MultiClassClassifier('bert-base-uncased',num_classes, 768,500,140,dropout=0.1,freeze_bert=False)\n","\n","model_cascade.load_state_dict(torch.load(\"model_bert_difficulty_cascade_data_2/model_weights\"))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":69}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sj4KoYVkNJm-","outputId":"f77ca2e1-15ab-4292-ea14-8a93eeea7575"},"source":["model_cascade.cuda()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MultiClassClassifier(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (mlp): Sequential(\n","    (0): Linear(in_features=768, out_features=500, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=500, out_features=3, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":70}]},{"cell_type":"code","metadata":{"id":"rYLxYivOFpKo"},"source":["from torch import nn\n","\n","\n","class AttentionBlock(nn.Module):\n","  def __init__(self,vector_1_dim,vector_2_dim):\n","    super(AttentionBlock, self).__init__()\n","    self.Weights = nn.Parameter(torch.rand(vector_2_dim,vector_1_dim))\n","    self.bias = nn.Parameter(torch.zeros(1))\n","\n","  def forward(self,vector_1,vector_2):\n","    #(batch_size,vector_2_dim,vector_1_dim)\n","    weights = self.Weights.repeat(vector_2.size(0),1,1)\n","    vector_1 = vector_1.unsqueeze(-1)  # (batch_size,vector_2_dim,vector_1_dim)\n","    weights = weights.matmul(vector_1) # results in (batch_size,vector_2_dim,1)\n","    weights = weights.repeat(vector_2.size(1),1,1,1).transpose(0,1)\n","    vector_2 = vector_2.unsqueeze(-2)\n","    attention_weights = torch.tanh(vector_2.matmul(weights).squeeze() + self.bias) # batch_size, vector_2_dim.size(0)\n","    if len(attention_weights.shape) ==1:\n","      attention_weights = attention_weights.squeeze()\n","      attention_weights = attention_weights.reshape(1,-1)\n","    attention_weights = attention_weights.squeeze()\n","    # print(\"torch.exp(attention_weights)\",torch.exp(attention_weights).shape,attention_weights.shape,torch.exp(attention_weights).sum(dim=1).shape)\n","    attention_weights = torch.exp(attention_weights)/ torch.exp(attention_weights).sum(dim=1,keepdim=True)\n","\n","    return attention_weights\n","\n","# bloom interactive attention\n","class MultiClassClassifier(nn.Module):\n","    def __init__(self, bert_model_path, labels_count,skill_label_count, hidden_dim=768, mlp_dim=500, extras_dim=140, dropout=0.1, freeze_bert=False):\n","        super().__init__()\n","\n","        self.bert = BertModel.from_pretrained(bert_model_path,output_hidden_states=True,output_attentions=True)\n","        self.dropout = nn.Dropout(dropout)\n","        self.bloom_attention = AttentionBlock(768, 768)\n","\n","        self.mlp = nn.Sequential(\n","            nn.Linear(hidden_dim , mlp_dim),\n","            nn.ReLU(),\n","            # nn.Linear(mlp_dim, mlp_dim),\n","            # nn.ReLU(),\n","            # nn.Linear(mlp_dim, mlp_dim),\n","            # nn.ReLU(),            \n","            nn.Linear(mlp_dim, labels_count)\n","        )\n","        self.mlp2 = nn.Sequential(  \n","            nn.Linear(hidden_dim , mlp_dim),\n","            nn.ReLU(),         \n","            nn.Linear(mlp_dim, skill_label_count)\n","        )\n","        # self.softmax = nn.LogSoftmax(dim=1)\n","        if freeze_bert:\n","            print(\"Freezing layers\")\n","            for param in self.bert.parameters():\n","                param.requires_grad = False\n","\n","    def forward(self, tokens, masks):\n","        _, pooled_output,hidden_states,attentions = self.bert(tokens, attention_mask=masks)\n","        dropout_output = self.dropout(pooled_output)\n","        concat_output = dropout_output\n","\n","        # mlp_output = self.mlp(concat_output)\n","        skill_output_probas = self.mlp2(concat_output)\n","        skill_output = torch.argmax(skill_output_probas,axis=1).cpu().numpy()\n","        skill_output = LE_skill.inverse_transform(skill_output)\n","        skill_input_ids = []\n","        skill_attention_masks = []\n","        for skill_text in skill_output:\n","          encoded_skill_output = tokenizer.encode_plus(\n","                          skill_text,                      # Sentence to encode.\n","                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                          max_length = 128,           # Pad & truncate all sentences.\n","                          pad_to_max_length = True,\n","                          truncation=True,\n","                          return_attention_mask = True,   # Construct attn. masks.\n","                          return_tensors = 'pt',     # Return pytorch tensors.\n","                    )\n","          skill_input_ids.append(encoded_skill_output['input_ids'])\n","          skill_attention_masks.append(encoded_skill_output['attention_mask'])\n","        skill_input_ids = torch.cat(skill_input_ids,dim=0).cuda()\n","        skill_attention_masks = torch.cat(skill_attention_masks,dim=0).cuda()\n","        _,_,hidden_states_skill,_ = self.bert(skill_input_ids,skill_attention_masks)\n","\n","        skill_hidden_averaged =  torch.sum(hidden_states_skill[12],dim=1)/hidden_states_skill[12].shape[1]\n","\n","        bloom_attention_weights = self.bloom_attention(skill_hidden_averaged, hidden_states[12])\n","\n","        bloom_attention_weights = bloom_attention_weights.unsqueeze(-2)\n","        # print(\"context_attention_weights\",context_attention_weights.shape,context_out.shape)\n","        input_attended_vector = bloom_attention_weights.matmul(hidden_states[12]).squeeze()\n","\n","        mlp_output = self.mlp(input_attended_vector)\n","\n","        # print(\"bloom attention weights\", bloom_attention_weights.shape)\n","        # print(\"_hidden_states\",hidden_states_skill[12].shape, hidden_states[12].shape,skill_hidden_averaged.shape)\n","\n","        # proba = self.sigmoid(mlp_output)\n","        # proba = self.softmax(mlp_output)\n","\n","        return mlp_output,skill_output_probas"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xy6FOcfNRp0P","colab":{"base_uri":"https://localhost:8080/"},"outputId":"132deea7-afb0-426e-8b2b-b18de9e1674a"},"source":["skill_label_count = len(list(set(new_data[\"skill_label\"].values)))\n","skill_label_count"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5"]},"metadata":{},"execution_count":72}]},{"cell_type":"code","metadata":{"id":"qIbXoNcuUukV"},"source":["!cp -r \"/content/drive/MyDrive/research_skill_name_prediction/model_bert_multi_task_interactive_data_2_final\" /content/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wDaXrwL5Fut_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5c8eaae5-03cc-4127-e9e9-1070d141aa4e"},"source":["model_interactive = MultiClassClassifier('bert-base-uncased',num_classes, skill_label_count,768,500,140,dropout=0.1,freeze_bert=False)\n","model_interactive.load_state_dict(torch.load(\"model_bert_multi_task_interactive_data_2_final/model_weights\"))\n","model_interactive.cuda()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MultiClassClassifier(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (bloom_attention): AttentionBlock()\n","  (mlp): Sequential(\n","    (0): Linear(in_features=768, out_features=500, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=500, out_features=3, bias=True)\n","  )\n","  (mlp2): Sequential(\n","    (0): Linear(in_features=768, out_features=500, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=500, out_features=5, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":74}]},{"cell_type":"code","metadata":{"id":"VP9As7pNfeVC"},"source":["!cp -r \"/content/drive/MyDrive/research_skill_name_prediction/model_bert_multi_task_interactive_skill_given_data_2\" /content"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yjCpYGCyfZlf"},"source":["from torch import nn\n","\n","\n","class Attention(nn.Module):\n","  def __init__(self,vector_1_dim,vector_2_dim):\n","    super(Attention, self).__init__()\n","    self.Weights = nn.Parameter(torch.rand(vector_2_dim,vector_1_dim))\n","    self.bias = nn.Parameter(torch.zeros(1))\n","\n","  def forward(self,vector_1,vector_2):\n","    #(batch_size,vector_2_dim,vector_1_dim)\n","    weights = self.Weights.repeat(vector_2.size(0),1,1)\n","    vector_1 = vector_1.unsqueeze(-1)  # (batch_size,vector_2_dim,vector_1_dim)\n","    weights = weights.matmul(vector_1) # results in (batch_size,vector_2_dim,1)\n","    weights = weights.repeat(vector_2.size(1),1,1,1).transpose(0,1)\n","    vector_2 = vector_2.unsqueeze(-2)\n","    attention_weights = torch.tanh(vector_2.matmul(weights).squeeze() + self.bias) # batch_size, vector_2_dim.size(0)\n","    if len(attention_weights.shape) ==1:\n","      attention_weights = attention_weights.squeeze()\n","      attention_weights = attention_weights.reshape(1,-1)\n","    attention_weights = attention_weights.squeeze()\n","    # print(\"torch.exp(attention_weights)\",torch.exp(attention_weights).shape,attention_weights.shape,torch.exp(attention_weights).sum(dim=1).shape)\n","    attention_weights = torch.exp(attention_weights)/ torch.exp(attention_weights).sum(dim=1,keepdim=True)\n","\n","    return attention_weights\n","\n","# bloom interactive attention\n","class MultiClassClassifier(nn.Module):\n","    def __init__(self, bert_model_path, labels_count,skill_label_count, hidden_dim=768, mlp_dim=500, extras_dim=140, dropout=0.1, freeze_bert=False):\n","        super().__init__()\n","\n","        self.bert = BertModel.from_pretrained(bert_model_path,output_hidden_states=True,output_attentions=True)\n","        self.dropout = nn.Dropout(dropout)\n","        self.bloom_attention = Attention(768, 768)\n","\n","        self.mlp = nn.Sequential(\n","            nn.Linear(hidden_dim , mlp_dim),\n","            nn.ReLU(),\n","            # nn.Linear(mlp_dim, mlp_dim),\n","            # nn.ReLU(),\n","            # nn.Linear(mlp_dim, mlp_dim),\n","            # nn.ReLU(),            \n","            nn.Linear(mlp_dim, labels_count)\n","        )\n","        self.mlp2 = nn.Sequential(  \n","            nn.Linear(hidden_dim , mlp_dim),\n","            nn.ReLU(),         \n","            nn.Linear(mlp_dim, skill_label_count)\n","        )\n","        # self.softmax = nn.LogSoftmax(dim=1)\n","        if freeze_bert:\n","            print(\"Freezing layers\")\n","            for param in self.bert.parameters():\n","                param.requires_grad = False\n","\n","    def forward(self, tokens, masks,skill_label):\n","        _, pooled_output,hidden_states,attentions = self.bert(tokens, attention_mask=masks)\n","        # dropout_output = self.dropout(pooled_output)\n","        # concat_output = dropout_output\n","\n","        # # mlp_output = self.mlp(concat_output)\n","        # skill_output_probas = self.mlp2(concat_output)\n","        # skill_output = torch.argmax(skill_output_probas,axis=1).cpu().numpy()\n","        # skill_output = LE_skill.inverse_transform(skill_output)\n","        skill_input_ids = []\n","        skill_attention_masks = []\n","        skill_label = skill_label.cpu().numpy()\n","        skill_label = LE_skill.inverse_transform(skill_label)\n","\n","        for skill_text in skill_label:\n","          encoded_skill_output = tokenizer.encode_plus(\n","                          skill_text,                      # Sentence to encode.\n","                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                          max_length = 128,           # Pad & truncate all sentences.\n","                          pad_to_max_length = True,\n","                          truncation=True,\n","                          return_attention_mask = True,   # Construct attn. masks.\n","                          return_tensors = 'pt',     # Return pytorch tensors.\n","                    )\n","          skill_input_ids.append(encoded_skill_output['input_ids'])\n","          skill_attention_masks.append(encoded_skill_output['attention_mask'])\n","        skill_input_ids = torch.cat(skill_input_ids,dim=0).cuda()\n","        skill_attention_masks = torch.cat(skill_attention_masks,dim=0).cuda()\n","        _,_,hidden_states_skill,_ = self.bert(skill_input_ids,skill_attention_masks)\n","\n","        skill_hidden_averaged =  torch.sum(hidden_states_skill[12],dim=1)/hidden_states_skill[12].shape[1]\n","\n","        bloom_attention_weights = self.bloom_attention(skill_hidden_averaged, hidden_states[12])\n","\n","        bloom_attention_weights = bloom_attention_weights.unsqueeze(-2)\n","        # print(\"context_attention_weights\",context_attention_weights.shape,context_out.shape)\n","        input_attended_vector = bloom_attention_weights.matmul(hidden_states[12]).squeeze()\n","        # print(\"input_attended_vector\",input_attended_vector.shape)\n","        mlp_output = self.mlp(input_attended_vector)\n","\n","        # print(\"bloom attention weights\", bloom_attention_weights.shape)\n","        # print(\"_hidden_states\",hidden_states_skill[12].shape, hidden_states[12].shape,skill_hidden_averaged.shape)\n","\n","        # proba = self.sigmoid(mlp_output)\n","        # proba = self.softmax(mlp_output)\n","\n","        return mlp_output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kNnbEpYvfb57","outputId":"bfc7c92b-e46c-4ce8-c042-e429225cea02"},"source":["model_skill_given = MultiClassClassifier('bert-base-uncased',num_classes, skill_label_count,768,500,140,dropout=0.1,freeze_bert=False)\n","model_skill_given.load_state_dict(torch.load(\"model_bert_multi_task_interactive_skill_given_data_2/model_weights\"))\n","model_skill_given.cuda()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MultiClassClassifier(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (bloom_attention): Attention()\n","  (mlp): Sequential(\n","    (0): Linear(in_features=768, out_features=500, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=500, out_features=3, bias=True)\n","  )\n","  (mlp2): Sequential(\n","    (0): Linear(in_features=768, out_features=500, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=500, out_features=5, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":77}]},{"cell_type":"code","metadata":{"id":"nQ9_trOX96Tv"},"source":["from torch import nn\n","class SkillClassifier(nn.Module):\n","    def __init__(self, bert_model_path, labels_count, hidden_dim=768, mlp_dim=500, extras_dim=140, dropout=0.1, freeze_bert=False):\n","        super().__init__()\n","\n","        self.bert = BertModel.from_pretrained(bert_model_path,output_hidden_states=True,output_attentions=True)\n","        self.dropout = nn.Dropout(dropout)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(hidden_dim , mlp_dim),\n","            nn.ReLU(),\n","            # nn.Linear(mlp_dim, mlp_dim),\n","            # nn.ReLU(),\n","            # nn.Linear(mlp_dim, mlp_dim),\n","            # nn.ReLU(),            \n","            nn.Linear(mlp_dim, 5)\n","        )\n","        # self.softmax = nn.LogSoftmax(dim=1)\n","        if freeze_bert:\n","            print(\"Freezing layers\")\n","            for param in self.bert.parameters():\n","                param.requires_grad = False\n","\n","    def forward(self, tokens, masks):\n","        _, pooled_output,_,_ = self.bert(tokens, attention_mask=masks)\n","        dropout_output = self.dropout(pooled_output)\n","        concat_output = dropout_output\n","        mlp_output = self.mlp(concat_output)\n","        # proba = self.sigmoid(mlp_output)\n","        # proba = self.softmax(mlp_output)\n","\n","        return mlp_output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kucz3b5-Uq-A"},"source":["!cp -r \"/content/drive/MyDrive/research_skill_name_prediction/model_bert_skill_prediction_data_2\" /content"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7uqkkw379_WB"},"source":["skill_model = SkillClassifier('bert-base-uncased',num_classes, 768,500,140,dropout=0.1,freeze_bert=False)\n","skill_model.load_state_dict(torch.load(\"model_bert_skill_prediction_data_2/model_weights\"))\n","skill_model.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t19eboqPUFw9"},"source":["from torch import nn\n","\n","\n","class Attention(nn.Module):\n","  def __init__(self,vector_1_dim,vector_2_dim):\n","    super(Attention, self).__init__()\n","    self.Weights = nn.Parameter(torch.rand(vector_2_dim,vector_1_dim))\n","    self.bias = nn.Parameter(torch.zeros(1))\n","\n","  def forward(self,vector_1,vector_2):\n","    #(batch_size,vector_2_dim,vector_1_dim)\n","    weights = self.Weights.repeat(vector_2.size(0),1,1)\n","    vector_1 = vector_1.unsqueeze(-1)  # (batch_size,vector_2_dim,vector_1_dim)\n","    weights = weights.matmul(vector_1) # results in (batch_size,vector_2_dim,1)\n","    weights = weights.repeat(vector_2.size(1),1,1,1).transpose(0,1)\n","    vector_2 = vector_2.unsqueeze(-2)\n","    attention_weights = torch.tanh(vector_2.matmul(weights).squeeze() + self.bias) # batch_size, vector_2_dim.size(0)\n","    if len(attention_weights.shape) ==1:\n","      attention_weights = attention_weights.squeeze()\n","      attention_weights = attention_weights.reshape(1,-1)\n","    attention_weights = attention_weights.squeeze()\n","    # print(\"torch.exp(attention_weights)\",torch.exp(attention_weights).shape,attention_weights.shape,torch.exp(attention_weights).sum(dim=1).shape)\n","    attention_weights = torch.exp(attention_weights)/ torch.exp(attention_weights).sum(dim=1,keepdim=True)\n","\n","    return attention_weights\n","\n","# bloom interactive attention\n","class MultiClassClassifier(nn.Module):\n","    def __init__(self, bert_model_path, labels_count,skill_label_count, hidden_dim=768, mlp_dim=500, extras_dim=140, dropout=0.1, freeze_bert=False):\n","        super().__init__()\n","\n","        self.bert = BertModel.from_pretrained(bert_model_path,output_hidden_states=True,output_attentions=True)\n","\n","        self.skill_bert = skill_model\n","        self.dropout = nn.Dropout(dropout)\n","        self.bloom_attention = Attention(768, 768)\n","\n","        self.mlp = nn.Sequential(\n","            nn.Linear(hidden_dim , mlp_dim),\n","            nn.ReLU(),\n","            # nn.Linear(mlp_dim, mlp_dim),\n","            # nn.ReLU(),\n","            # nn.Linear(mlp_dim, mlp_dim),\n","            # nn.ReLU(),            \n","            nn.Linear(mlp_dim, labels_count)\n","        )\n","        self.mlp2 = nn.Sequential(  \n","            nn.Linear(hidden_dim , mlp_dim),\n","            nn.ReLU(),         \n","            nn.Linear(mlp_dim, skill_label_count)\n","        )\n","        # self.softmax = nn.LogSoftmax(dim=1)\n","        if freeze_bert:\n","            print(\"Freezing layers\")\n","            for param in self.bert.parameters():\n","                param.requires_grad = False\n","\n","    def forward(self, tokens, masks):\n","        _, pooled_output,hidden_states,attentions = self.bert(tokens, attention_mask=masks)\n","        dropout_output = self.dropout(pooled_output)\n","        concat_output = dropout_output\n","\n","        # mlp_output = self.mlp(concat_output)\n","        skill_output_probas = self.skill_bert(tokens,masks)\n","        skill_output = torch.argmax(skill_output_probas,axis=1).cpu().numpy()\n","        skill_output = LE_skill.inverse_transform(skill_output)\n","        skill_input_ids = []\n","        skill_attention_masks = []\n","        for skill_text in skill_output:\n","          encoded_skill_output = tokenizer.encode_plus(\n","                          skill_text,                      # Sentence to encode.\n","                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                          max_length = 128,           # Pad & truncate all sentences.\n","                          pad_to_max_length = True,\n","                          truncation=True,\n","                          return_attention_mask = True,   # Construct attn. masks.\n","                          return_tensors = 'pt',     # Return pytorch tensors.\n","                    )\n","          skill_input_ids.append(encoded_skill_output['input_ids'])\n","          skill_attention_masks.append(encoded_skill_output['attention_mask'])\n","        skill_input_ids = torch.cat(skill_input_ids,dim=0).cuda()\n","        skill_attention_masks = torch.cat(skill_attention_masks,dim=0).cuda()\n","        _,_,hidden_states_skill,_ = self.skill_bert.bert(skill_input_ids,skill_attention_masks)\n","\n","        skill_hidden_averaged =  torch.sum(hidden_states_skill[12],dim=1)/hidden_states_skill[12].shape[1]\n","\n","        bloom_attention_weights = self.bloom_attention(skill_hidden_averaged, hidden_states[12])\n","\n","        bloom_attention_weights = bloom_attention_weights.unsqueeze(-2)\n","        # print(\"context_attention_weights\",context_attention_weights.shape,context_out.shape)\n","        input_attended_vector = bloom_attention_weights.matmul(hidden_states[12]).squeeze()\n","\n","        mlp_output = self.mlp(input_attended_vector)\n","\n","        # print(\"bloom attention weights\", bloom_attention_weights.shape)\n","        # print(\"_hidden_states\",hidden_states_skill[12].shape, hidden_states[12].shape,skill_hidden_averaged.shape)\n","\n","        # proba = self.sigmoid(mlp_output)\n","        # proba = self.softmax(mlp_output)\n","\n","\n","        return mlp_output,skill_output_probas"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eBIZgO4OUF1O","outputId":"de48106d-9c81-470d-ea0f-b63bda8706c9"},"source":["model_interactive_pre_trained = MultiClassClassifier('bert-base-uncased',num_classes, 5,768,500,140,dropout=0.1,freeze_bert=False)\n","model_interactive_pre_trained.load_state_dict(torch.load(\"model_bert_multi_task_interactive_pre_trained_skill_bert_data_2/model_weights\"))\n","model_interactive_pre_trained.cuda()\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MultiClassClassifier(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (skill_bert): SkillClassifier(\n","    (bert): BertModel(\n","      (embeddings): BertEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): BertEncoder(\n","        (layer): ModuleList(\n","          (0): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (mlp): Sequential(\n","      (0): Linear(in_features=768, out_features=500, bias=True)\n","      (1): ReLU()\n","      (2): Linear(in_features=500, out_features=5, bias=True)\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (bloom_attention): Attention()\n","  (mlp): Sequential(\n","    (0): Linear(in_features=768, out_features=500, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=500, out_features=3, bias=True)\n","  )\n","  (mlp2): Sequential(\n","    (0): Linear(in_features=768, out_features=500, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=500, out_features=5, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":82}]},{"cell_type":"code","metadata":{"id":"6gtKYG0VeVwk"},"source":["# for param in model.bert.encoder.layer[0:12].parameters():\n","#     param.requires_grad=False\n","# for param in model.bert.embeddings.parameters():\n","#     param.requires_grad=False\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"awQ2Y9Jb3kht"},"source":["optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Ys-M4-e3khv"},"source":["\n","from transformers import get_linear_schedule_with_warmup\n","\n","\n","epochs = 30\n","\n","# Total number of training steps is [number of batches] x [number of epochs]. \n","total_steps = len(train_dataloader) * epochs\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QrYqErOD3khx","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f799af63-1c4a-4576-e9a1-66b3d2190d55"},"source":["len(train_dataloader) "],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1151"]},"metadata":{},"execution_count":75}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EWVSE9LM3kh0","outputId":"6420121f-712e-4747-a840-a51964598123"},"source":["1935 * 32"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["61920"]},"metadata":{},"execution_count":76}]},{"cell_type":"code","metadata":{"id":"rcvxVVi63kh3"},"source":["\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GUw3zm6g3kh5"},"source":["import numpy as np\n","\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ta6zfUTa3kh7"},"source":["import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GFq9gd5kQSHb"},"source":["import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9AjZgn3fwTmX"},"source":["class EarlyStopping:\n","    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n","    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n","        \"\"\"\n","        Args:\n","            patience (int): How long to wait after last time validation loss improved.\n","                            Default: 7\n","            verbose (bool): If True, prints a message for each validation loss improvement. \n","                            Default: False\n","            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n","                            Default: 0\n","            path (str): Path for the checkpoint to be saved to.\n","                            Default: 'checkpoint.pt'\n","            trace_func (function): trace print function.\n","                            Default: print            \n","        \"\"\"\n","        self.patience = patience\n","        self.verbose = verbose\n","        self.counter = 0\n","        self.best_score = None\n","        self.early_stop = False\n","        self.val_loss_min = np.Inf\n","        self.delta = delta\n","        self.path = path\n","        self.trace_func = trace_func\n","    def __call__(self, val_loss, model):\n","\n","        score = -val_loss\n","\n","        if self.best_score is None:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","        elif score < self.best_score + self.delta:\n","            self.counter += 1\n","            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","            self.counter = 0\n","\n","    def save_checkpoint(self, val_loss, model):\n","        '''Saves model when validation loss decrease.'''\n","        if self.verbose:\n","            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n","        torch.save(model.state_dict(), self.path)\n","        self.val_loss_min = val_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-qclnCpSZb2O"},"source":["loss_func = nn.CrossEntropyLoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":735},"id":"FcSwwzAFyE7S","outputId":"b73cdb9f-9d95-428b-a711-165594f4ef1c"},"source":["test"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Question</th>\n","      <th>Answer</th>\n","      <th>DifficultyFromAnswerer</th>\n","      <th>difficulty_label</th>\n","      <th>question_answer</th>\n","      <th>skill_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>How are western-style xylophones characterised?</td>\n","      <td>by a bright, sharp tone and high register</td>\n","      <td>medium</td>\n","      <td>2</td>\n","      <td>How are western-style xylophones characterised...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Is Nairobi the capital of Kenya?</td>\n","      <td>Yes</td>\n","      <td>easy</td>\n","      <td>0</td>\n","      <td>Is Nairobi the capital of Kenya? Yes</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>How many sister cities does the City of Melbou...</td>\n","      <td>six</td>\n","      <td>medium</td>\n","      <td>2</td>\n","      <td>How many sister cities does the City of Melbou...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Is the electric eel a true eel?</td>\n","      <td>No</td>\n","      <td>easy</td>\n","      <td>0</td>\n","      <td>Is the electric eel a true eel? No</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Does Swedish use the perfect participle to for...</td>\n","      <td>No.</td>\n","      <td>easy</td>\n","      <td>0</td>\n","      <td>Does Swedish use the perfect participle to for...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>337</th>\n","      <td>Where was there a vast swarm of butterflies?</td>\n","      <td>In Kyoto there was a vast swarm of butterflies.</td>\n","      <td>medium</td>\n","      <td>2</td>\n","      <td>Where was there a vast swarm of butterflies? I...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>338</th>\n","      <td>What is the most common romanization standard ...</td>\n","      <td>Hanyu Pinyin</td>\n","      <td>medium</td>\n","      <td>2</td>\n","      <td>What is the most common romanization standard ...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>339</th>\n","      <td>Is Jakarta the 12th largest city in the world?</td>\n","      <td>yes</td>\n","      <td>medium</td>\n","      <td>2</td>\n","      <td>Is Jakarta the 12th largest city in the world?...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>340</th>\n","      <td>What sort of turtles are ectothermic?</td>\n","      <td>all of them</td>\n","      <td>medium</td>\n","      <td>2</td>\n","      <td>What sort of turtles are ectothermic? all of them</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>341</th>\n","      <td>Was Gellu Naum the leader of the surrealist mo...</td>\n","      <td>Yes</td>\n","      <td>easy</td>\n","      <td>0</td>\n","      <td>Was Gellu Naum the leader of the surrealist mo...</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>342 rows × 6 columns</p>\n","</div>"],"text/plain":["                                              Question  ... skill_label\n","0      How are western-style xylophones characterised?  ...           2\n","1                     Is Nairobi the capital of Kenya?  ...           3\n","2    How many sister cities does the City of Melbou...  ...           3\n","3                      Is the electric eel a true eel?  ...           3\n","4    Does Swedish use the perfect participle to for...  ...           3\n","..                                                 ...  ...         ...\n","337       Where was there a vast swarm of butterflies?  ...           3\n","338  What is the most common romanization standard ...  ...           3\n","339     Is Jakarta the 12th largest city in the world?  ...           2\n","340              What sort of turtles are ectothermic?  ...           2\n","341  Was Gellu Naum the leader of the surrealist mo...  ...           3\n","\n","[342 rows x 6 columns]"]},"metadata":{},"execution_count":85}]},{"cell_type":"code","metadata":{"id":"4178_yLFMWmx"},"source":["test_features = test[\"question_answer\"].values\n","test_labels = test[\"difficulty_label\"].values\n","test_skill_labels = test[\"skill_label\"].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DggP9Sxdv_-l","outputId":"0891a715-2133-4176-a814-1d6de7ac6c8f"},"source":["\n","\n","\n","\n","test_labels"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([2, 0, 2, 0, 0, 0, 1, 2, 1, 2, 2, 1, 0, 0, 1, 1, 1, 0, 2, 2, 0, 2,\n","       1, 1, 1, 2, 0, 0, 0, 0, 2, 2, 2, 0, 0, 2, 1, 0, 1, 2, 1, 2, 2, 0,\n","       2, 0, 2, 2, 0, 2, 2, 1, 0, 2, 2, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 2,\n","       0, 0, 2, 2, 2, 2, 0, 0, 2, 2, 0, 2, 0, 2, 2, 2, 2, 0, 0, 0, 0, 2,\n","       0, 1, 0, 0, 0, 2, 2, 0, 1, 0, 0, 0, 0, 1, 1, 2, 2, 0, 2, 2, 0, 2,\n","       0, 0, 0, 1, 0, 2, 2, 1, 1, 1, 1, 2, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n","       0, 1, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 2, 0, 0, 1, 0, 2, 2, 2,\n","       2, 2, 0, 1, 2, 2, 0, 2, 2, 2, 1, 1, 2, 2, 1, 0, 0, 0, 2, 1, 1, 0,\n","       0, 0, 0, 0, 1, 2, 0, 2, 2, 2, 1, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0,\n","       2, 0, 0, 2, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0, 2, 2, 0, 0, 2, 2, 2, 1,\n","       2, 2, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 0, 1, 2, 2, 1, 0, 0, 0, 1,\n","       2, 0, 0, 2, 0, 2, 0, 0, 2, 1, 1, 2, 2, 0, 2, 0, 2, 2, 1, 2, 2, 2,\n","       0, 1, 2, 0, 1, 1, 2, 2, 0, 2, 1, 0, 2, 1, 0, 1, 1, 2, 0, 1, 1, 1,\n","       1, 2, 1, 0, 2, 0, 0, 2, 0, 1, 0, 1, 1, 0, 2, 0, 1, 2, 0, 0, 1, 0,\n","       1, 2, 1, 1, 1, 2, 2, 1, 2, 0, 2, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 2,\n","       2, 2, 1, 0, 1, 1, 2, 2, 2, 2, 2, 0])"]},"metadata":{},"execution_count":87}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"MlOvANUwprAw","outputId":"30d0b941-3635-418b-a82b-c2ec1310d71a"},"source":["test_features[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'How are western-style xylophones characterised? by a bright, sharp tone and high register'"]},"metadata":{},"execution_count":89}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LmmWYcW2sNHe","outputId":"c2c50439-da5b-42ce-c570-a24cd3b5b5ce"},"source":["input_ids = []\n","attention_masks = []\n","for sent in test_features:\n","\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 128,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        truncation=True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","test_labels = torch.tensor(test_labels)\n","test_skill_labels = torch.tensor(test_skill_labels)\n","\n","# Set the batch size.  \n","batch_size = 34\n","\n","# test_poincare_tensor = torch.tensor(poincare_embeddings_final,dtype=torch.float)\n","# print(test_poincare_tensor.shape)\n","# difficulty_tensor = torch.tensor(difficulty_level_vectors,dtype=torch.float)\n","# print(\"difficulty_tensor\",difficulty_tensor.shape)\n","# Combine the training inputs into a TensorDataset.\n","prediction_data = TensorDataset(input_ids, attention_masks, test_labels,test_skill_labels)\n","# Create the DataLoader.\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KkLhcFdI9doK","outputId":"61b494f2-f681-4222-836e-3045d423e6e2"},"source":["\n","\n","!pip install ax-platform==0.1.9"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: ax-platform==0.1.9 in /usr/local/lib/python3.7/dist-packages (0.1.9)\n","Requirement already satisfied: botorch==0.2.1 in /usr/local/lib/python3.7/dist-packages (from ax-platform==0.1.9) (0.2.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from ax-platform==0.1.9) (1.1.5)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from ax-platform==0.1.9) (4.4.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from ax-platform==0.1.9) (2.11.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from ax-platform==0.1.9) (1.4.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from ax-platform==0.1.9) (0.22.2.post1)\n","Requirement already satisfied: gpytorch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from botorch==0.2.1->ax-platform==0.1.9) (1.5.1)\n","Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from botorch==0.2.1->ax-platform==0.1.9) (1.9.0+cu102)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.1->botorch==0.2.1->ax-platform==0.1.9) (3.7.4.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->ax-platform==0.1.9) (2.0.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->ax-platform==0.1.9) (2.8.2)\n","Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas->ax-platform==0.1.9) (1.19.5)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->ax-platform==0.1.9) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->ax-platform==0.1.9) (1.15.0)\n","Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly->ax-platform==0.1.9) (1.3.3)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->ax-platform==0.1.9) (1.0.1)\n"]}]},{"cell_type":"code","metadata":{"id":"G0H4JBrl-FnZ"},"source":["from ax import optimize"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SX-RiYbQPDpx"},"source":["from sklearn.metrics import precision_recall_fscore_support\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zFediYEjlKjX"},"source":["def get_confusion_matrix(predicted,actual):\n","    conf_matrix = np.zeros((3, 3))\n","    for pred,act in zip(predicted,actual):\n","        conf_matrix[act,pred]+=1\n","    return conf_matrix\n","        \n","def get_TP(confusion_matrix,label):\n","    tp = confusion_matrix[label][label]\n","    return tp\n","\n","def get_FN(confusion_matrix,label):\n","    row = confusion_matrix[label,]\n","    row_truepositives = row[label]\n","    fn = row.sum() - row_truepositives\n","    return fn\n","\n","def get_FP(confusion_matrix,tag):\n","    col = confusion_matrix[:,tag]\n","    col_tp = col[tag]\n","    #  sum of all values in column except tp\n","    fp = col.sum() - col_tp\n","    return fp\n","def Precision(conf_matrix):\n","    precision = 0.0\n","    for label in [0,1,2]:\n","        dividor= get_TP(conf_matrix,label)+get_FP(conf_matrix,label)\n","        if dividor != 0.0:\n","            precision += (get_TP(conf_matrix,label))/dividor\n","    return (precision / 3)\n","\n","def Recall(conf_matrix):\n","    recall = 0.0\n","    for label in [0,1,2]:\n","        dividor=get_TP(conf_matrix,label)+get_FN(conf_matrix,label)\n","        if dividor != 0.0:\n","            recall += (get_TP(conf_matrix,label))/dividor\n","    return (recall / 3)\n","\n","def F1(precision,recall):\n","    return (2*precision*recall)/(precision+recall)\n","def accuracy_per_class(preds_flat, labels_flat):\n","\n","    for label in np.unique(labels_flat):\n","        y_preds = preds_flat[labels_flat==label]\n","        y_true = labels_flat[labels_flat==label]\n","        print(f'Class: {label}')\n","        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')\n","def print_metrics(predictions,test_labels):\n","    conf_matrix = get_confusion_matrix(predictions,test_labels)\n","    precision = Precision(conf_matrix)\n","    recall = Recall(conf_matrix)\n","    f1_score = F1(precision,recall)\n","    return (precision,recall,f1_score)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aFEi23ejPfOm"},"source":["\n","def Precision_macro_weighted(conf_matrix,test_samples):\n","    accum =0\n","    label_wise_precision = dict()\n","    for label in [0,1,2]:\n","        true_sample = [sample for sample in test_samples if sample==label ]\n","        if (get_TP(conf_matrix,label)+get_FP(conf_matrix,label))!=0:\n","            accum+= float(len(true_sample)) *(get_TP(conf_matrix,label)/(get_TP(conf_matrix,label)+get_FP(conf_matrix,label)))\n","            label_wise_precision[label] = get_TP(conf_matrix,label)/(get_TP(conf_matrix,label)+get_FP(conf_matrix,label))\n","\n","    \n","    precision =  accum/len(test_samples)\n","            \n","    return precision\n","\n","\n","def Recall_macro_weighted(conf_matrix,test_samples):\n","    accum =0\n","    label_wise_recall = dict()\n","    for label in [0,1,2]:\n","        true_sample = [sample for sample in test_samples if sample==label ]\n","\n","        if (get_TP(conf_matrix,label)+get_FN(conf_matrix,label))!=0:\n","            accum+= float(len(true_sample)) * (get_TP(conf_matrix,label)/(get_TP(conf_matrix,label)+get_FN(conf_matrix,label)))\n","            label_wise_recall[label] = get_TP(conf_matrix,label)/(get_TP(conf_matrix,label)+get_FN(conf_matrix,label))\n","\n","    \n","    recall =  accum/len(test_samples)\n","    return recall\n","def print_weighted_metrics(predictions,test_labels):\n","    conf_matrix = get_confusion_matrix(predictions,test_labels)\n","    precision = Precision_macro_weighted(conf_matrix,test_labels)\n","    recall = Recall_macro_weighted(conf_matrix,test_labels)\n","    f1_score = F1(precision,recall)\n","    return (precision,recall,f1_score)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9F7JWv8uFrL-"},"source":["def make_lstm_and_gru_predictions(index, params):\n","  model_lstm.eval()\n","  # model_gru.eval()\n","  for i,batch in enumerate(test_iter):\n","    outputs = []\n","    if i == index:\n","      # print(\"i\",i,index)\n","      with torch.no_grad():\n","          question, x_len = batch.text\n","          x = question.cuda()\n","          # outs = sigmoid(outs.cpu().data.numpy()).tolist()\n","          y = batch.label.type(torch.long).cuda()\n","          if params['lstm']>=0.5:\n","            lstm_outputs = model_lstm(x,x_len)\n","            outputs.append(lstm_outputs)\n","          # if params[\"gru\"]>=0.5:\n","          #   gru_outputs = model_gru(x,x_len)\n","          #   outputs.append(gru_outputs)\n","          \n","          return outputs\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZqKPe4ZN6vPY"},"source":["def make_predictions(params):\n","  # Prediction on test set\n","    print(params)\n","    print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n","\n","    # Put model in evaluation mode\n","    model.eval()\n","    model_multi_task.eval()\n","    model_cascade.eval()\n","    model_interactive.eval()\n","\n","    # Tracking variables \n","    predictions , true_labels,ids = [], [], []\n","\n","\n","    # Predict \n","    for index,batch in enumerate(prediction_dataloader):\n","      final_outputs = []\n","      # Add batch to GPU\n","      batch = tuple(t.to(device) for t in batch)\n","      \n","      # Unpack the inputs from our dataloader\n","      b_input_ids, b_input_mask, b_labels, skill_labels = batch\n","      # print(\"skill_labels\",skill_labels)\n","      # Telling the model not to compute or store gradients, saving memory and \n","      # speeding up prediction\n","      with torch.no_grad():\n","          # Forward pass, calculate logit predictions\n","          if params['multi_task']>=0.5:\n","            # print(\"multi\")\n","            outputs,_ = model_multi_task(b_input_ids,b_input_mask)\n","            # print(\"outputs\",outputs.shape,b_input_ids.shape,batch[0].shape,index)\n","            final_outputs.append(outputs)\n","          if params['cascade']>=0.5:\n","            # print(\"cascade\")\n","            output_cascade = model_cascade(b_input_ids,b_input_mask)\n","            final_outputs.append(output_cascade)\n","\n","          if params['interactive']>=0.5:\n","            output_interactive, skill_probs  = model_interactive(b_input_ids,b_input_mask)\n","            final_outputs.append(output_interactive)\n","          if params['difficulty'] >=0:\n","            # print(\"normal\")\n","            output_difficulty = model(b_input_ids,b_input_mask)\n","            final_outputs.append(output_difficulty)\n","\n","          out = make_lstm_and_gru_predictions(index,params)\n","          # print(\"out\",out[0].shape,output_difficulty.shape)\n","          if len(out)>0:\n","            final_outputs.append(out[0])\n","          if len(out) >1:\n","            final_outputs.append(out[1])\n","      # logits_2 = outputs\n","      # logist_1 = output_bert[0]\n","      predictions_1 = final_outputs\n","      logits = torch.mean(torch.stack(predictions_1), dim=0)\n","      # else:\n","        # logits = predictions_1[0]\n","      # Move logits and labels to CPU\n","      logits = logits.detach().cpu().numpy()\n","      label_ids = b_labels.to('cpu').numpy()\n","\n","      \n","      # Store predictions and true labels\n","      predictions.append(logits)\n","      true_labels.append(label_ids)\n","    flat_predictions = np.concatenate(predictions, axis=0)\n","\n","    flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","\n","    # Combine the correct labels for each batch into a single list.\n","    flat_true_labels = np.concatenate(true_labels, axis=0)\n","    # metrics = precision_recall_fscore_support(flat_true_labels, flat_predictions, average='micro')\n","    metrics = print_weighted_metrics(flat_predictions,flat_true_labels)\n","    print(metrics)\n","    print('    DONE.')\n","    return metrics[2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QkC2S9bkuVA1"},"source":["for i,batch in enumerate(test_iter):\n","  if len(batch)==1:\n","    print(i,len(batch))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dmZeexRZhF6U","outputId":"07fc2b96-0c59-4297-a2d5-d25fa654c019"},"source":["import torch.nn.functional as F\n","from ax.modelbridge.generation_strategy import GenerationStep, GenerationStrategy\n","from ax.modelbridge.registry import Models\n","\n","best_parameters, best_values, experiment, model_100 = optimize(\n","        parameters=[\n","          {\n","              \n","            \"name\": \"multi_task\",\n","            \"type\": \"range\",\n","            \"bounds\": [1,2],\n","          },\n","           {\n","              \n","            \"name\": \"interactive\",\n","            \"type\": \"range\",\n","            \"bounds\": [1,2],\n","          },\n","          {\n","            \"name\": \"cascade\",\n","            \"type\": \"range\",\n","            \"bounds\": [0,1],\n","          },\n","               {\n","            \"name\": \"lstm\",\n","            \"type\": \"range\",\n","            \"bounds\": [0,1],\n","          },\n"," \n","           {\n","            \"name\": \"difficulty\",\n","            \"type\": \"range\",\n","            \"bounds\": [0,1],\n","          },\n","        ],\n","        # Booth function\n","        evaluation_function=make_predictions,\n","        generation_strategy = GenerationStrategy(name=\"Sobol+GPEI\", steps=[GenerationStep(model=Models.SOBOL, num_arms=10),\n","                GenerationStep(model=Models.GPEI, num_arms=12)]),\n","        minimize=False,\n","    )"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[INFO 09-05 19:18:33] ax.service.managed_loop: Started full optimization with 20 steps.\n","[INFO 09-05 19:18:33] ax.service.managed_loop: Running optimization trial 1...\n"]},{"output_type":"stream","name":"stdout","text":["{'multi_task': 2, 'interactive': 1, 'cascade': 0, 'lstm': 1, 'difficulty': 1}\n","Predicting labels for 342 test sentences...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning:\n","\n","The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\n","[INFO 09-05 19:18:54] ax.service.managed_loop: Running optimization trial 2...\n"]},{"output_type":"stream","name":"stdout","text":["(0.6758465505422514, 0.6812865497076024, 0.6785556471446246)\n","    DONE.\n","{'multi_task': 2, 'interactive': 1, 'cascade': 1, 'lstm': 1, 'difficulty': 1}\n","Predicting labels for 342 test sentences...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning:\n","\n","The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\n","[INFO 09-05 19:19:21] ax.service.managed_loop: Running optimization trial 3...\n"]},{"output_type":"stream","name":"stdout","text":["(0.6894674873998182, 0.6929824561403509, 0.691220503251964)\n","    DONE.\n","{'multi_task': 1, 'interactive': 1, 'cascade': 1, 'lstm': 1, 'difficulty': 1}\n","Predicting labels for 342 test sentences...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning:\n","\n","The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\n","[INFO 09-05 19:19:47] ax.service.managed_loop: Running optimization trial 4...\n"]},{"output_type":"stream","name":"stdout","text":["(0.6962598710803114, 0.7017543859649122, 0.6989963311742783)\n","    DONE.\n","{'multi_task': 2, 'interactive': 1, 'cascade': 1, 'lstm': 0, 'difficulty': 1}\n","Predicting labels for 342 test sentences...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning:\n","\n","The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\n","[INFO 09-05 19:20:14] ax.service.managed_loop: Running optimization trial 5...\n"]},{"output_type":"stream","name":"stdout","text":["(0.7145273440010282, 0.716374269005848, 0.7154496145514749)\n","    DONE.\n","{'multi_task': 2, 'interactive': 2, 'cascade': 1, 'lstm': 1, 'difficulty': 1}\n","Predicting labels for 342 test sentences...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning:\n","\n","The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\n","[INFO 09-05 19:20:40] ax.service.managed_loop: Running optimization trial 6...\n"]},{"output_type":"stream","name":"stdout","text":["(0.700235267114248, 0.6988304093567251, 0.6995321328985777)\n","    DONE.\n","{'multi_task': 1, 'interactive': 2, 'cascade': 0, 'lstm': 0, 'difficulty': 1}\n","Predicting labels for 342 test sentences...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning:\n","\n","The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\n","[INFO 09-05 19:21:02] ax.service.managed_loop: Running optimization trial 7...\n"]},{"output_type":"stream","name":"stdout","text":["(0.6837606225117427, 0.6900584795321637, 0.6868951157142076)\n","    DONE.\n","{'multi_task': 2, 'interactive': 1, 'cascade': 1, 'lstm': 0, 'difficulty': 0}\n","Predicting labels for 342 test sentences...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning:\n","\n","The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\n","[INFO 09-05 19:21:28] ax.service.managed_loop: Running optimization trial 8...\n"]},{"output_type":"stream","name":"stdout","text":["(0.7145273440010282, 0.716374269005848, 0.7154496145514749)\n","    DONE.\n","{'multi_task': 2, 'interactive': 2, 'cascade': 0, 'lstm': 0, 'difficulty': 1}\n","Predicting labels for 342 test sentences...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning:\n","\n","The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\n","[INFO 09-05 19:21:49] ax.service.managed_loop: Running optimization trial 9...\n"]},{"output_type":"stream","name":"stdout","text":["(0.6837606225117427, 0.6900584795321637, 0.6868951157142076)\n","    DONE.\n","{'multi_task': 2, 'interactive': 2, 'cascade': 0, 'lstm': 1, 'difficulty': 1}\n","Predicting labels for 342 test sentences...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning:\n","\n","The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\n","[INFO 09-05 19:22:11] ax.service.managed_loop: Running optimization trial 10...\n"]},{"output_type":"stream","name":"stdout","text":["(0.6484063782368977, 0.6637426900584795, 0.6559849092468403)\n","    DONE.\n","{'multi_task': 2, 'interactive': 2, 'cascade': 1, 'lstm': 1, 'difficulty': 0}\n","Predicting labels for 342 test sentences...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning:\n","\n","The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\n","[INFO 09-05 19:22:37] ax.service.managed_loop: Running optimization trial 11...\n"]},{"output_type":"stream","name":"stdout","text":["(0.6728254122256512, 0.6783625730994152, 0.6755826470352501)\n","    DONE.\n","{'multi_task': 2, 'interactive': 1, 'cascade': 1, 'lstm': 0, 'difficulty': 1}\n","Predicting labels for 342 test sentences...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning:\n","\n","The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\n","[INFO 09-05 19:23:05] ax.service.managed_loop: Running optimization trial 12...\n"]},{"output_type":"stream","name":"stdout","text":["(0.7145273440010282, 0.716374269005848, 0.7154496145514749)\n","    DONE.\n","{'multi_task': 2, 'interactive': 1, 'cascade': 1, 'lstm': 0, 'difficulty': 1}\n","Predicting labels for 342 test sentences...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning:\n","\n","The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\n","[INFO 09-05 19:23:32] ax.service.managed_loop: Running optimization trial 13...\n"]},{"output_type":"stream","name":"stdout","text":["(0.7145273440010282, 0.716374269005848, 0.7154496145514749)\n","    DONE.\n","{'multi_task': 2, 'interactive': 1, 'cascade': 1, 'lstm': 0, 'difficulty': 1}\n","Predicting labels for 342 test sentences...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning:\n","\n","The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\n","[INFO 09-05 19:24:00] ax.service.managed_loop: Running optimization trial 14...\n"]},{"output_type":"stream","name":"stdout","text":["(0.7145273440010282, 0.716374269005848, 0.7154496145514749)\n","    DONE.\n","{'multi_task': 2, 'interactive': 1, 'cascade': 1, 'lstm': 0, 'difficulty': 1}\n","Predicting labels for 342 test sentences...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning:\n","\n","The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\n","[INFO 09-05 19:24:27] ax.service.managed_loop: Running optimization trial 15...\n"]},{"output_type":"stream","name":"stdout","text":["(0.7145273440010282, 0.716374269005848, 0.7154496145514749)\n","    DONE.\n","{'multi_task': 2, 'interactive': 1, 'cascade': 1, 'lstm': 0, 'difficulty': 1}\n","Predicting labels for 342 test sentences...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning:\n","\n","The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\n","[INFO 09-05 19:24:54] ax.service.managed_loop: Running optimization trial 16...\n"]},{"output_type":"stream","name":"stdout","text":["(0.7145273440010282, 0.716374269005848, 0.7154496145514749)\n","    DONE.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/gpytorch/utils/cholesky.py:44: NumericalWarning:\n","\n","A not p.d., added jitter of 1.0e-08 to the diagonal\n","\n"]},{"output_type":"stream","name":"stdout","text":["{'multi_task': 2, 'interactive': 1, 'cascade': 1, 'lstm': 0, 'difficulty': 1}\n","Predicting labels for 342 test sentences...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning:\n","\n","The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\n","[INFO 09-05 19:25:22] ax.service.managed_loop: Running optimization trial 17...\n"]},{"output_type":"stream","name":"stdout","text":["(0.7145273440010282, 0.716374269005848, 0.7154496145514749)\n","    DONE.\n","{'multi_task': 2, 'interactive': 1, 'cascade': 1, 'lstm': 0, 'difficulty': 1}\n","Predicting labels for 342 test sentences...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning:\n","\n","The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\n","[INFO 09-05 19:25:49] ax.service.managed_loop: Running optimization trial 18...\n"]},{"output_type":"stream","name":"stdout","text":["(0.7145273440010282, 0.716374269005848, 0.7154496145514749)\n","    DONE.\n","{'multi_task': 2, 'interactive': 1, 'cascade': 1, 'lstm': 0, 'difficulty': 1}\n","Predicting labels for 342 test sentences...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning:\n","\n","The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\n","[INFO 09-05 19:26:16] ax.service.managed_loop: Running optimization trial 19...\n"]},{"output_type":"stream","name":"stdout","text":["(0.7145273440010282, 0.716374269005848, 0.7154496145514749)\n","    DONE.\n","{'multi_task': 2, 'interactive': 1, 'cascade': 1, 'lstm': 0, 'difficulty': 1}\n","Predicting labels for 342 test sentences...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning:\n","\n","The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\n","[INFO 09-05 19:26:43] ax.service.managed_loop: Running optimization trial 20...\n"]},{"output_type":"stream","name":"stdout","text":["(0.7145273440010282, 0.716374269005848, 0.7154496145514749)\n","    DONE.\n","{'multi_task': 2, 'interactive': 1, 'cascade': 1, 'lstm': 0, 'difficulty': 1}\n","Predicting labels for 342 test sentences...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning:\n","\n","The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\n"]},{"output_type":"stream","name":"stdout","text":["(0.7145273440010282, 0.716374269005848, 0.7154496145514749)\n","    DONE.\n"]}]},{"cell_type":"code","metadata":{"id":"uw569V5iQT0i"},"source":["# output_difficulty = model(b_input_ids,b_input_mask)\n","#weighted\n","best_parameters,best_values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EOBMqL72_6-e","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ab61155e-7b99-44c8-bd2d-ec35eb426f83"},"source":["    print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n","\n","    # Put model in evaluation mode\n","    model.eval()\n","    model_multi_task.eval()\n","    model_cascade.eval()\n","    model_interactive.eval()\n","\n","    # Tracking variables \n","    predictions , true_labels,ids = [], [], []\n","\n","\n","    # Predict \n","    for index,batch in enumerate(prediction_dataloader):\n","      final_outputs = []\n","      # Add batch to GPU\n","      batch = tuple(t.to(device) for t in batch)\n","      \n","      # Unpack the inputs from our dataloader\n","      b_input_ids, b_input_mask, b_labels, skill_labels = batch\n","      # Telling the model not to compute or store gradients, saving memory and \n","      # speeding up prediction\n","      with torch.no_grad():\n","          # Forward pass, calculate logit predictions\n","            # print(\"multi\")\n","          outputs,_ = model_multi_task(b_input_ids,b_input_mask)\n","            # print(\"outputs\",outputs.shape,b_input_ids.shape,batch[0].shape,index)\n","          final_outputs.append(outputs)\n","\n","          output_cascade = model_cascade(b_input_ids,b_input_mask)\n","          final_outputs.append(output_cascade)\n","          # outputs,_ = model_interactive_pre_trained(b_input_ids,b_input_mask)\n","          # final_outputs.append(outputs)\n","\n","            # print(\"normal\")\n","          output_difficulty = model(b_input_ids,b_input_mask)\n","          final_outputs.append(output_difficulty)\n","          interactive_output,_ = model_interactive(b_input_ids,b_input_mask)\n","          final_outputs.append(interactive_output)\n","      # logits_2 = outputs\n","      # logist_1 = output_bert[0]\n","      predictions_1 = final_outputs\n","      logits = torch.mean(torch.stack(predictions_1), dim=0)\n","      # else:\n","        # logits = predictions_1[0]\n","      # Move logits and labels to CPU\n","      logits = logits.detach().cpu().numpy()\n","      label_ids = b_labels.to('cpu').numpy()\n","\n","      \n","      # Store predictions and true labels\n","      predictions.append(logits)\n","      true_labels.append(label_ids)\n","    flat_predictions = np.concatenate(predictions, axis=0)\n","\n","    flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","\n","    # Combine the correct labels for each batch into a single list.\n","    flat_true_labels = np.concatenate(true_labels, axis=0)\n","    # metrics = precision_recall_fscore_support(flat_true_labels, flat_predictions, average='micro')\n","    metrics = print_weighted_metrics(flat_predictions,flat_true_labels)\n","    macro_metrics = print_metrics(flat_predictions,flat_true_labels)\n","    print(\"macro_metrics\",macro_metrics)\n","\n","    print(\"weighted\",metrics)\n","    print('    DONE.')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicting labels for 342 test sentences...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning:\n","\n","The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\n"]},{"output_type":"stream","name":"stdout","text":["macro_metrics (0.7050915750915752, 0.6795655867006433, 0.6920932966317752)\n","weighted (0.7145273440010282, 0.716374269005848, 0.7154496145514749)\n","    DONE.\n"]}]},{"cell_type":"markdown","metadata":{"id":"dBS6OmWO1DJ6"},"source":["Now for comapring statistical significance between ensemble mlp and bo ensemble\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TBqbaDo11IZc","outputId":"dccfd8fd-bbd7-46b6-f64c-777ad6b71216"},"source":["from sklearn.model_selection import KFold\n","kf = KFold(n_splits=5, shuffle=True)\n","kf.split(test_features)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<generator object _BaseKFold.split at 0x7f562313c7d0>"]},"metadata":{},"execution_count":109}]},{"cell_type":"code","metadata":{"id":"k6Fk2gObC8iw"},"source":["def get_bo_predictions(prediction_dataloader):\n","  predictions=[]\n","  true_labels=[]\n","  for index,batch in enumerate(prediction_dataloader):\n","      final_outputs = []\n","      # Add batch to GPU\n","      batch = tuple(t.to(device) for t in batch)\n","      \n","      # Unpack the inputs from our dataloader\n","      b_input_ids, b_input_mask, b_labels, id = batch\n","      # Telling the model not to compute or store gradients, saving memory and \n","      # speeding up prediction\n","      with torch.no_grad():\n","          # Forward pass, calculate logit predictions\n","            # print(\"multi\")\n","          outputs,_ = model_multi_task(b_input_ids,b_input_mask)\n","            # print(\"outputs\",outputs.shape,b_input_ids.shape,batch[0].shape,index)\n","          final_outputs.append(outputs)\n","\n","          output_cascade = model_cascade(b_input_ids,b_input_mask)\n","          final_outputs.append(output_cascade)\n","          # outputs,_ = model_interactive_pre_trained(b_input_ids,b_input_mask)\n","          # final_outputs.append(outputs)\n","\n","            # print(\"normal\")\n","          output_difficulty = model(b_input_ids,b_input_mask)\n","          final_outputs.append(output_difficulty)\n","          interactive_output,_ = model_interactive(b_input_ids,b_input_mask)\n","          final_outputs.append(interactive_output)\n","      # logits_2 = outputs\n","      # logist_1 = output_bert[0]\n","      predictions_1 = final_outputs\n","      logits = torch.mean(torch.stack(predictions_1), dim=0)\n","      # else:\n","        # logits = predictions_1[0]\n","      # Move logits and labels to CPU\n","      logits = logits.detach().cpu().numpy()\n","      label_ids = b_labels.to('cpu').numpy()\n","\n","      \n","      # Store predictions and true labels\n","      predictions.append(logits)\n","      true_labels.append(label_ids)\n","  flat_predictions = np.concatenate(predictions, axis=0)\n","\n","  flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","\n","    # Combine the correct labels for each batch into a single list.\n","  flat_true_labels = np.concatenate(true_labels, axis=0)\n","  # metrics = precision_recall_fscore_support(flat_true_labels, flat_predictions, average='micro')\n","  metrics = print_weighted_metrics(flat_predictions,flat_true_labels)\n","  macro_metrics = print_metrics(flat_predictions,flat_true_labels)\n","  return metrics[2],macro_metrics[2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YX2oGxtlEwDb"},"source":["!cp -r \"/content/drive/MyDrive/research_skill_name_prediction/MLP_ensemble_data_2\" /content/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MpxaFiZIFWnx"},"source":["# from torch import nn\n","# class MLPStackedEnsemble(nn.Module):\n","#   def __init__(self,hidden_dim=5,dropout=0.2):\n","#     super(MLPStackedEnsemble, self).__init__()\n","\n","#     self.dropout = nn.Dropout(p=dropout)\n","#     self.linear1 = nn.Linear(hidden_dim, hidden_dim)\n","#     self.linear2 = nn.Linear(hidden_dim,3)\n","#   def forward(self,input):\n","#     intermediate_out = self.linear1(input)\n","#     intermediate_out = self.dropout(intermediate_out)\n","#     final_out = self.linear2(intermediate_out)\n","#     return final_out\n","from torch import nn\n","class MLPStackedEnsemble(nn.Module):\n","  def __init__(self,hidden_dim=4,dropout=0.2):\n","    super(MLPStackedEnsemble, self).__init__()\n","\n","    # self.dropout = nn.Dropout(p=dropout)\n","    # self.linear1 = nn.Linear(hidden_dim, hidden_dim)\n","    # self.linear2 = nn.Linear(hidden_dim,hidden_dim)\n","    self.mlp = nn.Sequential(\n","        nn.Linear(hidden_dim, hidden_dim),\n","\n","        nn.ReLU(),\n","        nn.Dropout(p=dropout),\n","\n","        nn.Linear(hidden_dim,hidden_dim),\n","        nn.ReLU()\n","    )\n","    self.output = nn.Linear(hidden_dim,3)\n","  def forward(self,input):\n","    # intermediate_out = self.linear1(input)\n","    # intermediate_out = nn.ReLU(intermediate_out)\n","    # intermediate_out = self.dropout(intermediate_out)\n","    # final_out = self.linear2(intermediate_out)\n","    # final_out = nn.ReLU(final_out)\n","    output = self.mlp(input)\n","    final_out = self.output(output)\n","    return final_out\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qL6Ivc9zFdhO","outputId":"d02b33c7-73a8-417e-cf06-9208d2f86d69"},"source":["stacking_model = MLPStackedEnsemble()\n","stacking_model.load_state_dict(torch.load(\"MLP_ensemble_data_2/model_weights\"))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":122}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K4OYjlXrG4PB","outputId":"3476f6f8-8549-4ffa-9c49-e4698167dd4a"},"source":["stacking_model.cuda()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MLPStackedEnsemble(\n","  (mlp): Sequential(\n","    (0): Linear(in_features=4, out_features=4, bias=True)\n","    (1): ReLU()\n","    (2): Dropout(p=0.2, inplace=False)\n","    (3): Linear(in_features=4, out_features=4, bias=True)\n","    (4): ReLU()\n","  )\n","  (output): Linear(in_features=4, out_features=3, bias=True)\n",")"]},"metadata":{},"execution_count":123}]},{"cell_type":"code","metadata":{"id":"vPqqc9JXGG7j"},"source":["def make_lstm_and_gru_predictions_mlp(index,iterator):\n","  model_lstm.eval()\n","  # model_gru.eval()\n","  for i,batch in enumerate(iterator):\n","    outputs = []\n","    if i == index:\n","      with torch.no_grad():\n","          question, x_len = batch.text\n","          x = question.cuda()\n","          # outs = sigmoid(outs.cpu().data.numpy()).tolist()\n","          y = batch.label.type(torch.long).cuda()\n","\n","          lstm_outputs = model_lstm(x,x_len)\n","          outputs.append(lstm_outputs)\n","          # gru_outputs = model_gru(x,x_len)\n","          # outputs.append(gru_outputs)\n","\n","          return outputs\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1h94v1ElFyOY"},"source":["def make_mlp_predictions(prediction_dataloader):\n","  # Prediction on test set\n","    predictions = []\n","    true_labels = []\n","    for index,batch in enumerate(prediction_dataloader):\n","\n","        # Progress update every 40 batches.\n","        stacking_model.eval()\n","        model.eval()\n","        model_multi_task.eval()\n","        model_cascade.eval()\n","        final_outputs = []\n","        # Add batch to GPU\n","        batch = tuple(t.to(device) for t in batch)\n","        # print(\"index\",index)\n","        # Unpack the inputs from our dataloader\n","        b_input_ids, b_input_mask, b_labels,_ = batch\n","\n","        # Telling the model not to compute or store gradients, saving memory and \n","        # speeding up prediction\n","        with torch.no_grad():\n","            # Forward pass, calculate logit predictions\n","            # if params['multi_task']>=0.5:\n","              # print(\"multi\")\n","            outputs,_ = model_multi_task(b_input_ids,b_input_mask)\n","            # print(np.concatenate(outputs,axis=0).shape)\n","            final_outputs.append(np.argmax(outputs.detach().cpu().numpy(), axis=1).flatten())\n","          # if params['cascade']>=0.5:\n","            # print(\"cascade\")\n","            output_cascade = model_cascade(b_input_ids,b_input_mask)\n","            final_outputs.append(np.argmax(output_cascade.detach().cpu().numpy(),axis=1).flatten())\n","          # if params['difficulty'] >=0:\n","            # print(\"normal\")\n","            output_difficulty = model(b_input_ids,b_input_mask)\n","            final_outputs.append(np.argmax(output_difficulty.detach().cpu().numpy(),axis=1).flatten())\n","\n","            out = make_lstm_and_gru_predictions_mlp(index,test_iter)\n","            final_outputs.append(np.argmax(out[0].detach().cpu().numpy(),axis=1).flatten())\n","            # final_outputs.append(np.argmax(out[1].detach().cpu().numpy(),axis=1).flatten())\n","\n","\n","\n","\n","            inputs_ensemble = np.vstack(final_outputs).transpose()\n","            inputs_ensemble = torch.tensor(inputs_ensemble,dtype=float).float().cuda()  \n","            probas = stacking_model(inputs_ensemble)\n","      # else:\n","        # logits = predictions_1[0]\n","      # Move logits and labels to CPU\n","        logits = probas.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","      \n","      # Store predictions and true labels\n","        predictions.append(logits)\n","        true_labels.append(label_ids)\n","    flat_predictions = np.concatenate(predictions, axis=0)\n","\n","    flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","\n","    # Combine the correct labels for each batch into a single list.\n","    flat_true_labels = np.concatenate(true_labels, axis=0)\n","    # metrics = precision_recall_fscore_support(flat_true_labels, flat_predictions, average='micro')\n","    metrics = print_weighted_metrics(flat_predictions,flat_true_labels)\n","    macro_metrics = print_metrics(flat_predictions, flat_true_labels)\n","    print(\"weighted_metrics\",metrics)\n","    print(\"macro_metrics\",macro_metrics)\n","\n","    print('    DONE.')\n","    return metrics[2],macro_metrics[2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0_k-mOsuSKYR","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fe470d69-6ad7-44f6-e472-4a32ea7f6eb4"},"source":["import torch.nn.functional as F\n","make_mlp_predictions(prediction_dataloader)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["weighted_metrics (0.6529484153955918, 0.6608187134502924, 0.6568599903853103)\n","macro_metrics (0.6358292559022486, 0.6294098612627917, 0.6326032736657096)\n","    DONE.\n"]},{"output_type":"execute_result","data":{"text/plain":["(0.6568599903853103, 0.6326032736657096)"]},"metadata":{},"execution_count":133}]},{"cell_type":"code","metadata":{"id":"19sDbZXQF0k7"},"source":["import torch.nn.functional as F\n","make_mlp_predictions(prediction_dataloader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3QFUh7sqKv9n"},"source":["test_labels = test[\"difficulty_label\"].values\n","test_skill_labels = test[\"skill_label\"].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YlXpc52FUTp1","colab":{"base_uri":"https://localhost:8080/","height":108},"outputId":"1abeb270-079b-48b0-ad4b-957516d0fed6"},"source":["test.iloc[[2,3],:]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Question</th>\n","      <th>Answer</th>\n","      <th>DifficultyFromAnswerer</th>\n","      <th>difficulty_label</th>\n","      <th>question_answer</th>\n","      <th>skill_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2</th>\n","      <td>How many sister cities does the City of Melbou...</td>\n","      <td>six</td>\n","      <td>medium</td>\n","      <td>2</td>\n","      <td>How many sister cities does the City of Melbou...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Is the electric eel a true eel?</td>\n","      <td>No</td>\n","      <td>easy</td>\n","      <td>0</td>\n","      <td>Is the electric eel a true eel? No</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            Question  ... skill_label\n","2  How many sister cities does the City of Melbou...  ...           3\n","3                    Is the electric eel a true eel?  ...           3\n","\n","[2 rows x 6 columns]"]},"metadata":{},"execution_count":135}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OaozdK_RDJ0I","outputId":"42ea2b92-2b14-4068-829e-725ffdaf1843"},"source":["for indices in kf.split(test_features):\n","  print(len(indices[0]),len(indices[1]))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["273 69\n","273 69\n","274 68\n","274 68\n","274 68\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K3gCmcJN2lJH","outputId":"84f95b97-8d7d-489c-eb79-4c94b24fb582"},"source":["import torch.nn.functional as F\n","f1_bo_ensemble = []\n","f1_mlp_ensemble =[]\n","macro_f1_bo_ensemble = []\n","macro_f1_mlp_ensemble = []\n","for indices in kf.split(test_features):\n","  input_ids = []\n","  attention_masks = []\n","  for sent in test_features[indices[1]]:\n","\n","\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 256,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        truncation=True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","  input_ids = torch.cat(input_ids, dim=0)\n","  attention_masks = torch.cat(attention_masks, dim=0)\n","  test_labels_tensor = torch.tensor(test_labels[indices[1]])\n","  test_skill_labels_tensor = torch.tensor(test_skill_labels[indices[1]])\n","  text = torchtext.data.Field(lower=True, batch_first=True, tokenize='spacy', include_lengths=True)\n","  target_diff = torchtext.data.Field(sequential=False, use_vocab=False, is_target=True)\n","  test.iloc[indices[1],:].to_csv(\"interim_test.csv\",index=False)\n","  text.build_vocab(train, val, test_text,vectors=\"glove.6B.100d\", min_freq=3)\n","\n","  test_text = torchtext.data.TabularDataset(path=\"interim_test.csv\",format='csv',\n","                                     fields={'difficulty_label': ('label', target_diff),\n","                                             'question_answer': ('text',text)})\n","  # test_text = torchtext.data.TabularDataset(examples=test.iloc[indices[1],:],\n","  #                                    fields={'difficulty_label': ('label', target_diff),\n","  #                                            'question_answer': ('text',text)})\n","  test_iter = torchtext.data.Iterator(dataset=test_text, batch_size=16,train=False, sort=False, sort_within_batch=False,shuffle=False)\n","# Set the batch size.  /\n","  batch_size = 16  \n","\n","  prediction_data = TensorDataset(input_ids, attention_masks, test_labels_tensor,test_skill_labels_tensor)\n","  # Create the DataLoader.\n","  prediction_sampler = SequentialSampler(prediction_data)\n","  prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n","  f1_bo,macro_f1_bo = get_bo_predictions(prediction_dataloader)\n","  f1_mlp,macro_f1_mlp = make_mlp_predictions(prediction_dataloader)\n","  f1_bo_ensemble.append(f1_bo)\n","  f1_mlp_ensemble.append(f1_mlp)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning:\n","\n","The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\n"]},{"output_type":"stream","name":"stdout","text":["weighted_metrics (0.5872249060654858, 0.5942028985507246, 0.5906932948791394)\n","macro_metrics (0.5802469135802469, 0.5722222222222223, 0.5762066297684938)\n","    DONE.\n","weighted_metrics (0.6627169931517758, 0.6521739130434783, 0.657403184747589)\n","macro_metrics (0.5882783882783883, 0.5843531468531469, 0.5863091979316648)\n","    DONE.\n","weighted_metrics (0.6892911010558069, 0.6911764705882353, 0.6902324983530698)\n","macro_metrics (0.6880341880341879, 0.6798433048433049, 0.6839142228363785)\n","    DONE.\n","weighted_metrics (0.7436570782159018, 0.75, 0.746815071272078)\n","macro_metrics (0.724969474969475, 0.7155047204066811, 0.7202060032126539)\n","    DONE.\n","weighted_metrics (0.6227941176470587, 0.6176470588235294, 0.6202099096900171)\n","macro_metrics (0.6083333333333333, 0.5982808191046177, 0.6032652015889978)\n","    DONE.\n"]}]},{"cell_type":"code","metadata":{"id":"eLO3BUOEKMpm","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3c8d959d-2423-4e32-c05d-38b5e4f51870"},"source":["print(f1_bo_ensemble)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.6644451176411188, 0.7467752339754965, 0.6765082935241985, 0.7940204960058294, 0.7059392201621116]\n"]}]},{"cell_type":"code","metadata":{"id":"71jVD2AAcMPe","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2b739976-745e-462e-a75c-972650c5a20e"},"source":["f1_mlp_ensemble"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.5906932948791394,\n"," 0.657403184747589,\n"," 0.6902324983530698,\n"," 0.746815071272078,\n"," 0.6202099096900171]"]},"metadata":{},"execution_count":142}]},{"cell_type":"code","metadata":{"id":"87jSlrNZJsfZ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0c85ae15-78dd-4060-bf33-2e876bee28a4"},"source":["from scipy import stats\n","stats.ttest_rel(f1_bo_ensemble,f1_mlp_ensemble)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Ttest_relResult(statistic=2.9657097365105622, pvalue=0.04131967319891361)"]},"metadata":{},"execution_count":143}]},{"cell_type":"markdown","metadata":{"id":"UDxKTJqNSnFp"},"source":["Now for macro f1"]},{"cell_type":"code","metadata":{"id":"t9YL5hHaSmd9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5c6c589a-514e-4e48-bc42-5bea774d837a"},"source":["\n","macro_f1_bo_ensemble = []\n","macro_f1_mlp_ensemble = []\n","for indices in kf.split(test_features):\n","  input_ids = []\n","  attention_masks = []\n","  for sent in test_features[indices[1]]:\n","\n","\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 256,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        truncation=True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","  input_ids = torch.cat(input_ids, dim=0)\n","  attention_masks = torch.cat(attention_masks, dim=0)\n","  test_labels_tensor = torch.tensor(test_labels[indices[1]])\n","  test_skill_labels_tensor = torch.tensor(test_skill_labels[indices[1]])\n","  text = torchtext.data.Field(lower=True, batch_first=True, tokenize='spacy', include_lengths=True)\n","  target_diff = torchtext.data.Field(sequential=False, use_vocab=False, is_target=True)\n","  test.iloc[indices[1],:].to_csv(\"interim_test.csv\",index=False)\n","  text.build_vocab(train, val, test_text,vectors=\"glove.6B.100d\", min_freq=3)\n","\n","  test_text = torchtext.data.TabularDataset(path=\"interim_test.csv\",format='csv',\n","                                     fields={'difficulty_label': ('label', target_diff),\n","                                             'question_answer': ('text',text)})\n","  # test_text = torchtext.data.TabularDataset(examples=test.iloc[indices[1],:],\n","  #                                    fields={'difficulty_label': ('label', target_diff),\n","  #                                            'question_answer': ('text',text)})\n","  test_iter = torchtext.data.Iterator(dataset=test_text, batch_size=16,train=False, sort=False, sort_within_batch=False,shuffle=False)\n","# Set the batch size.  \n","  batch_size = 16  \n","\n","  prediction_data = TensorDataset(input_ids, attention_masks, test_labels_tensor,test_skill_labels_tensor)\n","  # Create the DataLoader.\n","  prediction_sampler = SequentialSampler(prediction_data)\n","  prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n","  f1_bo,macro_f1_bo = get_bo_predictions(prediction_dataloader)\n","  f1_mlp,macro_f1_mlp = make_mlp_predictions(prediction_dataloader)\n","  macro_f1_bo_ensemble.append(macro_f1_bo)\n","  macro_f1_mlp_ensemble.append(macro_f1_mlp)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning:\n","\n","The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\n"]},{"output_type":"stream","name":"stdout","text":["weighted_metrics (0.5966614906832298, 0.5942028985507246, 0.59542965668095)\n","macro_metrics (0.5749007936507936, 0.5763532763532763, 0.5756261187377384)\n","    DONE.\n","weighted_metrics (0.5367535278021212, 0.5652173913043478, 0.5506178493416464)\n","macro_metrics (0.5259152612093788, 0.5483870967741935, 0.5369161504684382)\n","    DONE.\n","weighted_metrics (0.7072559793148028, 0.7205882352941176, 0.7138598634519169)\n","macro_metrics (0.6868131868131869, 0.6897240377632534, 0.6882655346187238)\n","    DONE.\n","weighted_metrics (0.7186458575525072, 0.7058823529411765, 0.7122069259474909)\n","macro_metrics (0.6656785243741766, 0.6668166141850352, 0.6662470832559124)\n","    DONE.\n","weighted_metrics (0.720978283786633, 0.7205882352941176, 0.7207832067721455)\n","macro_metrics (0.6958183990442054, 0.6851022540677713, 0.690418747134717)\n","    DONE.\n"]}]},{"cell_type":"code","metadata":{"id":"31_jaZqASy6P","colab":{"base_uri":"https://localhost:8080/"},"outputId":"96e8d698-ef79-4105-f340-e8e376310e18"},"source":["print(macro_f1_bo_ensemble)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.6705405345261144, 0.6529824599415647, 0.7109654908066055, 0.6987687309055189, 0.7177710723522684]\n"]}]},{"cell_type":"code","metadata":{"id":"IoqkoAbaSy6Q","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6487c507-db51-49e4-830a-08d098a17a15"},"source":["macro_f1_mlp_ensemble"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.5756261187377384,\n"," 0.5369161504684382,\n"," 0.6882655346187238,\n"," 0.6662470832559124,\n"," 0.690418747134717]"]},"metadata":{},"execution_count":146}]},{"cell_type":"code","metadata":{"id":"ManKEa4kSy6R","colab":{"base_uri":"https://localhost:8080/"},"outputId":"361bbd53-3a29-4228-9bc4-f2bbae46ccaa"},"source":["from scipy import stats\n","stats.ttest_rel(macro_f1_bo_ensemble,macro_f1_mlp_ensemble)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Ttest_relResult(statistic=3.018494465876295, pvalue=0.03922123795435013)"]},"metadata":{},"execution_count":147}]},{"cell_type":"code","metadata":{"id":"qPPkoifkKLxK"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dhb1vToRNmkx"},"source":["def accuracy_per_class(preds_flat, labels_flat):\n","\n","    for label in np.unique(labels_flat):\n","        y_preds = preds_flat[labels_flat==label]\n","        y_true = labels_flat[labels_flat==label]\n","        print(f'Class: {get_labels(label)}')\n","        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')\n","accuracy_per_class(flat_predictions,flat_true_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BjdqIPm96UYJ"},"source":[""],"execution_count":null,"outputs":[]}]}